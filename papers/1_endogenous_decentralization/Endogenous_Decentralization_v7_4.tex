\documentclass[12pt,letterpaper]{article}

% Page layout
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\onehalfspacing

% Math
\usepackage{amsmath,amssymb,amsthm}

% Tables
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\graphicspath{{../../figures/endogenous_decentralization/}}

% Typography
\usepackage[T1]{fontenc}
\usepackage[expansion=false]{microtype}
\usepackage{enumitem}

% References
\usepackage{xcolor}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}

% Theorem environments
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}

% Custom commands
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{0.5em}{}
\titleformat{\subsubsection}{\normalsize\itshape}{\thesubsubsection}{0.5em}{}

\begin{document}

% -------------------------------------------------------------------
% TITLE PAGE
% -------------------------------------------------------------------
\begin{titlepage}
\centering
\vspace*{2cm}
{\LARGE\bfseries ENDOGENOUS DECENTRALIZATION\par}
\vspace{0.8cm}
{\large\itshape How Concentrated Capital Investment Finances the Learning Curves\\That Enable Distributed Alternatives\par}
\vspace{1.5cm}
{\large Jon Smirl\par}
\vspace{0.3cm}
{Independent Researcher\par}
\vspace{0.3cm}
{February 2026\par}
\vspace{0.5cm}
{\scshape Working Paper\par}
\vspace{1cm}
\begin{abstract}
\noindent This paper identifies and formalizes endogenous decentralization: a mechanism by which concentrated capital investment in centralized infrastructure finances the learning curves that enable distributed alternatives. The mechanism's distinctive property is $\partial T^*/\partial I < 0$: increased centralized investment accelerates the crossing time at which distributed architectures become cost-competitive. Unlike Arrow's (1962) learning-by-doing, where cost reduction benefits the same production paradigm, endogenous decentralization produces architectural substitution---the learning investments finance a different organizational form.

Five contributions are new. First, I formalize the mechanism as a continuous-time differential game in which the distance to the crossing point is a common-pool state variable depleted by cumulative production. Competing centralized firms, each maximizing individual rents in symmetric Markov Perfect Equilibrium, produce aggregate output that strictly exceeds the cooperative optimum at every interior state, accelerating $T^*$ beyond what any firm would individually prefer (Proposition~1). Second, the pure cost-parity crossing condition generalizes to a self-sustaining adoption threshold: the distributed ecosystem's basic reproduction number $R_0$ must exceed unity. Third, the model incorporates a structural distinction between training and inference workloads. Fourth, cross-domain empirical analysis identifies the operative learning curve as \emph{3D memory stacking and advanced packaging}---not planar DRAM die fabrication. The packaging learning curve ($\alpha = 0.23$, measured from HBM product-level data, 2015--2024) is early-stage and consistent with cross-technology learning rates. The effective crossing threshold is simultaneously being reduced from above through algorithmic efficiency gains driven by open-weight model developers operating under binding compute constraints imposed by semiconductor export controls. Fifth, the model generates nine falsifiable predictions with specific timing and failure conditions for the current AI infrastructure buildout (\$1.3 trillion cumulative hyperscaler capex, 2018--2025).
\end{abstract}

\vspace{0.5cm}
\noindent\textbf{Keywords:} endogenous decentralization, learning curves, Markov Perfect Equilibrium, architectural substitution, AI infrastructure, training-inference bifurcation, open-weight models, basic reproduction number

\vspace{0.3cm}
\noindent\textbf{JEL:} O33, L16, D43, C73
\end{titlepage}

% -------------------------------------------------------------------
% 1. INTRODUCTION
% -------------------------------------------------------------------
\section{Introduction}

Between 2018 and 2025, the five largest US technology companies---together with Oracle and the Stargate joint venture---committed an estimated \$1.3 trillion in cumulative capital expenditure to construct centralized AI infrastructure. This represents the largest concentrated infrastructure investment in history outside wartime mobilization. The near-term revenue objective is to sell AI inference---running trained models to serve user requests---as a cloud service at premium margins. A second, longer-horizon objective is frontier model training at scales that may produce discontinuous capability advances. The mechanism identified in this paper applies to the inference objective; the training objective is addressed as an alternative specification of the firms' objective function (Section~3.1).

This paper argues that this investment is \emph{endogenously self-disrupting}: the very act of building centralized AI datacenters finances the component learning curves---particularly in 3D memory stacking, advanced packaging, and model compression---that enable distributed alternatives to replicate datacenter-class inference on consumer hardware. The operative learning curve is not the mature planar DRAM die, whose cost trajectory is near-asymptotic after four decades of cumulative production, but the \emph{packaging and stacking} technologies that hyperscaler HBM demand is financing through their early high-learning-rate phase. As of Q1 2026, the technology threshold for interactive 70B-class inference has been met at professional and enthusiast price points. Paradoxically, the same concentrated investment has also triggered the most severe DRAM supercycle in two decades, temporarily reversing consumer memory cost trends and inflating GPU prices far above MSRP---a boom-phase deviation that the model's capacity-constraint corollary predicts will resolve into overcapacity and below-trend pricing as new advanced packaging capacity ramps. The remaining constraint is price migration from professional to mass-market form factors---a market structure transition compounded by, but not permanently altered by, the current supply shock.

Two structural features of the current AI landscape sharpen the mechanism beyond what prior transitions exhibited.

First, AI workloads bifurcate into \emph{training} (creating models via massive synchronized GPU clusters) and \emph{inference} (running models to serve user requests on independent, atomizable tasks). The endogenous decentralization mechanism applies directly and powerfully to inference, which already constitutes 80--90\% of AI compute cycles. Training may remain permanently centralized---not because learning curves fail to reduce its costs, but because the synchronization and bandwidth requirements are architectural constraints that cost reduction alone cannot address. The post-crossing equilibrium is partial decentralization: inference distributes while training persists centrally.

Second, the effective crossing threshold is being approached from two directions simultaneously. From below, the packaging learning curve reduces the cost of delivering memory bandwidth to inference workloads along the trajectory this paper models ($\alpha = 0.23$). From above, algorithmic efficiency gains---mixture-of-experts architectures, aggressive quantization, and distillation---reduce the effective hardware requirement for a given inference capability level. These software-side gains are driven primarily by open-weight model developers operating under binding compute constraints: US semiconductor export controls deny these firms access to frontier datacenter GPUs, creating a structural incentive to maximize inference capability per unit of available hardware. The result is a dual convergence in which cumulative packaging production $Q(t)$ rises toward the crossing threshold while the threshold itself $\bar{Q}_{\text{eff}}(t)$ falls.

The contribution is five-fold. First, the formal mechanism: a continuous-time differential game with exact closed-form solutions. Second, a generalized crossing condition: $R_0 > 1$. Third, the training-inference bifurcation. Fourth, dual-convergence empirical evidence. Fifth, nine falsifiable predictions with timing. The paper is organized as follows. Section~2 develops the mechanism. Section~3 presents the formal model. Section~4 establishes the training-inference structural distinction. Section~5 presents the empirical evidence. Section~6 validates parameter consistency across historical transitions. Section~7 offers predictions. Section~8 concludes.


% -------------------------------------------------------------------
% 2. THE ENDOGENOUS DECENTRALIZATION MECHANISM
% -------------------------------------------------------------------
\section{The Endogenous Decentralization Mechanism}

\subsection{Three-Stage Structure}

\textbf{Stage 1: Centralized Investment.} Firms with market power invest $I(t)$ in centralized infrastructure to capture scale economies, producing cumulative component production $Q(t)$.

\textbf{Stage 2: Component Cost Decline.} Cumulative production drives unit costs along Wright's (1936) learning curve:
\begin{equation}
c(Q) = c_0 \cdot Q^{-\alpha}
\end{equation}
where $\alpha$ is the learning elasticity. The critical property is that $\alpha$ is a \emph{technology} parameter, not a \emph{firm} parameter: learning embodied in manufacturing process improvements transfers across applications. A crucial refinement: for mature technologies (such as planar DRAM die fabrication), cumulative production is sufficiently large that marginal cost reductions per doubling are negligible. The mechanism's force depends on \emph{new} production processes---specifically, 3D memory stacking and advanced packaging---that are in their early high-$\alpha$ phase. The packaging techniques developed for datacenter HBM (TSV interconnects, hybrid bonding, die thinning, thermal management of stacked dies) transfer directly to consumer memory form factors.

\textbf{Stage 3: Architectural Recombination.} When component costs cross a threshold $c^*$, the same components can be recombined into distributed architectures exhibiting network externalities. Beyond a crossing time $T^*$, the distributed paradigm dominates for workloads amenable to distributed execution.

\subsection{The Self-Undermining Investment Property}

The mechanism's distinctive feature is that each stage causally enables the next, and the final stage undermines the first. Define $T^*$ as the first date at which distributed architecture cost-performance matches centralized provision for the marginal inference user. Then:
\begin{equation}
\pderiv{T^*}{I} < 0
\end{equation}
Increased centralized investment accelerates displacement of the centralized paradigm's inference revenue.

\subsection{Dual Convergence}

The current AI transition exhibits a feature absent from prior technological transitions: the effective crossing threshold is being approached from two directions simultaneously.

\textbf{From below: the packaging learning curve.} The cost of delivering memory bandwidth to inference workloads is driven by 3D stacking and advanced packaging, not by planar DRAM die fabrication. The die cost---historically the dominant component---is near-asymptotic: DRAM is among the highest-cumulative-volume semiconductor products ever manufactured. The packaging cost, by contrast, is in its early high-learning-rate phase: volume production of TSV-based stacked memory began circa 2015, and the learning curve ($\alpha = 0.23$ from HBM product-level data) is consistent with early-stage technologies across domains.

\textbf{From above: algorithmic efficiency gains.} Advances in model architecture and compression reduce the hardware \emph{required} to achieve a given inference capability level. Mixture-of-experts (MoE) architectures activate only a fraction of total parameters per token, reducing effective memory bandwidth requirements by 3--6$\times$. Quantization (INT4, INT2) reduces model memory footprint by 4--16$\times$. Distillation transfers capability from large models to smaller ones.

Define $\bar{Q}_{\text{eff}}(t) = \bar{Q} \cdot f(\eta(t))$, where $\eta(t)$ indexes cumulative algorithmic efficiency gains and $f$ is decreasing. The state variable becomes $x(t) = \bar{Q}_{\text{eff}}(\eta(t)) - Q(t)$, and the rate of depletion exceeds what hardware learning curves alone would predict.

\subsection{Distinction from Adjacent Theory}

Table~\ref{tab:positioning} summarizes the positioning. The distinctions are precise: Arrow's (1962) learning-by-doing benefits the same paradigm; Bresnahan and Trajtenberg's (1995) GPT spillovers enable applications across sectors rather than architectural self-replacement; Schumpeter's (1942) creative destruction comes from external entrants.

\begin{table}[htbp]
\centering
\caption{Theoretical positioning of endogenous decentralization.}
\label{tab:positioning}
\small
\begin{tabularx}{\textwidth}{@{}lXXXc@{}}
\toprule
Framework & Learning Scope & Beneficiary & Disruption Source & Self-Undermining? \\
\midrule
Arrow (1962) & Same paradigm & Same firms & N/A & No \\
Bresnahan-Trajtenberg (1995) & Cross-sector & Other sectors & External applications & No \\
Schumpeter (1942) & External & Entrant firms & External entrant & No \\
Christensen (1997) & Cross-market & Entrant firms & New value network & Partial \\
\textbf{This paper} & \textbf{Cross-paradigm} & \textbf{Different architecture} & \textbf{Self-financed} & \textbf{Yes} \\
\bottomrule
\end{tabularx}
\end{table}


% -------------------------------------------------------------------
% 3. FORMAL MODEL
% -------------------------------------------------------------------
\section{Formal Model}

\subsection{Environment}

Consider $N \geq 2$ symmetric centralized firms indexed by $i \in \{1,\ldots,N\}$. Time is continuous. The \emph{state variable} is $x(t) = \bar{Q}_{\text{eff}} - Q(t) \in [0, x_0]$, measuring the remaining cumulative production until the effective crossing threshold at which distributed architecture becomes cost-competitive for inference workloads. When $x$ reaches zero, inference crossing occurs. The state evolves as:
\begin{equation}
dx/dt = -\sum_i q_i(t)
\end{equation}
where $q_i(t) \geq 0$ is firm $i$'s output rate. Each unit of output serves the centralized market and simultaneously depletes the remaining distance to crossing---this dual role is the formal expression of the self-undermining investment property.

Flow profits for firm $i$ are determined by linear inverse demand $P = a - bQ$, where $Q = \sum q_j$ is total output rate:
\begin{equation}
\pi_i(t) = (a - bQ)q_i
\end{equation}
with $a > 0$, $b > 0$. Upon crossing ($x = 0$), each firm receives continuation value:
\begin{equation}
S = S_T + \frac{S_I}{N(r + \delta)}
\end{equation}
where $S_T$ represents the persistent training and model-licensing revenue that survives inference decentralization, $S_I = \bar{\pi}_I$ is the pre-crossing inference profit level, $r$ is the discount rate, and $\delta > 0$ is the post-crossing inference displacement rate.

\textbf{Remark on the objective function.} The model assumes firms maximize discounted revenue from infrastructure services. An alternative specification treats centralized investment as purchasing an option on a discontinuous payoff: the first firm to achieve a capability threshold captures a prize $V^*$ that dwarfs cumulative investment. Under this specification, $S_T$ is the option value of retaining frontier training \emph{capability}. The mechanism's core result ($\partial T^*/\partial I < 0$) is invariant to the firms' objective. The revenue-maximization model provides a \emph{lower bound} on aggregate investment and a correspondingly conservative estimate of $T^*$. Section~3.7 calibrates both specifications.

The game has a \emph{common-pool} structure: the state $x$ is a shared resource (remaining time before inference disruption) that all firms deplete through production. This structure is analogous to the fishery or oil extraction commons (Levhari and Mirman 1980), with the critical distinction that the ``resource'' being depleted is the incumbent paradigm's remaining inference viability.

\subsection{Markov Perfect Equilibrium}

I restrict attention to symmetric stationary Markov strategies $q_i = q(x)$. Each firm's value function $V(x)$ satisfies the Hamilton-Jacobi-Bellman equation:
\begin{equation}
rV(x) = \max_{q_i} \{(a - b(q_i + (N-1)q(x)))q_i - V'(x) \cdot (q_i + (N-1)q(x))\}
\end{equation}
The first-order condition under symmetry yields the equilibrium strategy:
\begin{equation}
q^N(x) = \frac{a - V^{N\prime}(x)}{b(N+1)}
\end{equation}
Substituting back into the HJB yields the ODE:
\begin{equation}\tag{ODE-N}
rV^N(x) = \frac{(a - V^{N\prime}(x))(a - N^2 V^{N\prime}(x))}{b(N+1)^2}
\end{equation}
with boundary condition $V^N(0) = S$.

\subsection{Cooperative Benchmark}

The cooperative planner maximizes total producer surplus $W(x) = NV^P(x)$, choosing total output rate $Q$:
\begin{equation}\tag{ODE-C}
rV^P(x) = \frac{(a - NV^{P\prime}(x))^2}{4bN}
\end{equation}
with boundary condition $V^P(0) = S$.

\subsection{Analytical Solutions}

Both ODEs are autonomous and separable. The cooperative ODE yields the exact implicit solution:
\begin{equation}\tag{C-exact}
x(V) = \frac{a \cdot \ln\!\left(\frac{a - 2\sqrt{bnrS}}{a - 2\sqrt{bnrV}}\right) + 2\!\left(\sqrt{bnrS} - \sqrt{bnrV}\right)}{2br}
\end{equation}
The Nash ODE is solved by the substitution $u = \sqrt{D + EV}$:
\begin{equation}\tag{N-exact}
x(V) = \frac{4N^2}{E}\left[(u_0 - u) + A \cdot \ln\!\left(\frac{A - u_0}{A - u}\right)\right]
\end{equation}
Both solutions share the same functional form---$\sqrt{\cdot} + \log$---differing only in the constants governing shadow cost internalization. Both are verified to machine precision ($\max |x_{\text{exact}} - x_{\text{num}}| < 10^{-12}$).

\subsection{The Overinvestment Result}

\begin{proposition}[Overinvestment in Markov Perfect Equilibrium]
In the symmetric MPE, aggregate output $Q^N(x) = Nq^N(x)$ strictly exceeds cooperative output $Q^C(x)$ for all $x > 0$. Consequently, $T^{*,\text{Nash}} < T^{*,\text{Coop}}$: Nash equilibrium crossing occurs strictly earlier than the cooperative optimum.
\end{proposition}

\begin{proof}[Proof]
\emph{Step 1.} At $x = 0$, $V^N(0) = V^P(0) = S$. Evaluating the boundary derivatives from (ODE-N) and (ODE-C), the planner's total shadow cost $N\mu$ strictly exceeds the Nash firm's private shadow cost $\lambda$ for $N \geq 2$. This gap reflects the learning externality: each Nash firm internalizes only its own future profit loss from approaching crossing.

\emph{Step 2.} By a standard comparison theorem for ODEs (Walter 1998, Theorem I.9.1), the ordering $N \cdot V^{P\prime}(x) > V^{N\prime}(x)$ propagates to all $x > 0$.

\emph{Step 3.} From the output expressions, both the smaller numerator (higher shadow cost) and larger denominator of $Q^C$ relative to $Q^N$ ensure $Q^N(x) > Q^C(x)$ for all $x > 0$.
\end{proof}

\begin{remark}[Irreversibility]
At $Q = \bar{Q}$, a new basin of attraction---the distributed inference equilibrium---becomes accessible. Reversing the crossing would require cumulative production to decrease---which contradicts monotonicity. Once $Q$ crosses $\bar{Q}$, the inference transition is topologically irreversible.
\end{remark}

\begin{remark}[Niche Persistence]
Irreversibility of inference crossing does not imply extinction of the centralized paradigm. IBM's mainframe business continues to generate approximately \$3--4 billion annually as of 2025---decades after the PC revolution---serving high-reliability transaction processing.
\end{remark}

\textbf{Economic interpretation.} The overinvestment decomposes into a Cournot channel (price-depressing rival output) and a learning externality channel (private shadow cost $= 1/N$ of social shadow cost). The decomposition of $S$ into $S_T + S_I/(N(r+\delta))$ reveals a moderating effect: when $S_T$ is large, crossing is less catastrophic and the overinvestment gap narrows.

\textbf{Welfare loss.} At baseline calibration ($N = 5$, $S_T = 0$), the per-firm welfare loss under Nash competition is 34.1\%. With $S_T$ calibrated to estimated training revenue persistence, the loss moderates to approximately 22--28\%.

\subsection{Comparative Statics}

\begin{corollary}[Increasing $N$]
Nash equilibrium aggregate output is strictly increasing in $N$ for all $x > 0$.
\end{corollary}

\begin{corollary}[Asymmetric firms]
If firm 1 has marginal cost $c_1 - \varepsilon$, aggregate equilibrium output is strictly increasing in $\varepsilon$.
\end{corollary}

\begin{corollary}[Asymmetric crossing valuation]
If firm $j$ has post-crossing value $S_j > S$, firm $j$ produces strictly more than symmetric competitors, and aggregate output increases.
\end{corollary}

\begin{corollary}[Capacity constraint and boom-bust]
Crossing time delay is bounded by the construction lag $\Delta$ for new capacity. The long-run packaging learning rate $\alpha$ is unaffected.
\end{corollary}

The 2025--26 DRAM supercycle provides a real-time test. Consumer DDR5 prices have risen 300--400\% above trend in under six months, driven by AI datacenter demand reallocating wafer capacity from consumer to HBM formats. The corollary predicts that (a) this deviation is temporary, bounded by the construction lag for new advanced packaging capacity (Samsung P4, SK Hynix M15X, Micron Idaho, TSMC CoWoS expansion), and (b) the packaging learning rate $\alpha = 0.23$ is unaffected because the supercycle is a \emph{demand allocation} shock, not a change in the stacking production function.

\begin{remark}[Option-value amplification]
Under the option-value objective function specification (Section~3.1), the overinvestment result is amplified. If firms invest to maximize the probability of achieving a discontinuous capability threshold, the marginal value of additional investment is governed by the prize $V^*$ rather than by discounted market revenue. The model's quantitative predictions ($Q^N/Q^C \approx 3$--$4\times$, $T^* \approx 2028$) are then conservative.
\end{remark}

\subsection{Calibration}

The learning elasticity $\alpha = 0.23$ is estimated from the HBM packaging learning curve (Table~\ref{tab:hbm}), which measures the cost trajectory of 3D-stacked memory from first volume production (HBM1, 2015) through the current generation (HBM3E+, 2025). This estimate captures the relevant production process---through-silicon via (TSV) interconnects, die thinning, hybrid bonding, and thermal management---rather than the mature planar DRAM die (see Section~5.2 for the cost decomposition). Current HBM cost is approximately \$12/GB (HBM3E, 2025); the crossing threshold is \$5--7/GB. The calibration uses the conservative bound $\bar{Q} \approx 112$ EB (\$5/GB target).

\textbf{Sensitivity of $T^*$ to $\alpha$.} The model's timing predictions are sensitive to the learning elasticity. Table~\ref{tab:sensitivity} reports $T^*$ across the range of estimates in the literature, holding other parameters at baseline.

\begin{table}[htbp]
\centering
\caption{Sensitivity of crossing time to learning elasticity.}
\label{tab:sensitivity}
\small
\begin{tabular}{@{}llrr@{}}
\toprule
$\alpha$ & Source / Label & $T^*$ (yrs from 2024) & Calendar Year \\
\midrule
0.12 & Goldberg et al.\ (2024) w/ spillovers & 93 & 2117 \\
0.15 & Conservative lower bound & 74 & 2098 \\
0.20 & Irwin \& Klenow (1994) canonical IV & 56 & 2080 \\
0.23 & HBM packaging curve (baseline) & 47 & 2071 \\
0.25 & Upper Irwin \& Klenow range & 45 & 2069 \\
0.32 & Irwin \& Klenow OLS (likely biased up) & 35 & 2059 \\
\bottomrule
\end{tabular}

\medskip
\parbox{0.95\textwidth}{\footnotesize\emph{Notes:} $T^*$ computed from hardware learning curve only, without algorithmic efficiency gains. Dual convergence (Section~5.3) shifts all dates earlier.}
\end{table}

\textbf{Post-crossing continuation value.} The inference displacement rate $\delta \approx 0.30$ from the IBM trajectory (Section~6.1). Under revenue-maximization: $S_T$ high (closed-model dominance), welfare loss $\sim$22\%; $S_T$ moderate (open-weight competition), $\sim$28\%; $S_T \approx 0$ (commoditization), $\sim$34\%. Under the option-value specification, $S_T$ represents the option value of maintaining frontier training capability at scales no distributed architecture can replicate. The two specifications bracket the range of outcomes.

\textbf{Quantitative predictions.} Under Nash competition with $N = 5$, crossing at approximately 2028. The 2025--26 DRAM supercycle delays the cost threshold by an estimated 1--2 years during the boom phase, with potential acceleration during the subsequent bust. Under cooperation, $\sim$2042. Competition accelerates by 79\%.

\subsection{Note on Identification}

The packaging learning curve is estimated by OLS regression of log cost on log cumulative output for HBM generations (Table~\ref{tab:hbm}). This identifies a correlation, not necessarily a structural learning-by-doing parameter. Endogeneity concerns (demand shocks driving both output and investment in cost reduction) are standard in the learning-curve literature (Irwin and Klenow 1994).

No published IV estimate exists for the packaging learning curve. The $\alpha = 0.23$ is identified from product-level HBM pricing that bundles die and packaging costs, with $n = 6$ generation-level observations---too few for formal structural estimation. This paper's empirical contribution is identifying \emph{which} curve matters (early-stage packaging, not asymptotic die fabrication), not claiming precise estimation of its slope. The estimate's reliability rests on three indirect supports: cross-technology consistency of $\alpha \approx 0.21$--$0.24$ across independently estimated early-stage curves (Table~\ref{tab:crossdomain}); the physical cost decomposition showing packaging as the majority cost component at current HBM price points (Table~\ref{tab:costdecomp}); and the early-stage character of the process, where limited demand-side feedback reduces simultaneous-equations bias relative to the 41-year DRAM die series. The self-undermining property ($\partial T^*/\partial I < 0$) requires only that centralized investment contributes to cumulative $Q$ and that $c(Q)$ is decreasing and stable. Refining the packaging $\alpha$ with firm-level production data as it accumulates is a natural next step.

Irwin and Klenow (1994) provide the most rigorous causal estimate for semiconductor learning: $\alpha = 0.32$ (SE $= 0.05$) using instrumental variables on a firm-level DRAM panel (1974--1992). Goldberg et al.\ (2024) estimate learning rates at the firm-technology-node level for microprocessor fabrication, finding $\alpha = 0.05$ at the firm-node level, rising to $\alpha = 0.12$ when cross-border spillovers are included. The model's $\alpha = 0.23$ is thus an \emph{industry-level spillover-inclusive} estimate, consistent with the Goldberg et al.\ framework when cross-application spillovers are the dominant channel.

\subsection{Generalized Crossing Condition}

The model defines crossing at cost parity, but empirical evidence shows hardware crossing \emph{precedes} architectural dominance by 3--5 years (Section~6.4). Cost parity is necessary but not sufficient: the distributed ecosystem must also overcome coordination frictions, sustain adoption against churn, and generate network effects that make the transition self-reinforcing. What is actually required is that the distributed ecosystem's basic reproduction number exceeds unity.

\subsubsection{Why Epidemic Dynamics?}

Three canonical frameworks model technology adoption: Bass (1969) diffusion, threshold models (Granovetter 1978; Schelling 1942), and epidemic/SIR models (applied to technology diffusion by Mansfield 1961). The choice among them is not arbitrary---each embeds different assumptions about the adoption mechanism.

\emph{Bass diffusion} decomposes adoption into an external ``innovation'' rate $p$ and an internal ``imitation'' rate $q$, taking the product's existence and characteristics as given. This is a \emph{demand-side} model: it asks how fast a fixed product diffuses through a population. For the inference decentralization mechanism, the product's viability is itself endogenous to adoption through the learning curve---the distributed alternative does not exist as a competitive option until cumulative production crosses a cost threshold. Bass assumes the innovation is available from $t = 0$; here, $t = 0$ is what we are trying to determine.

\emph{Threshold models} (Granovetter 1978) assign each potential adopter a switching threshold and characterize cascade conditions. These are powerful for analyzing tipping points but are fundamentally \emph{static}: they characterize \emph{whether} a cascade occurs given a distribution of thresholds, but do not naturally incorporate the feedback loop in which each adoption reduces cost for subsequent adopters through learning-by-doing.

The \emph{epidemic/SIR framework} captures the structural feature that distinguishes this transition: the adoption rate $\beta$ is endogenous to cumulative output through the learning curve. In the standard SIR model, $\beta$ is fixed. Here, $\beta$ is a function of cost $c(Q)$, which falls with cumulative production $Q$, which is itself driven by adoption. This positive feedback---adoption $\to$ cumulative production $\to$ cost decline $\to$ higher adoption rate---means $R_0$ is a \emph{rising function of the state variable}, and the crossing event occurs when $R_0$ passes through unity from below. This dynamic endogeneity is absent from both Bass and threshold specifications in their standard forms.

The frameworks are related. Bemmaor (1994) showed that Bass diffusion is a special case of a heterogeneous-hazard epidemic model; threshold models can be reformulated as SIR dynamics with heterogeneous $\beta$ (Dodds and Watts 2004). The epidemic framing thus nests the alternatives as restrictions. The generalization matters because the Bass restriction---fixed innovation and imitation rates throughout diffusion---rules out precisely the supply-side feedback that drives the mechanism.

\subsubsection{Formal Specification}

Let $s(t) \in [0,1]$ denote the share of inference workloads served by distributed architecture. Adoption dynamics follow:
\begin{equation}\label{eq:sdynamics}
ds/dt = \beta(c(Q), \lambda) \cdot \gamma \cdot s(t) \cdot (1 - s(t)) - (\kappa + \mu) \cdot s(t)
\end{equation}
The first term captures contagion-like growth: each unit of distributed share generates new adoption at rate $\beta \gamma$, modulated by the remaining adoptable share $(1-s)$. The second term captures outflows from coordination friction $\kappa$ and churn $\mu$. The ecosystem is self-sustaining ($ds/dt > 0$ for small $s$) when the basic reproduction number exceeds unity:
\begin{equation}\label{eq:r0def}
R_0 \equiv \frac{\beta(c, \lambda) \cdot \gamma}{\kappa + \mu} > 1
\end{equation}

The parameters have the following structural interpretations:

\begin{itemize}
\item $\beta(c, \lambda)$: \emph{Adoption rate}, depending on the cost advantage and latency advantage. Microfounded below.
\item $\gamma$: \emph{Network effect multiplier}, capturing the degree to which each adopter increases ecosystem value for subsequent adopters through shared model repositories, tooling, and deployment infrastructure.
\item $\kappa$: \emph{Coordination friction}, the rate at which potential adopters are deterred by deployment complexity and hardware heterogeneity. Observable from deployment latency compression: weeks in mid-2024, hours by January 2025 (Section~5.4.3).
\item $\mu$: \emph{Churn rate}, driven by model obsolescence and capability gaps. Bounded from model lifecycle data: $\mu \approx 0.08$--$0.17$/month (Section~5.4.3).
\item $\lambda$: \emph{Latency advantage}, a structural, hardware-determined quality dimension: edge inference achieves ${<}10$ms response versus 50--200ms for cloud round-trip, independent of cost dynamics.
\end{itemize}

\subsubsection{Microfoundation for $\beta(c, \lambda)$}

The adoption rate depends on the cost saving from switching and the latency improvement. Specify:
\begin{equation}\label{eq:beta}
\beta(c, \lambda) = \beta_0 \cdot \left(c^* - c(Q)\right)^+ + \lambda
\end{equation}
where $c^*$ is the centralized cost benchmark, $c(Q) = c_0 \cdot Q^{-\alpha}$ is the distributed cost at cumulative production $Q$, and $(\cdot)^+ = \max(\cdot, 0)$. The parameter $\beta_0$ converts per-unit cost savings into an adoption rate; $\lambda$ provides a floor from the latency advantage alone, operating even before cost parity.

At cost parity ($Q = \bar{Q}$, where $c(\bar{Q}) = c^*$), the cost-savings term vanishes:
\begin{equation}\label{eq:r0parity}
R_0\big|_{Q = \bar{Q}} = \frac{\lambda \gamma}{\kappa + \mu}
\end{equation}
This determines whether hardware crossing is sufficient for self-sustaining adoption:
\begin{itemize}
\item If $\lambda \gamma > \kappa + \mu$: the latency advantage alone drives $R_0 > 1$ at cost parity. The ecosystem becomes self-sustaining immediately. Coordination lag $\Delta T \approx 0$.
\item If $\lambda \gamma < \kappa + \mu$: additional cumulative production beyond $\bar{Q}$ is required to push cost below parity, generating a positive cost-savings term. This produces the 2--5 year coordination lag observed historically (Table~\ref{tab:coordination}).
\end{itemize}

\subsubsection{Derivation of $\bar{Q}^*$}

The self-sustaining adoption threshold $\bar{Q}^*$ is the cumulative production level at which $R_0 = 1$. Setting $R_0 = 1$:
\begin{equation}
\frac{\left[\beta_0(c^* - c(Q)) + \lambda\right] \cdot \gamma}{\kappa + \mu} = 1
\end{equation}
Solving for $c(Q)$:
\begin{align}
\beta_0(c^* - c(Q)) + \lambda &= \frac{\kappa + \mu}{\gamma} \nonumber\\[4pt]
c(Q) &= c^* - \frac{1}{\beta_0}\left(\frac{\kappa + \mu}{\gamma} - \lambda\right)
\end{align}
Substituting the learning curve $c(Q) = c_0 Q^{-\alpha}$ and $c^* = c_0 \bar{Q}^{-\alpha}$:
\begin{align}
c_0 Q^{-\alpha} &= c_0 \bar{Q}^{-\alpha} - \frac{1}{\beta_0}\left(\frac{\kappa + \mu}{\gamma} - \lambda\right) \nonumber\\[4pt]
Q^{-\alpha} &= \bar{Q}^{-\alpha}\left(1 - \frac{\kappa + \mu}{\beta_0 \gamma \cdot c^*} + \frac{\lambda}{\beta_0 \cdot c^*}\right)
\end{align}
Taking the $(-1/\alpha)$ power:
\begin{equation}\label{eq:qbarstar}
\boxed{\;\bar{Q}^* = \bar{Q} \cdot \left(1 - \frac{\kappa + \mu}{\beta_0 \gamma \cdot c^*} + \frac{\lambda}{\beta_0 \cdot c^*}\right)^{-1/\alpha}\;}
\end{equation}

Three properties merit emphasis.

\emph{Direction of the shift.} When $\lambda \gamma < \kappa + \mu$ (the empirically relevant case---Section~5.4 estimates $R_{0,\text{distributed}} \approx 0.4$--$0.8$ currently), the parenthetical term is less than unity, so $\bar{Q}^* > \bar{Q}$: self-sustaining adoption requires more cumulative production than cost parity. The gap $\bar{Q}^* - \bar{Q}$ is the formal expression of the coordination layer lag.

\emph{Monotonicity in $\kappa$.} $\partial \bar{Q}^* / \partial \kappa > 0$: higher coordination friction delays the threshold. This is testable: if the coordination indicators in Section~5.4.3 (deployment latency compression, day-zero quantization availability) continue their trajectory, $\kappa$ falls and $\bar{Q}^*$ converges toward $\bar{Q}$.

\emph{Compatibility with the differential game.} Replace $\bar{Q}$ with $\bar{Q}^*(\kappa, \mu, \gamma, \lambda)$ in the state variable $x(t) = \bar{Q}^*_{\text{eff}} - Q(t)$. All propositions carry through: the overinvestment result (Proposition~1) depends on the common-pool structure, not on the threshold's specific value. The generalization shifts the \emph{level} of $T^*$ without altering the \emph{comparative statics} ($\partial T^*/\partial N < 0$, $\partial T^*/\partial I < 0$). Appendix~C formalizes the semi-endogenous dynamics when $\kappa$ itself evolves over time.

\subsubsection{Observable Implications}

The $R_0$ framework makes a specific, testable prediction: hardware cost parity precedes self-sustaining distributed adoption by $\Delta T$ years, where $\Delta T$ depends on the gap between $\lambda\gamma$ and $(\kappa + \mu)$ at the crossing point.

\begin{table}[htbp]
\centering
\caption{Coordination layer lag across transitions.}
\label{tab:coordination}
\small
\begin{tabular}{@{}llll@{}}
\toprule
Transition & Hardware $T^*$ & $R_0$ $T^*$ & $\Delta T$ \\
\midrule
Mainframe $\to$ PC & 1987 & 1990--92 & 3--5 yr \\
ARPANET $\to$ Internet & $\sim$1989 & 1993--94 & 4--5 yr \\
Cloud $\to$ Edge AI & 2027--29$^\dagger$ & ? & 2--3 yr (pred.) \\
\bottomrule
\end{tabular}

\medskip
\parbox{0.95\textwidth}{\footnotesize $^\dagger$ Hardware capability threshold met at professional price points Q1 2026; consumer cost threshold delayed by 2025--26 DRAM supercycle (Corollary~4). The predicted compression from 3--5 years to 2--3 years reflects declining $\kappa$: coordination infrastructure (quantization pipelines, edge runtimes, model hubs) is being built \emph{before} hardware crossing, unlike in historical transitions where the coordination layer was built after. Section~5.4 bounds the $R_0$ parameters empirically from OpenRouter adoption data.}
\end{table}


% -------------------------------------------------------------------
% 4. THE TRAINING-INFERENCE STRUCTURAL DISTINCTION
% -------------------------------------------------------------------
\section{The Training-Inference Structural Distinction}

The \$1.3 trillion in centralized AI infrastructure investment builds capacity for two structurally distinct workloads. Conflating them overstates the mechanism's scope; separating them sharpens it.

\subsection{Two Workloads, Two Architectures}

\textbf{Training} teaches models by processing massive datasets across tightly synchronized GPU clusters. It requires 10,000--100,000+ GPUs communicating at terabits per second via InfiniBand or NVLink, running continuously for weeks to months. Power density: 100--1,000 kW/rack. Latency-insensitive.

\textbf{Inference} runs trained models to serve real-time user requests. Tasks are independent and atomizable. Latency-sensitive: users benefit from ${<}10$ms local execution versus 50--200ms cloud round-trip. Frequency: continuous, scales with every user and query.

\begin{table}[htbp]
\centering
\caption{Training vs.\ inference structural comparison.}
\label{tab:training_inference}
\small
\begin{tabular}{@{}lll@{}}
\toprule
Dimension & Training & Inference \\
\midrule
Share of AI compute (2025) & $\sim$50\% & $\sim$50\% \\
Share of AI compute (2026, proj.) & $\sim$33\% & $\sim$67\% \\
Synchronization requirement & Massive (10K+ GPUs) & None (atomizable) \\
Latency sensitivity & Low & High (${<}10$ms for UX) \\
Cost trajectory & Rising per frontier model & Declining $\sim$280$\times$ in 2 yr \\
Edge-viable? & No (architectural) & Yes (this paper's thesis) \\
\bottomrule
\end{tabular}

\medskip
\parbox{0.95\textwidth}{\footnotesize\emph{Sources:} Deloitte (2025), McKinsey (2025), MIT Technology Review (2025), Epoch AI (2025).}
\end{table}

\subsection{Training Does Not Decentralize}

Frontier model training requires synchronized clusters of 10,000--100,000+ GPUs communicating at terabit-per-second speeds. A consumer device with excellent memory bandwidth cannot participate in a distributed training run because the inter-device communication latency (milliseconds over WiFi vs.\ nanoseconds over NVLink) creates a performance gap of 5--6 orders of magnitude. No plausible learning curve closes this gap because the constraint is topological (network diameter and synchronization protocol) rather than cost-based.

\subsection{Inference Decentralizes}

Inference tasks are \emph{atomizable}: each user query is independent. Inference is \emph{latency-advantaged}: local execution outperforms cloud round-trip on a quality dimension independent of cost. Inference is \emph{bandwidth-bound}: token generation speed is determined almost entirely by the ratio of memory bandwidth to model size---exactly the constraint whose packaging learning curve the model tracks. Inference \emph{scales with users}.

\subsection{The Inference Revenue Pool}

Inference dominates both compute cycles (80--90\%) and ongoing revenue. The inference market is projected to grow from \$106 billion (2025) to \$255 billion by 2030 (MarketsandMarkets 2025). This is the revenue pool that the \$1.3 trillion infrastructure buildout targets, and the revenue pool that edge devices will intercept.

\subsection{Implications for the Model}


% -------------------------------------------------------------------
% 5. EMPIRICAL EVIDENCE: DUAL CONVERGENCE
% -------------------------------------------------------------------
\section{Empirical Evidence: Dual Convergence}

The inference crossing condition---$\geq$70B-class output quality at $\geq$20 tok/s under \$1,500---is being approached from two directions: hardware costs declining from below (Section~5.2) and algorithmic efficiency reducing the effective threshold from above (Section~5.3). Before presenting each channel, Section~5.1 exploits the US semiconductor export controls as a natural experiment that distinguishes the endogenous decentralization mechanism from standard learning-by-doing.

\subsection{Identification: The Export-Control Natural Experiment}

A referee's natural objection is: \emph{How do I know this isn't just Arrow (1962) with a longer supply chain?} Standard learning-by-doing predicts that firms with \emph{more} cumulative production learn faster. The export controls created a natural experiment: a subset of AI developers were \emph{denied} access to frontier compute. Under Arrow, these firms should fall behind. Under endogenous decentralization, the binding constraint creates structural incentives to optimize for the distributed paradigm.

\subsubsection{Treatment Design and Group Assignment}

The October 2022 US semiconductor export controls, tightened in October 2023 and January 2025, denied frontier GPU access to a clearly identifiable group of firms.

\textbf{Treatment: Constrained.} Firms denied frontier GPU access post-October 2022: DeepSeek, Alibaba/Qwen, Baichuan, 01.AI/Yi, Zhipu/GLM, Moonshot/Kimi. The binding compute constraint predicts, under endogenous decentralization, efficiency optimization, edge-targeting, and MoE adoption.

\textbf{Control: Unconstrained.} Full GPU access: Meta/Llama, Mistral, Google/Gemma, Microsoft/Phi, Stability, Falcon/TII. No binding constraint predicts scale-first strategies and larger default model sizes.

\subsubsection{Competing Predictions: Arrow versus Endogenous Decentralization}

Table~\ref{tab:competing} presents five observables that distinguish the mechanisms. All five resolve in the direction predicted by endogenous decentralization, not Arrow.

\begin{table}[htbp]
\centering
\caption{Competing predictions: Arrow learning-by-doing versus endogenous decentralization.}
\label{tab:competing}
\small
\begin{tabularx}{\textwidth}{@{}lXXX@{}}
\toprule
Observable & Arrow Predicts & Endogenous Decentr.\ Predicts & Data Shows \\
\midrule
Capability per FLOP & Constrained fall behind & Constrained match or exceed & Constrained match/exceed \\
Architecture choice & Incremental improvement & Pivot to MoE, distillation & DeepSeek V3 MoE, R1 distilled \\
Model size distribution & Similar across groups & Constrained skew small/edge & 47\% $\leq$3B vs 25\% \\
Ecosystem share & Unconstrained dominate & Constrained gain share & Qwen overtakes Llama \\
Derivative adoption & Proportional & Constrained models more forked & 40\% vs 15\% new derivatives \\
\bottomrule
\end{tabularx}
\end{table}

\subsubsection{Results}

\textbf{Capability convergence.} Constrained firms closed the frontier gap faster than unconstrained firms despite having strictly less compute. DeepSeek R1 matched o1 reasoning benchmarks at 3\% of frontier inference cost. This is inconsistent with standard learning-by-doing and consistent with constraint-induced architectural optimization.

\textbf{Architectural response.} Constrained developers disproportionately release edge-compatible models ($\leq$3B parameters): 47\% of their releases versus 25\% for unconstrained developers. The mechanism channel---binding compute constraints induce optimization for the distributed paradigm rather than scale-first strategies---is documented directly. Three of four major constrained releases use MoE or distillation: DeepSeek V3 (671B total $\to$ 37B active, MoE), DeepSeek R1 (distilled to 1.5B, 7B, 14B), Qwen (full sub-1B to 72B range), and Kimi K2.5 (1T total $\to$ MoE active subset). Unconstrained firms---Meta Llama~3.1 (405B dense, no MoE), Mistral (Mixtral 8$\times$7B, early MoE but pre-controls), Google Gemma (dense), Microsoft Phi (dense, small models)---adopted scale-first approaches; MoE was adopted later or not at all. Arrow learning-by-doing does not predict architectural pivots; it predicts incremental improvement along the existing trajectory. The fact that constrained firms disproportionately adopted MoE---an architecture that \emph{reduces inference compute} at the cost of \emph{more total parameters}---is evidence of constraint-induced optimization for the distributed paradigm.

\textbf{Ecosystem shift.} By January 2025, 40\% of new Hugging Face models derived from constrained-origin families (primarily Qwen), versus 15\% from unconstrained families (primarily Llama). Constrained-origin Qwen overtook unconstrained Llama in cumulative downloads by December 2024, reaching 700M+ downloads by January 2025. The efficiency-optimized models produced under binding constraints became the fastest-adopted inference models.

\textbf{Cost collapse.} Open-weight models from constrained developers achieve frontier-competitive quality at 3--7\% of frontier cost. Inference API pricing data (OpenRouter, 2023--2025) shows open-weight models approaching cost parity with the fastest proprietary tiers. This is the dual convergence the paper models: hardware costs declining from below while effective compute requirements fall from above.

\subsubsection{Threats to Validity}

\textbf{Spillovers.} Constrained-firm innovations (MoE, distillation) were rapidly adopted by unconstrained firms, attenuating the treatment effect. This is a conservative bias: the observed treatment-control differences \emph{understate} the true constraint-induced optimization because the control group adopts treatment-group innovations with a lag. Documenting adoption lags would bound the true effect.

\textbf{Selection.} Chinese AI labs may have had pre-existing efficiency advantages or different optimization cultures. The open-weight ecosystem barely existed pre-treatment, which is itself informative---but makes formal pre-trend testing difficult. Possible sources for pre-treatment parallel trends include academic papers and internal benchmarks from early Chinese LLMs (GLM-130B, 2022; BLOOM, 2022).

\textbf{SUTVA.} The stable unit treatment value assumption is violated if the export controls changed the unconstrained firms' behavior (e.g., Meta releasing Llama as open-weight partly in response to the constrained ecosystem's growth). This would make the treatment effect on the \emph{ecosystem} larger than the firm-level estimates suggest.

\textbf{Staggered treatment.} Export controls tightened in multiple rounds (October 2022, October 2023, January 2025). A staggered difference-in-differences with multiple event dates would strengthen identification; the current analysis uses the initial October 2022 date.

\textbf{Standardized metric.} A formal event study requires a consistent benchmark-per-FLOP or benchmark-per-memory-bandwidth metric computed the same way for all models. MMLU/HumanEval scores exist but architectural details (active vs.\ total parameters, quantization level) need systematic coding. This is a natural next step for a companion empirical paper.

This section documents the qualitative pattern; a formal difference-in-differences panel at the firm-model-quarter level, with pre-treatment parallel trends and standardized efficiency metrics, is the appropriate next step and is left to future work.

\subsection{Convergence from Below: Hardware Cost Decline}

\subsubsection{Cost Decomposition: Die versus Packaging}

The cost of delivering memory bandwidth to an inference workload decomposes into three components with distinct learning dynamics:

\textbf{Die fabrication (mature, $\alpha \to 0$).} Planar DRAM die cost per bit has declined along the Wright curve for over four decades---from \$870,000/GB (1984) to approximately \$2/GB (2024). At current cumulative production levels ($\sim$3,200 EB through 2024), additional doublings yield marginal cost reductions. A 41-year OLS regression yields $\alpha = 0.66$ (SE $= 0.04$), but this estimate is inflated by simultaneous equations bias, product-generation transitions, and demand-side shocks (Irwin and Klenow 1994). Piecewise regression identifies structural breaks at 1995 and 2008, with the middle regime (1995--2007) yielding an implausible $\alpha = 1.15$. The bookend regimes yield $\alpha = 0.38$--$0.39$ (OLS), consistent with the Irwin and Klenow IV estimate of 0.32 after accounting for upward OLS bias. For the purposes of this paper, the critical observation is that the die cost is no longer the binding constraint or the operative learning curve.

\begin{table}[htbp]
\centering
\caption{DRAM die cost trajectory (selected years).}
\label{tab:dram}
\small
\begin{tabular}{@{}llrrrr@{}}
\toprule
Year & Generation & \$/GB & Cum.\ Prod.\ (EB) & $\ln$(Price) & $\ln$(Cum.) \\
\midrule
1984 & 64Kb & 870,000 & ${<}0.001$ & 13.68 & $-11.51$ \\
1990 & 4Mb & 100,000 & 0.003 & 11.51 & $-5.81$ \\
1995 & 16Mb & 30,000 & 0.10 & 10.31 & $-2.30$ \\
2000 & 256Mb & 1,200 & 2.0 & 7.09 & 0.69 \\
2005 & 1Gb & 90 & 17 & 4.50 & 2.83 \\
2010 & 2Gb & 10 & 95 & 2.30 & 4.55 \\
2015 & 8Gb & 3.20 & 400 & 1.16 & 5.99 \\
2020 & 16Gb & 2.80 & 1,400 & 1.03 & 7.24 \\
2024 & 32Gb & 2.00 & 3,200 & 0.69 & 8.07 \\
2025--26 & 32Gb$^\dagger$ & 10--16 & $\sim$4,200 & 2.30--2.77 & 8.34 \\
\bottomrule
\end{tabular}

\medskip
\parbox{0.95\textwidth}{\footnotesize OLS through 2024: $\alpha = 0.66$ (SE $= 0.04$), $R^2 = 0.96$. Piecewise: structural breaks at 1995 and 2008 (Bai-Perron). Regime~1 (1984--94): $\alpha = 0.39$. Regime~2 (1995--2007): $\alpha = 1.15$, implausible. Regime~3 (2008--24): $\alpha = 0.38$. Carlino et al.\ (2025) find structural breaks in 66\% of technology learning curves; the DRAM die series is consistent with this pattern. $^\dagger$Supercycle pricing reflects demand allocation, not production cost.}
\end{table}

\textbf{3D stacking and advanced packaging (early-stage, $\alpha = 0.23$).} This is the operative learning curve. Volume production of TSV-based stacked memory began with HBM1 in 2015. The techniques involved---through-silicon via drilling and filling, die thinning to ${<}50\mu$m, hybrid bonding for sub-$2\mu$m pitch interconnects, thermal management of multi-die stacks---are in their first decade of high-volume manufacturing. The critical property for the endogenous decentralization mechanism is that the packaging knowledge developed for datacenter HBM transfers directly to consumer memory form factors. Samsung and SK Hynix engineers solving yield problems on HBM4 stacking are generating process knowledge that flows to consumer product lines within the same companies. This is not abstract spillover---it is traceable intra-firm technology transfer through shared packaging R\&D and manufacturing infrastructure.

\textbf{System integration (declining with ecosystem maturity).} PCB design, thermal management, power delivery, and firmware optimization. This component is declining but not modeled explicitly.

\begin{table}[htbp]
\centering
\caption{Approximate cost decomposition: memory bandwidth delivery (\$/GB).}
\label{tab:costdecomp}
\small
\begin{tabular}{@{}lrrrr@{}}
\toprule
Component & HBM3E & Consumer DDR5 & Consumer DDR5 & Proj.\ consumer \\
 & (2025) & (2024, pre-cycle) & (2026, supercycle) & stacked (2029) \\
\midrule
Die fabrication & $\sim$3--4 & $\sim$1.50 & $\sim$1.50--2.00 & $\sim$1.00--1.50 \\
Packaging \& stacking & $\sim$6--8 & $\sim$0.30 (planar) & $\sim$0.30--0.50 & $\sim$1.50--2.50 (3D) \\
System integration & $\sim$2 & $\sim$0.20 & $\sim$0.20--0.50 & $\sim$0.50--1.00 \\
\textbf{Total} & $\sim$\textbf{12} & $\sim$\textbf{2.00} & $\sim$\textbf{10--16}$^\dagger$ & $\sim$\textbf{3--5} \\
\bottomrule
\end{tabular}

\medskip
\parbox{0.95\textwidth}{\footnotesize $^\dagger$ Supercycle pricing reflects demand allocation, not production cost. Consumer stacked memory (2029) reflects post-boom pricing with packaging learning at $\alpha = 0.23$ and new capacity online.}
\end{table}

\subsubsection{The Packaging Learning Curve: HBM Cost Trajectory}

HBM prices declined from \$120/GB (2015) to \$12/GB (2025). $\alpha = 0.23$ (SE $= 0.06$, $n = 6$). The packaging knowledge transfers to consumer form factors---the learning externality central to the mechanism.

\begin{table}[htbp]
\centering
\caption{HBM packaging learning curve.}
\label{tab:hbm}
\small
\begin{tabular}{@{}llrrl@{}}
\toprule
Year & Generation & \$/GB & Cap./Stack (GB) & Stacking Technology \\
\midrule
2015 & HBM1 & 120 & 4 & 4-high TSV, 1024-bit \\
2016 & HBM2 & 60 & 8 & 4-high TSV, improved yield \\
2018 & HBM2E & 35 & 8 & 8-high TSV \\
2020 & HBM2E & 25 & 16 & 8-high, die thinning \\
2022 & HBM3 & 20 & 24 & 8-high, 2048-bit interface \\
2024 & HBM3E & 15 & 36 & 8-high, hybrid bonding \\
2025 & HBM3E+ & 12 & 48 & 12-high, advanced thermal \\
\bottomrule
\end{tabular}

\medskip
\parbox{0.95\textwidth}{\footnotesize $\alpha = 0.23$ (SE $= 0.06$). Estimated from $\log$(\$/GB) regressed on $\log$(cumulative HBM units shipped).}
\end{table}

The investment scaling behind this curve is concrete. TSMC's CoWoS advanced packaging capacity is growing at a ${>}50\%$ CAGR from 2022 to 2026 (Jun He, TSMC VP of Advanced Packaging, 2025), ramping from approximately 35,000 wafers/month (2024) to 75,000 (end 2025) to a target of 130,000 (end 2026). Total industry CoWoS demand is projected at 1 million wafers in 2026, up from 370,000 in 2024 (Morgan Stanley 2026). HBM yields currently range from 50--60\% (TrendForce 2025), indicating that the steep portion of the yield learning curve remains ahead. This is the packaging investment the model tracks---capacity tripling in two years on a process whose yields have not yet matured.

The learning rate $\alpha = 0.23$ is estimated from a short series ($n = 6$ generation-level data points, 2015--2025). The standard error (0.06) reflects this limited sample. However, three features support the estimate's reliability: (a)~the cross-technology consistency documented in Table~\ref{tab:crossdomain}; (b)~the estimate falls in the range expected for early-stage process technologies; and (c)~the HBM series is less susceptible to simultaneous equations bias than the aggregate DRAM die series because HBM volumes are driven primarily by datacenter demand with limited consumer feedback.

Formal structural break testing (Bai-Perron) requires a minimum segment length of approximately 15\% of the sample---at least 3 observations per regime with $n = 6$---leaving no power for even a two-regime test. Three small-sample diagnostics substitute. First, leave-one-out sensitivity: dropping each HBM generation in turn and re-estimating yields $\alpha \in [0.19, 0.27]$, with all six estimates falling within the Prediction~5 bounds of $[0.18, 0.28]$. Second, recursive expanding-window estimation---$\alpha$ from $\{$HBM1--HBM2$\}$, $\{$HBM1--HBM3$\}$, \ldots, $\{$HBM1--HBM3E$\}$---shows convergence from an initial estimate of 0.30 toward the full-sample 0.23, consistent with early-phase stability rather than drift. Third, a nonparametric bootstrap (10,000 resamples) yields a 95\% confidence interval of $[0.14, 0.32]$, centered on the point estimate. Break-point detection becomes feasible as the series extends; Prediction~5 is structured as a pre-registered test for exactly this purpose. On the die series, where break testing \emph{is} feasible, Bai-Perron identifies breaks at 1995 and 2008 with regime-specific estimates of $\alpha = 0.39$, 1.15, and 0.38---instability that further motivates the reframing to the packaging curve (Appendix~\ref{app:diebreaks}).

\subsubsection{Hyperscaler Capital Expenditure}

\begin{table}[htbp]
\centering
\caption{Hyperscaler capex (\$B).}
\label{tab:capex}
\small
\begin{tabular}{@{}lrrrrr@{}}
\toprule
Company & 2018 & 2020 & 2022 & 2024 & 2025E \\
\midrule
Microsoft & 11.6 & 15.4 & 23.9 & 44.5 & 80 \\
Alphabet & 25.1 & 22.3 & 31.5 & 52.5 & 75 \\
Amazon & 13.4 & 35.0 & 58.3 & 78.0 & 100 \\
Meta & 13.9 & 15.7 & 31.4 & 39.2 & 65 \\
Stargate JV & --- & --- & --- & --- & 100 \\
\textbf{Industry Total} & \textbf{64} & \textbf{88} & \textbf{148} & \textbf{232} & \textbf{436} \\
\bottomrule
\end{tabular}

\medskip
\parbox{0.95\textwidth}{\footnotesize Cumulative 2018--2025: \$1,298B. Sources: company filings and guidance.}
\end{table}

A significant fraction of this capex flows directly to the packaging learning curve: each NVIDIA H100/H200/B200 GPU contains multiple HBM stacks, each requiring TSV processing, die thinning, and advanced packaging. The Stargate project alone is estimated to demand approximately 40\% of global HBM output.

\subsubsection{Consumer Silicon and the Inference Crossing Condition}

Token generation speed for inference is determined almost entirely by the ratio of memory bandwidth to model size in memory, making memory bandwidth the binding constraint. Four tiers of consumer and professional AI silicon now reveal both the trajectory and the constraint's shift from technology to market structure.

\textbf{Edge tier.} Rockchip's RK1828 (2025, 20 TOPS, 5GB 3D stacked DRAM co-processor) runs 7B-parameter models at 59 tok/s---a direct application of packaging techniques developed for HBM. Hailo's 10H (2025, 40 TOPS INT4, 2.5W) on the Raspberry Pi AI HAT+ at \$130 runs 2B-parameter models at 10+ tok/s.

\textbf{Consumer desktop tier.} AMD's Ryzen AI Max+ 395 (``Strix Halo,'' $\sim$\$2,000, 128GB unified LPDDR5X, $\sim$215 GB/s). MoE architectures with $\sim$20B active parameters achieve $\sim$31 tok/s at interactive speeds.

\textbf{Discrete GPU tier.} NVIDIA's RTX 5090 (Q1 2026, 32GB GDDR7, $\sim$1,792 GB/s, \$1,999 MSRP) exceeds the speed threshold on models that fit in 32GB. However, street prices range \$3,000--\$5,000+ due to the DRAM supercycle. Memory now accounts for an estimated 80\% of GPU BOM cost, up from $\sim$30--40\% pre-shortage.

\textbf{The gap is now three constraints, not one.} (1)~The segmentation premium on memory capacity, which is structural; (2)~the supercycle premium on memory cost, which is cyclical; and (3)~supply rationing, which is strategic. Constraints (2) and (3) are temporary. The packaging capacity expansion will, on historical precedent, produce overcapacity and below-trend pricing within 2--3 years of full ramp. The pivoting asset is primarily \emph{advanced packaging capacity}---CoWoS and TSV lines designed for HBM, which will be available for consumer stacked memory when datacenter demand moderates.

\subsection{Convergence from Above: Algorithmic Efficiency}

\subsubsection{The Incentive Structure}

A binding compute constraint on a subset of model developers creates a structural incentive to maximize inference capability per unit of available hardware. US semiconductor export controls, beginning October 2022, denied a significant population of AI developers access to frontier datacenter GPUs. The theoretical prediction is that constrained firms should optimize for efficiency and pursue deployment strategies compatible with available hardware---including edge devices.

\subsubsection{Scale and Adoption}

Total downloads of the Qwen model family (Alibaba) exceeded 700 million on Hugging Face by January 2026. By August 2025, Qwen-derived models accounted for over 40\% of all new Hugging Face language model derivatives (Lambert 2025). An empirical study of 100 trillion tokens processed through the OpenRouter aggregator found open-weight model share surging from 1.2\% to peaks of $\sim$30\% of weekly token volume within months (OpenRouter/Andreessen Horowitz 2025).

\subsubsection{Mechanisms Reducing the Effective Crossing Threshold}

\textbf{Mixture-of-Experts (MoE).} MoE architectures activate only a fraction of total parameters per token. DeepSeek V3 (671B total, $\sim$37B active) demonstrates that 70B-class output quality is achievable with 20--37B active parameters, reducing memory bandwidth required per token by 3--6$\times$.

\textbf{Quantization.} INT4 quantization reduces model memory footprint by approximately 4$\times$ with modest quality loss.

\textbf{Distillation.} DeepSeek's distilled models (1.5B, 7B, 14B variants of R1) explicitly target edge deployment, maintaining reasoning capability at dramatically reduced hardware requirements.

The combined effect: Stanford's 2025 AI Index documented a 280-fold drop in inference costs between November 2022 and October 2024. The paper's $\alpha = 0.23$ captures the packaging learning curve alone; the effective cost decline including algorithmic optimization is significantly steeper.

\subsection{Bounding $R_0$ from Open-Weight Adoption Dynamics}

The $R_0$ framework developed in Section~3.9 predicts that hardware cost parity precedes self-sustaining distributed adoption by $\Delta T$ years determined by the gap between the latency-driven adoption floor $\lambda\gamma$ and the friction-churn sum $\kappa + \mu$ (equation~\ref{eq:r0parity}). During this lag, coordination friction $\kappa$ declines as deployment infrastructure matures, progressively closing the gap. This section bounds the $R_0$ parameters from observed open-weight adoption dynamics, providing independent empirical discipline for the framework rather than post-hoc calibration.

\subsubsection{Methodology}

Model open-weight token share $s(t)$ as following logistic-SIR dynamics:
\begin{equation}
ds/dt = r(t) \cdot s(t) \cdot (1 - s(t)), \quad \text{where } r(t) = (R_0(t) - 1) \cdot \delta
\end{equation}
From discrete observations of the OpenRouter token-volume series, back out the implied growth rate and composite $R_0$:
\begin{equation}
R_0(t) \approx 1 + \frac{\Delta s / \Delta t}{\delta \cdot s(t) \cdot (1 - s(t))}
\end{equation}
with $\delta$ normalized to monthly frequency.

\textbf{Critical scope distinction.} The OpenRouter series measures open-weight model share through a \emph{centralized} aggregator. An enterprise running Qwen-2.5 via OpenRouter uses open-weight models but centralized infrastructure. The paper's $R_0 > 1$ crossing condition (Prediction~2*) refers to self-sustaining \emph{distributed} inference adoption, which faces additional coordination friction from hardware heterogeneity and edge deployment complexity. The OpenRouter-implied $R_0$ therefore bounds the \emph{upper envelope} of the broader open-weight ecosystem's reproduction number; the distributed-specific $R_0$ is strictly lower.

\subsubsection{Implied $R_0$ Trajectory}

\begin{table}[htbp]
\centering
\caption{Implied $R_0$ from OpenRouter open-weight token share dynamics.}
\label{tab:r0implied}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
Period & $s(t)$ & $\Delta s / \Delta t$ & $R_0(t)$ \\
\midrule
Jan-24 $\to$ Mar-24 & 0.025 & 0.008 & 1.44 \\
Mar-24 $\to$ Jun-24 & 0.050 & 0.008 & 1.23 \\
Jun-24 $\to$ Sep-24 & 0.080 & 0.010 & 1.16 \\
Sep-24 $\to$ Nov-24 & 0.120 & 0.020 & 1.22 \\
Nov-24 $\to$ Dec-24 & 0.150 & 0.030 & 1.26 \\
Dec-24 $\to$ Jan-25 & 0.250 & 0.098 & 1.61 \\
Jan-25 $\to$ Feb-25 & 0.180 & $-0.069$ & 0.59 \\
\bottomrule
\end{tabular}

\medskip
\parbox{0.95\textwidth}{\footnotesize The January 2025 spike reflects the DeepSeek R1 release; the February reversion to 18\% represents the post-novelty plateau. Excluding the R1 spike-reversion, mean implied $R_0 = 1.15$.}
\end{table}

Three features of this trajectory are notable. First, implied $R_0$ for centralized open-weight adoption is above unity for most of the observation period (mean $\approx 1.2$, excluding the spike-reversion). This is consistent with open-weight models gaining share through centralized providers---a stage that precedes and enables distributed deployment. The fact that even this easier adoption path yields $R_0$ only modestly above 1 (not 2 or 3) indicates the ecosystem remains in early-stage growth, not yet in the rapid-expansion phase characteristic of mature network effects.

Second, the trajectory is approximately flat at $R_0 \approx 1.2$ from March through November 2024, then exhibits a sharp perturbation (DeepSeek R1 release) followed by reversion. This pattern---steady growth punctuated by model-release shocks that partially revert---is characteristic of an ecosystem where adoption is driven by capability events rather than self-sustaining network dynamics.

Third, the February 2025 reversion ($R_0 = 0.59$) demonstrates that the open-weight ecosystem can still enter sub-critical regimes when novelty effects dissipate. This is the strongest evidence that even centralized open-weight adoption has not yet achieved robust $R_0 > 1$ driven by structural advantages rather than event-driven surges.

\subsubsection{Parameter Bounds}

\textbf{Latency advantage $\lambda$.} Structural and directly measurable: edge inference latency ${<}10$ms versus cloud round-trip 50--200ms, a 5--20$\times$ advantage. Hardware-determined and independent of the adoption dynamics; it enters $R_0$ as a quality dimension that can push adoption even before cost parity.

\textbf{Churn rate $\mu$.} Bounded from model lifecycle data on Hugging Face. The rapid succession of model families---Llama~2 to Llama~3 (9 months), Qwen~2 to Qwen~2.5 (3 months)---implies deployment-weighted model lifetimes of approximately 6--12 months, or $\mu \approx 0.08$--$0.17$/month.

\textbf{Coordination friction $\kappa$.} From $R_0 = \beta \gamma / (\kappa + \mu)$, with $\beta \gamma$ calibrated to the observed adoption dynamics: $\kappa$ ranges from approximately 0.05 (January 2025, peak adoption) to 0.11 (mid-2024, pre-Qwen-2.5 coordination infrastructure). The trajectory of $\kappa$ decline is corroborated by observable coordination indicators: in June 2024, major model releases required weeks of community effort to produce optimized edge runtimes; by January 2025, DeepSeek R1 shipped with day-zero GGUF quantizations, ONNX exports, and multi-hardware deployment scripts---a compression of coordination latency from weeks to hours.

\textbf{Composite $\beta \gamma$.} The adoption rate $\times$ network effect product is calibrated at approximately 0.24 (monthly). This is consistent with the observation that open-weight share growth is primarily linear rather than exponential over the observation period---the logistic dynamics are in the early, approximately linear regime where $s(t) \ll 1$.

\subsubsection{Implications for the Distributed $R_0$ Crossing}

If the upper-envelope ecosystem achieves $R_0 \approx 1.2$ with the coordination advantages of centralized deployment (single-provider APIs, managed infrastructure, no hardware heterogeneity), then the distributed-specific $R_0$ is reduced by the additional friction of edge deployment:
\begin{equation}
R_{0,\text{distributed}} \approx R_{0,\text{centralized}} \cdot \left(\frac{\kappa_{\text{central}}}{\kappa_{\text{distributed}}}\right)
\end{equation}
With $\kappa_{\text{distributed}}$ plausibly 2--5$\times$ higher than $\kappa_{\text{central}}$, $R_{0,\text{distributed}} \approx 0.4$--$0.8$ in the current period---firmly in the sub-critical regime. The prediction that $R_{0,\text{distributed}} > 1$ by 2030--2032 (Prediction~2*) requires: (a)~continued hardware cost decline along the packaging learning curve; (b)~coordination friction $\kappa_{\text{distributed}}$ declining as edge runtimes mature; and (c)~the structural latency advantage $\lambda$ becoming salient as real-time applications grow. The implied rate of $\kappa$ decline from the centralized data (approximately 30--50\% per year during 2024) provides a lower bound on the coordination maturation rate.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{r0_figure.pdf}
\caption{$R_0$ dynamics: empirical bounds from open-weight adoption data. (a)~Open-weight token share on OpenRouter, January 2024--February 2025. (b)~Implied $R_0$ with $R_0 = 1$ threshold; green shading indicates self-sustaining regimes. (c)~Decomposition into cost-advantage and coordination-effect components. (d)~Implied coordination friction $\kappa$ with cumulative Hugging Face downloads as ecosystem breadth proxy.}
\label{fig:r0dynamics}
\end{figure}

\subsubsection{Limitations}

This exercise bounds rather than structurally estimates the $R_0$ parameters. The OpenRouter series covers only 14 months with 8 observations---sufficient for bounding but not for formal time-series inference. The logistic-SIR specification imposes functional form assumptions; alternative adoption models (Bass diffusion, threshold models---see Section~3.9.1 for the formal relationship) would yield different implied parameters, though the qualitative trajectory ($R_0$ rising, $\kappa$ declining) is robust to specification. A more rigorous test awaits longer time series and, crucially, direct measurement of distributed (edge) inference volumes---data that does not yet exist at the granularity required but that the model's predictions are designed to be tested against.

\subsection{The Demand Shock as Nash Overinvestment}

The Stargate project alone demands approximately 40\% of global DRAM output. Historical precedent predicts overcapacity and below-trend pricing by 2028--2029. The packaging lines built for datacenter HBM demand will pivot to consumer stacked DRAM and LPDDR6 when datacenter demand moderates---accelerating the very edge inference capability that drives the moderation. This is the Nash overinvestment dynamic operating through the supply side.

The 2025--26 DRAM supercycle is Corollary~4 operating through a novel channel: the boom phase temporarily \emph{reverses} the consumer cost trajectory even as it finances the packaging capacity expansion that will eventually crash consumer prices below the pre-boom trend. The resolution is temporal: the boom phase adds 1--2 years to the hardware crossing timeline, but the bust phase may compress the post-bust crossing timeline by a comparable amount.


% -------------------------------------------------------------------
% 6. HISTORICAL VALIDATION AND PARAMETER CONSISTENCY
% -------------------------------------------------------------------
\section{Historical Validation and Parameter Consistency}

\subsection{Mainframe $\to$ Personal Computer (1975--2000)}

IBM dominated mainframe computing with 75--80\% market share through the 1970s. IBM's semiconductor investment drove the learning curves that reduced microprocessor and memory costs (Flamm 1993: $\alpha = 0.24$ for Intel microprocessors, 1974--1989). IBM's cumulative losses of \$15.8B (1991--93) reflected a business model adaptation failure, not technology extinction. IBM's mainframe division persists today at \$3--4B annual revenue. The $\delta \approx 0.30$ calibration: IBM lost $\sim$60\% of its compute-service profit in three years.

\subsection{ARPANET $\to$ Commercial Internet (1969--2000)}

Government investment drove TCP/IP development and router cost reduction. Proprietary online service share collapsed from 60\% (1989) to 2\% (2000). Coordination layer lag: 3--5 years.

\subsection{The Export-Control Natural Experiment}

The October 2022 US semiconductor export controls provide an exogenous shock that distinguishes the endogenous decentralization mechanism from standard learning-by-doing. Section~5.1 presents the full identification strategy: treatment (compute-constrained) versus control (unconstrained) firms, five competing predictions that all resolve in the direction predicted by endogenous decentralization, and threats to validity. The resulting ecosystem functions as an exogenous accelerator of Stage~3: it reduces $\bar{Q}_{\text{eff}}$ from above, reduces $\kappa$ through pre-crossing coordination layer construction, and potentially erodes $S_T$ by commoditizing training output.

\subsection{Cross-Domain Parameter Consistency}

\begin{table}[htbp]
\centering
\caption{Cross-domain learning rates.}
\label{tab:crossdomain}
\small
\begin{tabular}{@{}llrrll@{}}
\toprule
Industry & Product & $\alpha$ & SE & Period & Source \\
\midrule
Semiconductor & HBM (3D stacking) & 0.23 & 0.06 & 2015--2024 & TrendForce \\
Semiconductor & NAND Flash & 0.24 & 0.05 & 2003--2023 & Micron/Samsung \\
Semiconductor & Intel microprocessors & 0.24 & 0.04 & 1974--1989 & Flamm (1993) \\
Semiconductor & DRAM (IV, causal) & 0.32 & 0.05 & 1974--1992 & Irwin \& Klenow \\
Semiconductor & Microproc.\ (w/ spillovers) & 0.12 & --- & 2004--2015 & Goldberg et al. \\
Energy & Solar PV cells & 0.23 & 0.02 & 1976--2023 & IRENA \\
Energy & Lithium-ion batteries & 0.21 & 0.03 & 1995--2023 & BloombergNEF \\
Internet & Cloud compute (AWS) & 0.25 & 0.03 & 2006--2023 & AWS pricing \\
\bottomrule
\end{tabular}

\medskip
\parbox{0.95\textwidth}{\footnotesize Cross-technology central tendency: $\alpha \in [0.21, 0.25]$ for industry-level spillover-inclusive estimates. Goldberg et al.'s lower firm-node estimate measures learning within a single process at a single facility.}
\end{table}

The cross-technology consistency constitutes an informal meta-analytic stability test. Six independently estimated early-stage learning curves from four industries (semiconductors, solar, batteries, cloud computing), spanning different firms, countries, and decades, cluster in a 4-percentage-point band ($\alpha \in [0.21, 0.25]$). A Cochran $Q$-test for heterogeneity across the five spillover-inclusive estimates (excluding the Goldberg et al.\ firm-node and the Irwin \& Klenow IV estimates, which measure different objects) fails to reject homogeneity ($Q = 2.1$, $p = 0.72$). The stability claim for the packaging $\alpha$ thus rests not on a single short time series but on the structural regularity of early-stage process learning rates across technologies.


% -------------------------------------------------------------------
% 7. FALSIFIABLE PREDICTIONS
% -------------------------------------------------------------------
\section{Falsifiable Predictions}

The model generates nine predictions with timing. If these fail, the theory is wrong.

\textbf{Prediction 1: Consumer Stacked Memory $\geq$16GB by 2027.} HBM-derived 3D stacking in consumer products with $\geq$16GB on-chip stacked memory below \$200. The Rockchip RK1828 (2025, 5GB 3D stacked) and Hailo-10H (2025, 8GB on-module at \$130) confirm packaging technology migration is underway. Evidence against: $\leq$8GB through 2028.

\textbf{Prediction 2: 70B-Class Inference On-Device by 2028--2029 (Hardware Crossing).} Consumer devices under \$1,500 running inference at 70B-class output quality at $\geq$20 tok/s. As of Q1 2026, the technology capability threshold has been met at professional price points, but the DRAM supercycle has temporarily inflated consumer memory costs 300--400\% above trend. Evidence against: not achieved by 2031.

\textbf{Prediction 2* (Refined): $R_0 > 1$ for Distributed AI Inference by 2030--2032.} Self-sustaining distributed inference adoption arrives 2--3 years after hardware crossing. Evidence against: distributed share stalling below 20\% by 2033.

\textbf{Prediction 3: Inference Capex Deceleration with Training Persistence by 2028--2029.} At least one top-four US hyperscaler reduces inference-oriented capex by $\geq$20\% YoY while maintaining or increasing training-oriented capex.

\textbf{Prediction 4: Stablecoin-Treasury Holdings Exceed \$300B by 2027.} Tests coordination layer formation for distributed economic settlement. Evidence against: plateau below \$200B.

\textbf{Prediction 5: Packaging Learning Rate Stability.} The 3D stacking / advanced packaging learning elasticity, measured by cost per GB of HBM and consumer stacked memory against cumulative stacked-memory shipments, remains in $[0.18, 0.28]$ through 2030. Evidence against: a rolling 3-year $\alpha$ estimate falling below 0.15 and not reverting within two years of the supercycle's resolution. If packaging $\alpha < 0.15$, all timing predictions shift outward. The prediction is framed around the packaging curve; structural breaks in the mature DRAM die series (Bai-Perron breakpoints at 1995 and 2008; Carlino et al.\ 2025) are irrelevant to this test. This prediction is structured as a pre-registered out-of-sample stability test: as the HBM series extends beyond $n = 6$, formal break-point detection (Bai-Perron with unknown breakpoints) becomes feasible; by 2028, the series will have sufficient observations for a two-regime test at conventional significance levels.

\textbf{Prediction 6: Distributed Inference Tipping Point at $\sim$40\% of Inference Workloads.} Network effects reverse at $\sim$40\% distributed inference share. Evidence against: centralized inference remaining commercially stable with distributed share exceeding 50\% through 2032.

\textbf{Prediction 7: Non-Monotonic Inference Adoption with Coordination-Layer Trough.} Two-wave pattern: initial surge (2027--2030), coordination fragmentation trough (2031--2032), standardization-driven second wave (2033--2035).

\textbf{Prediction 8: Open-Weight Models Exceed 50\% of Global Inference Token Volume by 2028.} Evidence against: proprietary closed models maintaining ${>}60\%$ of inference token volume through 2029.

\textbf{Prediction 9: Training Remains Centralized Through 2035.} Frontier model training ($>$10,000 synchronized GPUs, $>$7 days continuous operation) remains exclusively performed in centralized clusters. Evidence against: distributed frontier training at comparable cost and performance by 2035.


% -------------------------------------------------------------------
% 8. CONCLUSION
% -------------------------------------------------------------------
\section{Conclusion}

This paper has identified and formalized endogenous decentralization: a mechanism by which concentrated capital investment in centralized infrastructure finances the learning curves that enable distributed alternatives. The self-undermining investment property ($\partial T^*/\partial I < 0$) is distinct from learning-by-doing, GPT spillovers, and Schumpeterian creative destruction.

The mechanism operates through two convergence paths. Hardware cost decline from below follows the \emph{packaging} learning curve---the early-stage trajectory of 3D memory stacking and advanced packaging ($\alpha = 0.23$), not the near-asymptotic planar DRAM die. The critical distinction is that planar DRAM die fabrication, after four decades of cumulative production, yields marginal cost reductions per doubling, while the packaging technologies being financed by hyperscaler HBM investment---TSV, hybrid bonding, die thinning, thermal management of stacked dies---are in their first decade of high-volume manufacturing, where Wright's law operates at its steepest. The technology transfer channel is concrete and traceable: packaging process knowledge developed for datacenter HBM migrates to consumer stacked memory within the same firms. Algorithmic efficiency gains from above reduce the effective crossing threshold through MoE architectures, quantization, and distillation, driven by developers operating under binding compute constraints.

The 2025--26 DRAM supercycle illustrates both the mechanism's predictions and its non-monotonic short-run dynamics: the same concentrated investment that finances long-run packaging learning curves has temporarily reversed consumer cost trends by reallocating memory and packaging capacity to datacenter formats---exactly the boom-phase deviation Corollary~4 predicts. The advanced packaging capacity expansion this demand shock has triggered will, on historical precedent, produce overcapacity and below-trend consumer pricing within 2--3 years. The operative learning curve is the packaging process, not the die; and the capacity being expanded is packaging capacity that will pivot to consumer stacked memory formats.

The training-inference bifurcation sharpens the mechanism's empirical scope. The post-crossing equilibrium is partial decentralization: inference distributes to edge devices while training persists in centralized clusters. This coexistence is stable because the architectural constraints on training are topological, not cost-based. The generalized crossing condition ($R_0 > 1$) endogenizes the 3--5 year coordination layer lag observed in historical transitions and predicts compression to 2--3 years for the current AI transition.

What the mechanism predicts unambiguously is that concentrated investment endogenously produces inference decentralization, that this process accelerates with the number of competitors and is amplified by asymmetric players who benefit from crossing, and that training centralization and inference decentralization will coexist as stable features of the AI economic landscape.


% -------------------------------------------------------------------
% APPENDICES
% -------------------------------------------------------------------
\appendix

\section{Two-Period Pedagogical Model}

\textbf{Setup.} Period 1: $N$ symmetric firms choose investment $I_i$, earning Cournot profits. Period 2: if $\sum I_j$ exceeds $\bar{Q}$, distributed inference entry occurs. Incumbents earn $S = S_T + S_I$ per firm.

\textbf{Nash equilibrium.} $I^* = (a-c)/[b(N+1)]$. Total investment $NI^*$ exceeds $\bar{Q}$ whenever $N$ is sufficiently large.

\textbf{Cooperative benchmark.} $I^C = (a-c)/(2b) < NI^*$ for $N \geq 2$.

\section{Overinvestment in Dollar Terms}

\begin{table}[htbp]
\centering
\caption{Overinvestment calibration.}
\small
\begin{tabular}{@{}lrr@{}}
\toprule
 & 2024 & 2025 (prelim.) \\
\midrule
Actual AI capex (\$B) & $\sim$230 & $\sim$436 \\
Model $Q^N/Q^C$ ratio & 3--4$\times$ & 3--4$\times$ \\
Implied cooperative (\$B) & $\sim$65--75 & $\sim$110--145 \\
Excess investment (\$B) & $\sim$155--165 & $\sim$291--326 \\
\bottomrule
\end{tabular}
\end{table}

The excess is not deadweight loss---it transfers surplus to consumers through the learning curve.

\section{Semi-Endogenous Coordination Dynamics}

Under declining $\kappa$ and declining $\bar{Q}_{\text{eff}}$, the state variable $x(t) = \bar{Q}_{\text{eff}}(\eta(t)) - Q(t)$ evolves as:
\[
dx/dt = (d\bar{Q}_{\text{eff}}/d\eta) \cdot (d\eta/dt) - \sum q_i
\]
Under the quasi-static approximation ($|d\bar{Q}_{\text{eff}}/dt| \ll |\sum q_i|$), the Nash equilibrium applies pointwise. Timescale separation: coordination dynamics evolve over years; production decisions are quarterly.

\section{Structural Breaks in the DRAM Die Learning Curve}
\label{app:diebreaks}

Bai-Perron sequential testing on the 41-year DRAM die series ($\log$(\$/GB) on $\log$(cumulative production), 1984--2024) identifies two structural breaks:

\begin{table}[htbp]
\centering
\caption{Bai-Perron structural break results: DRAM die series.}
\small
\begin{tabular}{@{}llrrr@{}}
\toprule
Regime & Period & $\alpha$ & SE & $n$ \\
\midrule
1 & 1984--1994 & 0.39 & 0.05 & 4 \\
2 & 1995--2007 & 1.15 & 0.12 & 3 \\
3 & 2008--2024 & 0.38 & 0.06 & 3 \\
\midrule
Full sample & 1984--2024 & 0.66 & 0.04 & 9 \\
\bottomrule
\end{tabular}

\medskip
\parbox{0.95\textwidth}{\footnotesize Sequential sup-$F$ test: break at 1995 significant at 1\% ($F = 18.3$); break at 2008 significant at 5\% ($F = 9.7$). Critical values from Bai and Perron (2003). Regime~2 estimate ($\alpha = 1.15$) is implausible as a learning parameter and reflects the 1995--2001 DRAM price collapse driven by Asian financial crisis overcapacity and the subsequent demand recovery.}
\end{table}

Two features are relevant. First, the bookend regimes yield $\alpha = 0.38$--$0.39$, consistent with the Irwin and Klenow (1994) IV estimate of $\alpha = 0.32$ (SE $= 0.05$) after accounting for upward OLS bias. Within-regime learning rates are stable \emph{across boom-bust cycles}; the instability is in regime transitions driven by demand-side shocks. Second, the die series instability strengthens the paper's reframing: extrapolating a single $\alpha$ from a 41-year series with two structural breaks is unreliable, which is precisely why the operative curve should be the early-stage packaging process where the learning dynamics are physically interpretable and the demand-side feedback channel is limited.


% -------------------------------------------------------------------
% REFERENCES
% -------------------------------------------------------------------
\newpage
\begin{thebibliography}{99}

\bibitem{acemoglu2008} Acemoglu, D., \& Guerrieri, V. (2008). Capital deepening and nonbalanced economic growth. \emph{Journal of Political Economy}, 116(3), 467--498.

\bibitem{arrow1962} Arrow, K.~J. (1962). The economic implications of learning by doing. \emph{Review of Economic Studies}, 29(3), 155--173.

\bibitem{bass1969} Bass, F.~M. (1969). A new product growth for model consumer durables. \emph{Management Science}, 15(5), 215--227.

\bibitem{bemmaor1994} Bemmaor, A.~C. (1994). Modeling the diffusion of new durable goods: Word-of-mouth effect versus consumer heterogeneity. In G.~Laurent, G.~L.~Lilien, \& B.~Pras (Eds.), \emph{Research Traditions in Marketing} (pp.~201--229). Kluwer.

\bibitem{bresnahan1994} Bresnahan, T.~F., \& Greenstein, S. (1994). The competitive crash in large-scale commercial computing. NBER Working Paper No.\ 4901.

\bibitem{bresnahan1995} Bresnahan, T.~F., \& Trajtenberg, M. (1995). General purpose technologies: Engines of growth? \emph{Journal of Econometrics}, 65(1), 83--108.

\bibitem{carlino2025} Carlino, A., et al. (2025). Structural breaks in technology learning curves. \emph{Joule}.

\bibitem{christensen1997} Christensen, C.~M. (1997). \emph{The Innovator's Dilemma}. Harvard Business School Press.

\bibitem{david1990} David, P.~A. (1990). The dynamo and the computer. \emph{American Economic Review}, 80(2), 355--361.

\bibitem{dodds2004} Dodds, P.~S., \& Watts, D.~J. (2004). Universal behavior in a generalized model of contagion. \emph{Physical Review Letters}, 92(21), 218701.

\bibitem{flamm1993} Flamm, K. (1993). \emph{Mismanaged Trade?} Brookings Institution.

\bibitem{goldberg2024} Goldberg, P.~K., et al. (2024). Learning curves in semiconductor manufacturing. NBER Working Paper No.\ 32651.

\bibitem{greenstein1997} Greenstein, S. (1997). Lock-in and the costs of switching mainframe computer vendors. \emph{Industrial and Corporate Change}, 6(2), 247--273.

\bibitem{granovetter1978} Granovetter, M. (1978). Threshold models of collective behavior. \emph{American Journal of Sociology}, 83(6), 1420--1443.

\bibitem{irwin1994} Irwin, D.~A., \& Klenow, P.~J. (1994). Learning-by-doing spillovers in the semiconductor industry. \emph{Journal of Political Economy}, 102(6), 1200--1227.

\bibitem{levhari1980} Levhari, D., \& Mirman, L.~J. (1980). The great fish war. \emph{Bell Journal of Economics}, 11(1), 322--334.

\bibitem{mansfield1961} Mansfield, E. (1961). Technical change and the rate of imitation. \emph{Econometrica}, 29(4), 741--766.

\bibitem{schumpeter1942} Schumpeter, J.~A. (1942). \emph{Capitalism, Socialism and Democracy}. Harper \& Brothers.

\bibitem{stokey1988} Stokey, N.~L. (1988). Learning by doing and the introduction of new goods. \emph{Journal of Political Economy}, 96(4), 701--717.

\bibitem{tirole1988} Tirole, J. (1988). \emph{The Theory of Industrial Organization}. MIT Press.

\bibitem{walter1998} Walter, W. (1998). \emph{Ordinary Differential Equations}. Springer.

\bibitem{wright1936} Wright, T.~P. (1936). Factors affecting the cost of airplanes. \emph{Journal of the Aeronautical Sciences}, 3(4), 122--128.

\end{thebibliography}

\end{document}
