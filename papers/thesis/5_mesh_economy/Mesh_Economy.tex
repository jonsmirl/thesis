\documentclass[12pt,letterpaper]{article}

% Page layout
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\onehalfspacing

% Math
\usepackage{amsmath,amssymb,amsthm}

% Tables
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}

% Typography
\usepackage[T1]{fontenc}
\usepackage[expansion=false]{microtype}
\usepackage{enumitem}

% References
\usepackage{xcolor}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}

% Theorem environments (numbered within sections)
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Custom commands (unified notation)
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\Rz}{R_0^{\text{mesh}}}
\newcommand{\Ceff}{C_{\text{eff}}}
\newcommand{\Cmesh}{C_{\text{mesh}}}
\newcommand{\Ccent}{C_{\text{cent}}}
\newcommand{\Sinf}{S_{\infty}}
\newcommand{\Nauto}{N_{\text{auto}}}
\newcommand{\Cmax}{C_{\text{max}}}
\newcommand{\phieff}{\varphi_{\text{eff}}}
\newcommand{\alphaeff}{\alpha_{\text{eff}}}
\newcommand{\alphacrit}{\alpha_{\text{crit}}}
\newcommand{\ddt}{\frac{d}{dt}}
\DeclareMathOperator*{\argmax}{arg\,max}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{0.5em}{}
\titleformat{\subsubsection}{\normalsize\itshape}{\thesubsubsection}{0.5em}{}

\begin{document}

% -------------------------------------------------------------------
% TITLE PAGE
% -------------------------------------------------------------------
\begin{titlepage}
\centering
\vspace*{2cm}
{\LARGE\bfseries THE MESH ECONOMY\par}
\vspace{0.8cm}
{\large\itshape Network Formation and Endogenous Capability Growth\\in Distributed AI Systems\par}
\vspace{1.5cm}
{\large Jon Smirl\par}
\vspace{0.3cm}
{Independent Researcher\par}
\vspace{0.3cm}
{February 2026\par}
\vspace{0.5cm}
{\scshape Thesis Chapter 5\par}
\vspace{1cm}
\begin{abstract}
\noindent Chapter~4 ends at the crossing point---the moment distributed AI inference becomes cost-competitive with centralized provision.  This chapter answers two questions: what organizational form emerges after the crossing, and can that form improve itself?

The answer to the first question is a self-organizing mesh of heterogeneous specialized agents whose collective capability exceeds centralized provision once the mesh reaches critical mass $N^*$.  The mesh equilibrium has three defining properties: connectivity (a giant component forms when $\Rz > 1$), heterogeneous specialization (agents develop complementary capabilities through adaptive threshold dynamics, with the CES diversity premium quantified by the curvature parameter $K$ from Chapter~2), and knowledge diffusion (information propagates self-sustainingly on scale-free topologies).  The phase transition from centralized to distributed provision is first-order (discontinuous) for more than two specialization types, via the Fortuin-Kasteleyn representation of the Potts model---the mesh crystallizes, it does not form gradually.  At the crossing point, inverse Bose-Einstein condensation dissolves the centralized traffic condensate as the learning curve broadens the fitness distribution.

The answer to the second question is regime-dependent.  Three parameters govern the mesh's endogenous capability growth: training productivity elasticity $\varphi$, training saturation $h$, and external data fraction $\alpha$.  When capability becomes a dynamical variable, the mesh contains a self-sustaining autocatalytic improvement core---a Reflexively Autocatalytic and Food-generated (RAF) set---above a threshold $\Nauto$ scaling logarithmically in system complexity.  The CES heterogeneity parameter $\rho < 1$ does double duty: it generates both the diversity premium for capability aggregation and the diversity protection against model collapse from self-referential training.  The most likely near-term regime is convergence to a ceiling determined by the Baumol bottleneck: the mesh's growth rate converges to the exogenous rate of frontier model improvement, closing the circle back to the concentrated investment modeled in Chapter~4.

Together, these results instantiate Levels~2 (Network) and~3 (Capability) of the four-level hierarchy developed in Chapter~3.  The model generates twelve falsifiable predictions with timing and failure conditions.
\end{abstract}

\vspace{0.5cm}
\noindent\textbf{Keywords:} mesh equilibrium, phase transition, Bose-Einstein condensation, heterogeneous specialization, CES aggregation, Potts model, autocatalytic sets, endogenous growth, model collapse, Baumol cost disease

\vspace{0.3cm}
\noindent\textbf{JEL:} D85, L14, O33, O41, C62, C73
\end{titlepage}

% -------------------------------------------------------------------
% 1. INTRODUCTION
% -------------------------------------------------------------------
\section{Introduction}\label{me:sec:intro}

Chapter~4 formalizes endogenous decentralization: concentrated capital investment in centralized AI infrastructure finances the component learning curves---particularly in 3D memory stacking and advanced packaging ($\alpha = 0.23$)---that enable distributed alternatives to replicate datacenter-class inference on consumer hardware.  The model's state variable $x(t) = \bar{Q}_{\text{eff}}(\eta(t)) - Q(t)$ measures remaining cumulative production until the crossing threshold.  When $x(T^*) = 0$, the basic reproduction number $R_0$ of the distributed ecosystem crosses unity, and the transition becomes self-sustaining and irreversible.

Chapter~4 ends at the crossing point.  This chapter begins there, and follows the distributed ecosystem through two stages: the formation of an organized network (Part~I) and the emergence of endogenous capability growth within that network (Part~II).

A natural but incorrect conjecture is that post-crossing dynamics are simply ``inference runs locally.''  Isolated devices running local models do not constitute an economic equilibrium.  A single consumer device, however capable, cannot match the breadth of a centralized provider serving millions of queries across every domain.  The question is whether there exists an organizational form---emergent, not designed---in which distributed devices collectively exceed centralized provision, and whether that form can improve itself without depending entirely on exogenous frontier model releases.

This chapter argues that both answers are affirmative.  The organizational form is a \emph{mesh} of heterogeneous specialized agents that self-organize through local interactions into a division of labor whose aggregate capability exceeds any centralized system.  And the mesh, once sufficiently large, contains an autocatalytic improvement core that makes capability endogenous---though the rate of improvement converges to the exogenous frontier training rate via a Baumol bottleneck.

The mathematical foundations draw on two results from the preceding chapters.  From Chapter~2 (Paper~A), the CES curvature parameter $K = (1-\rho)(J-1)/J$ (Definition~3.1) controls the superadditivity of the CES aggregate (Theorem~4.1), the correlation robustness that prevents model collapse (Theorem~5.1), and the strategic independence that stabilizes equilibrium (Theorem~7.1).  From Chapter~3 (Paper~B), the port topology theorem (Theorem~3.1) establishes the hierarchical architecture, and the activation threshold (Theorem~4.3) provides the spectral condition $\rho(\mathbf{K}) > 1$ under which nontrivial equilibrium exists.

The chapter proceeds as follows.  Section~\ref{me:sec:setup} establishes notation and the CES framework.  Part~I (Sections~\ref{me:sec:r0}--\ref{me:sec:postcrossing}) develops network formation: the $\Rz$ framework, giant component existence, Potts model crystallization, inverse Bose-Einstein condensation, heterogeneous specialization, knowledge diffusion, the central mesh equilibrium theorem, and post-crossing dynamics.  Part~II (Sections~\ref{me:sec:fixedrelaxed}--\ref{me:sec:baumol}) develops endogenous capability growth: the autocatalytic existence threshold, growth dynamics, the three growth regimes, diversity as collapse protection, and the Baumol bottleneck.  Section~\ref{me:sec:settlement} connects to the settlement infrastructure of Chapter~6.  Section~\ref{me:sec:rejected} discusses frameworks considered and rejected.  Section~\ref{me:sec:predictions} presents falsifiable predictions.  Section~\ref{me:sec:conclusion} concludes.


\subsection{Relation to the Thesis Framework}\label{me:sec:relation}

This chapter instantiates Levels~2 (Network, years timescale) and~3 (Capability, months timescale) of the four-level hierarchy developed in Chapter~3.  Level~2's state variable is heterogeneous AI agent density; Level~3's is training effectiveness.

The division into two parts mirrors the timescale separation that the hierarchy requires.  Network formation (Part~I) operates on a years timescale: adoption decisions, specialization emergence, and the crystallization of the division of labor unfold over the period 2028--2035.  Capability accumulation (Part~II) operates on a months timescale: training interactions, autocatalytic improvement, and variety expansion occur within each epoch of the network's evolution.  By the timescale separation principle (Chapter~3, Theorem~3.1(iv)), Part~I's equilibrium provides the backdrop---the slow manifold---against which Part~II's dynamics play out.

The hierarchical ceiling (Chapter~3, Proposition~8.1) binds at both levels.  Network size is bounded by hardware capability from Level~1: $F_2 \leq N^*(F_1)$.  Capability growth is bounded by network size from Level~2 and, ultimately, by the Baumol bottleneck that anchors the mesh's growth rate to the exogenous frontier training rate determined at Level~1.  The ceiling cascade---hardware bounds network, network bounds capability, capability bounds settlement---is a concrete instantiation of the hierarchical structure that Chapter~3 derives from CES geometry.

The activation threshold from Chapter~3 (Theorem~4.3) provides the spectral condition: the mesh equilibrium exists if and only if $\rho(\mathbf{K}) > 1$, where $\mathbf{K}$ is the next-generation matrix.  The $\Rz > 1$ condition developed in Part~I is the Level~2 component of this spectral threshold.  Cross-level amplification (Chapter~3, Section~4.4) implies that even when $\Rz < 1$ in isolation, coupling to Levels~1, 3, and~4 can push the system-wide reproduction number above unity.


% -------------------------------------------------------------------
% 2. SETUP
% -------------------------------------------------------------------
\section{Setup}\label{me:sec:setup}

\subsection{The CES Aggregate}\label{me:sec:ces}

The aggregate capability of the mesh is governed by the CES production function developed in Chapter~2 (Paper~A).  For $J \geq 2$ task types with capability levels $C_1, \ldots, C_J$:
\begin{equation}\label{me:eq:CES}
\Ceff = \left(\frac{1}{J}\sum_{j=1}^{J} C_j^{\rho}\right)^{1/\rho}, \quad \rho < 1, \; \rho \neq 0
\end{equation}
The curvature parameter $K = (1-\rho)(J-1)/J$ (Chapter~2, Definition~3.1) controls three properties simultaneously (Chapter~2, Theorem~7.1):
\begin{enumerate}[label=(\roman*)]
\item \emph{Superadditivity:} combining diverse capability profiles produces more aggregate capability than the sum of parts, with the gap proportional to $K$ (Chapter~2, Theorem~4.1).
\item \emph{Correlation robustness:} the CES nonlinearity extracts idiosyncratic variation that linear aggregates miss, with the bonus proportional to $K^2$ (Chapter~2, Theorem~5.1).
\item \emph{Strategic independence:} the balanced allocation is a Nash equilibrium; coalitions cannot profitably redistribute, with the penalty proportional to $K$ (Chapter~2, Theorem~6.1).
\end{enumerate}
These are not three separate assumptions but three views of the same geometric fact: the curvature of the CES isoquant at symmetric equilibrium.  This chapter exploits all three: superadditivity drives the diversity premium in Part~I, correlation robustness provides collapse protection in Part~II, and strategic independence ensures the mesh equilibrium is robust to manipulation.


\subsection{Notation}\label{me:sec:notation}

The following notation is used throughout both parts.

\begin{table}[htbp]
\centering
\caption{Unified notation.}
\label{me:tab:notation}
\small
\begin{tabular}{@{}lll@{}}
\toprule
Symbol & Definition & Source \\
\midrule
$\Rz$ & Mesh reproduction number $N \cdot \beta \cdot v / D$ & Part~I \\
$\Sinf$ & Giant component fraction & Part~I \\
$N^*$ & Critical mass for mesh dominance & Part~I \\
$\Ceff$ & CES aggregate capability & Both \\
$\Cmesh(N)$ & Mesh capability at $N$ agents & Part~I \\
$\Ccent$ & Centralized capability benchmark & Part~I \\
$\rho$ & CES substitution parameter ($< 1$) & Both \\
$K$ & Curvature parameter $(1-\rho)(J-1)/J$ & Chapter~2 \\
$J$ & Number of task/specialization types & Both \\
$\Nauto$ & Autocatalytic existence threshold & Part~II \\
$\Cmax$ & Capability ceiling under saturation & Part~II \\
$\phieff$ & Effective training productivity elasticity & Part~II \\
$\alphaeff$ & Effective external data fraction & Part~II \\
$\alphacrit$ & Model collapse threshold & Part~II \\
$\beta_{\text{auto}}$ & Autocatalytic fraction of training & Part~II \\
$h$ & Training saturation parameter & Part~II \\
$\delta$ & Model depreciation/obsolescence rate & Part~II \\
$g_Z$ & Exogenous frontier training growth rate & Part~II \\
\bottomrule
\end{tabular}
\end{table}


% ===================================================================
%
%  PART I: NETWORK FORMATION
%
% ===================================================================

\bigskip
\begin{center}
\rule{0.6\textwidth}{0.5pt}\\[6pt]
{\Large\bfseries Part I: Network Formation}\\[3pt]
{\itshape From the Crossing Point to Mesh Dominance}\\[6pt]
\rule{0.6\textwidth}{0.5pt}
\end{center}
\bigskip

\noindent At $t = T^*$, the ``mesh'' consists of a population of heterogeneous devices capable of running inference workloads locally but not yet organized into a connected network with routing, specialization, or coordination.  The number of capable devices $N(T^*)$ is positive but below critical mass.  Chapter~4's $R_0 > 1$ condition ensures that the population grows; the sections that follow characterize what it grows \emph{into}.


% -------------------------------------------------------------------
% 3. THE R_0 FRAMEWORK AND GIANT COMPONENT
% -------------------------------------------------------------------
\section{\texorpdfstring{The $\Rz$ Framework}{The R0mesh Framework}}\label{me:sec:r0}

The $R_0$ framework from Chapter~4 provides the boundary condition for the mesh.  The mesh reproduction number is:
\begin{equation}\label{me:eq:r0mesh}
\Rz = N \cdot \beta \cdot v / D
\end{equation}
where $N$ is the number of active nodes, $\beta$ is the connection probability per node pair, $v$ is the expected value per interaction, and $D$ is the attrition rate (encompassing coordination friction $\kappa$ and churn $\mu$ from Chapter~4).  The mesh is self-sustaining when $\Rz > 1$.  This is the Level~2 component of the activation threshold from Chapter~3 (Theorem~4.3).

\subsection{Giant Component Existence}\label{me:sec:gc}

The fraction of nodes belonging to the giant connected component satisfies:
\begin{equation}\label{me:eq:Sinf}
\Sinf = 1 - \exp(-\Rz \cdot \Sinf)
\end{equation}

\begin{proposition}[Giant Component Existence and Uniqueness]\label{me:prop:gc}
Equation~\eqref{me:eq:Sinf} has:
\begin{enumerate}[label=(\alph*)]
\item the trivial solution $\Sinf = 0$ for all $\Rz$;
\item a unique positive solution $\Sinf^* \in (0,1)$ if and only if $\Rz > 1$;
\item $\Sinf^*$ is locally asymptotically stable as a fixed point of the iteration $\Sinf \mapsto 1 - \exp(-\Rz \cdot \Sinf)$.
\end{enumerate}
\end{proposition}

\begin{proof}
Define $g(s) = 1 - \exp(-\Rz \cdot s) - s$.  At $s = 0$: $g(0) = 0$ and $g'(0) = \Rz - 1 > 0$ when $\Rz > 1$, so $g$ is initially increasing.  At $s = 1$: $g(1) = -\exp(-\Rz) < 0$.  By the intermediate value theorem, $g$ has a root $\Sinf^* \in (0,1)$.

For uniqueness: $g''(s) = -{\Rz}^2 \exp(-\Rz s) < 0$, so $g$ is strictly concave.  A strictly concave function crossing zero from above can have at most one positive root.

For stability: the iteration map $h(s) = 1 - \exp(-\Rz s)$ satisfies $h'(\Sinf^*) = \Rz \exp(-\Rz \Sinf^*) = \Rz(1 - \Sinf^*)$.  At the positive fixed point with $\Rz > 1$, $0 < h'(\Sinf^*) < 1$ (since $\Sinf^* > 0$ implies $1 - \Sinf^* < 1$ and $\Rz(1 - \Sinf^*) < 1$ by concavity), confirming local asymptotic stability by the contraction mapping principle.
\end{proof}


% -------------------------------------------------------------------
% 3.3 FORTUIN-KASTELEYN UNIFICATION
% -------------------------------------------------------------------
\subsection{The Fortuin-Kasteleyn Unification}\label{me:sec:FK}

The giant component result establishes \emph{connectivity}.  The question of whether the mesh develops \emph{specialization}---a division of labor among heterogeneous agents---might appear to require a separate model.  The Fortuin-Kasteleyn~\cite{me:FK1972} representation reveals that these are the same mathematical object.

The partition function of the $q$-state Potts model on graph $G = (V, E)$ has the exact cluster expansion:
\begin{equation}\label{me:eq:FK}
Z_{\text{Potts}} = \sum_{A \subseteq E} p^{|A|}(1-p)^{|E|-|A|} \cdot q^{c(A)}
\end{equation}
where $p = 1 - e^{-\beta_T J_c}$ is the bond occupation probability (with $\beta_T$ the inverse temperature and $J_c$ the coupling), $A$ is a subset of edges, and $c(A)$ is the number of connected components in the subgraph $(V, A)$.

At $q = 1$, this reduces to the generating function for bond percolation.  The Potts model at general $q$ describes a system in which nodes adopt one of $q$ states (specializations) and prefer to match their neighbors.  The FK representation shows that percolation and specialization are controlled by the same cluster structure evaluated at different $q$.

\begin{proposition}[First-Order Crystallization]\label{me:prop:firstorder}
For $q > 2$ specialization types on a graph with mean degree $\langle k \rangle > 1$, the specialization transition is first-order: the order parameter (fraction of nodes in the dominant specialization cluster) jumps discontinuously from zero to a positive value at the critical coupling $\beta_T J_c = \beta_c(q)$.
\end{proposition}

This is a classical result in statistical mechanics \cite{me:baxter1982,me:wu1982}.  The economic content is that the mesh does not form gradually.  When the parameter $p$ (interpretable as the probability that two neighboring agents successfully coordinate on a task division) crosses the threshold, division of labor crystallizes abruptly.  Early mesh growth appears stochastic and fragile; the crystallization event is sudden.

For $q = 2$ (two specialization types), the transition is second-order (continuous)---the standard Ising universality class.  This means the first-order prediction is specific to settings with $q \geq 3$ distinct specialization roles, which is the empirically relevant case for AI inference (coding, creative writing, mathematical reasoning, multimodal processing, domain-specific knowledge, real-time translation, etc.).


% -------------------------------------------------------------------
% 3.4 INVERSE BOSE-EINSTEIN CONDENSATION
% -------------------------------------------------------------------
\subsection{Inverse Bose-Einstein Condensation}\label{me:sec:BEC}

The Bianconi-Barab\'{a}si~\cite{me:barabasi2001} model assigns each node $i$ a fitness $\eta_i$ drawn from distribution $\rho(\eta)$.  The degree of node $i$ evolves as:
\begin{equation}\label{me:eq:BB}
k_i(t) \sim \left(\frac{t}{t_i}\right)^{\eta_i / C}
\end{equation}
where $t_i$ is the node's arrival time and $C$ satisfies the self-consistency equation:
\begin{equation}\label{me:eq:BEC}
\int \frac{\rho(\eta)}{C/\eta - 1}\, d\eta = 1
\end{equation}
This has the identical mathematical structure to the Bose gas number equation, with $C$ playing the role of the fugacity.

The phase structure depends on $\rho(\eta)$ near its maximum $\eta_{\max}$.  If $\rho(\eta) \sim (\eta_{\max} - \eta)^{\alpha_B}$ as $\eta \to \eta_{\max}$:

\begin{itemize}
\item $\alpha_B \leq 0$ (sharply peaked): \emph{Bose-Einstein condensation}.  The single fittest node captures a macroscopic fraction of all connections.  This is the centralized equilibrium---AWS, Google Cloud, and Azure dominate because they are the only nodes with high fitness.
\item $\alpha_B > 0$ (broad distribution): \emph{Fit-get-rich} phase.  Many nodes share traffic proportional to their fitness.  No single node dominates.  This is the mesh equilibrium.
\end{itemize}

\begin{proposition}[Learning-Curve-Driven Phase Transition]\label{me:prop:BEC}
Let the technology parameter $\theta(t)$ index the packaging learning curve output from Chapter~4.  Suppose the fitness of an edge device of type $j$ is $\eta_j(\theta) = \eta_j^0 + g_j(\theta)$, where $g_j$ is increasing in $\theta$ and $g_j(0) = 0$.  If $\theta(0)$ produces a fitness distribution $\rho_0(\eta)$ with $\alpha_B \leq 0$, and $\theta(\bar{t})$ produces $\rho_{\bar{t}}(\eta)$ with $\alpha_B > 0$ for some finite $\bar{t}$, then the system undergoes a BEC-to-FGR phase transition at the critical $\theta^*$ where $\alpha_B$ crosses zero.  This is inverse Bose-Einstein condensation: the centralized condensate dissolves.
\end{proposition}

\begin{proof}
The learning curve increases the fitness of previously low-fitness edge devices.  Initially, only datacenter nodes have $\eta$ near $\eta_{\max}$, so $\rho(\eta)$ is sharply peaked at the maximum---the BEC phase.  As $\theta$ increases, the support of $\rho$ broadens: more device types achieve fitness levels closer to $\eta_{\max}$.  The exponent $\alpha_B$ characterizing the behavior of $\rho$ near $\eta_{\max}$ transitions from $\leq 0$ to $> 0$ at $\theta = \theta^*$.  By the Bianconi-Barab\'{a}si classification, this is the BEC phase boundary.

The mapping to Chapter~4 is direct: $\theta^*$ corresponds to $x(t) = 0$.  At $T^*$, the fitness distribution has broadened sufficiently that the centralized condensate---the macroscopic concentration of traffic at a single node---dissolves.  Traffic distributes across the mesh proportional to fitness.
\end{proof}

\begin{remark}\label{me:rem:BEClayers}
The BEC framework describes the \emph{competitive dynamics} of traffic allocation on the network.  It does not describe the physical connectivity (percolation, Section~\ref{me:sec:gc}) or the specialization structure (CES aggregation, Section~\ref{me:sec:specialization}).  The layers compose: percolation ensures the mesh is connected, BEC dynamics govern how traffic flows within it, and CES aggregation determines whether the mesh's collective capability exceeds the centralized alternative.
\end{remark}


% -------------------------------------------------------------------
% 3.5 HETEROGENEOUS SPECIALIZATION
% -------------------------------------------------------------------
\section{Heterogeneous Specialization}\label{me:sec:specialization}

\subsection{Agent Capabilities and CES Aggregation}\label{me:sec:agent_CES}

Each agent $i \in \{1, \ldots, N\}$ has a capability vector $\mathbf{c}_i = (c_{i1}, \ldots, c_{iJ})$ across $J$ task types.  The effective capability of the mesh for task type $j$ is the sum of agent capabilities: $C_j = \sum_i c_{ij}$.  The aggregate mesh capability is the CES function~\eqref{me:eq:CES}.

The parameter $\rho < 1$ implies imperfect substitutability across task types.  Because task types are complements, the aggregate rewards \emph{diversity}.  Two agents with different specializations contribute more to $\Ceff$ than two identical agents.  This is the superadditivity result from Chapter~2 (Theorem~4.1) applied to the mesh's capability vectors.

\begin{lemma}[Diversity Premium]\label{me:lem:diversity}
Fix total capability $\bar{C} = \sum_j C_j$.  For $\rho < 1$, $\Ceff$ is maximized when $C_j = \bar{C}/J$ for all $j$ (equal coverage across task types).  More precisely:
\begin{equation}
\Ceff\big|_{\text{equal}} = J^{(1-\rho)/\rho} \cdot \Ceff\big|_{\text{concentrated}}
\end{equation}
where the concentrated case places all capability in a single task type.  The diversity premium $J^{(1-\rho)/\rho}$ is increasing in $J$ and decreasing in $\rho$.
\end{lemma}

\begin{proof}
With equal allocation: $\Ceff = (J \cdot (\bar{C}/J)^{\rho})^{1/\rho} = J^{1/\rho - 1} \cdot \bar{C}$.  With concentration in one type: $\Ceff = \bar{C}$.  The ratio is $J^{(1-\rho)/\rho}$, which exceeds 1 for $J \geq 2$ and $\rho < 1$.
\end{proof}

This is the Becker-Murphy~\cite{me:becker1992} division of labor result in CES form, and a direct application of Chapter~2's superadditivity theorem (Theorem~4.1).  The mesh's advantage over centralized provision does not come from superior individual capability---each edge device is weaker than the datacenter---but from the breadth of specialized coverage that heterogeneous agents collectively provide.


\subsection{Centralized Capability Benchmark}\label{me:sec:centralized}

A centralized provider operates $M$ identical high-capability units, each with capability $\bar{c}$ spread uniformly across all $J$ task types: $c_j^{\text{cent}} = \bar{c}/J$ per unit.  Total centralized capability for task $j$ is $C_j^{\text{cent}} = M\bar{c}/J$, giving:
\begin{equation}
\Ccent = \left(J \cdot \left(\frac{M\bar{c}}{J}\right)^{\rho}\right)^{1/\rho} = J^{(1-\rho)/\rho} \cdot M\bar{c}
\end{equation}
The centralized provider has fixed capacity $M\bar{c}$ (determined by datacenter investment).  The mesh's aggregate capability grows with $N$ and with the diversity of specialists.


\subsection{Specialization Dynamics: The Fixed Response Threshold Model}\label{me:sec:FRT}

Agents do not arrive pre-specialized.  Specialization emerges endogenously through local interactions.  The mechanism follows the Bonabeau-Theraulaz~\cite{me:bonabeau1998} fixed response threshold model, originally developed for division of labor in social insect colonies.

Agent $i$ has a vector of response thresholds $\boldsymbol{\theta}_i = (\theta_{i1}, \ldots, \theta_{iJ})$ for each task type.  When demand signal $s_j$ for task $j$ arrives, agent $i$ performs the task with probability:
\begin{equation}\label{me:eq:FRT}
P_{ij}(s_j) = \frac{s_j^n}{s_j^n + \theta_{ij}^n}
\end{equation}
where $n \geq 2$ is a steepness parameter.

Thresholds adapt through reinforcement:
\begin{equation}\label{me:eq:threshold_dynamics}
\dot{\theta}_{ij} = -\xi \cdot \mathbf{1}[\text{$i$ performs task $j$}] + \varphi_t \cdot \mathbf{1}[\text{$i$ does not perform task $j$}]
\end{equation}
where $\xi > 0$ is the reinforcement rate (performing a task lowers the threshold, increasing future responsiveness) and $\varphi_t > 0$ is the decay rate (not performing a task raises the threshold).

\begin{proposition}[Emergent Specialization]\label{me:prop:specialization}
Under the threshold dynamics~\eqref{me:eq:threshold_dynamics} with heterogeneous initial thresholds $\theta_{ij}(0)$, agents self-sort into specialist roles: for each agent $i$, there exists $j^*(i) = \argmax_j c_{ij}(t)$ such that $c_{ij^*}(t) \to \bar{c}_i$ and $c_{ij}(t) \to 0$ for $j \neq j^*$ as $t \to \infty$, where $\bar{c}_i$ is agent $i$'s maximum achievable capability.
\end{proposition}

The proof follows from the reinforcement dynamics: an agent that performs task $j$ frequently sees $\theta_{ij}$ decrease, making it more responsive to future demand for $j$, which further increases frequency of performance---a positive feedback loop.  Simultaneously, thresholds for other tasks rise.  The dynamics converge to a fixed point where each agent responds primarily to one task type.  This is the Becker-Murphy division of labor emerging from local interactions without central coordination.

\begin{remark}[Connection to Potts Crystallization]\label{me:rem:potts_connection}
The specialization dynamics of Proposition~\ref{me:prop:specialization} are the micro-level mechanism underlying the Potts model crystallization of Proposition~\ref{me:prop:firstorder}.  The Potts ``state'' of each node is its dominant specialization $j^*(i)$.  The ``coupling'' $J_c$ in the Potts Hamiltonian corresponds to the task-sharing benefit between neighboring agents with compatible specializations.  The first-order crystallization at $q > 2$ means that when the number of specialization types exceeds two, the transition from unspecialized to specialized is abrupt---consistent with the reinforcement dynamics exhibiting a bifurcation from mixed to specialized response profiles.
\end{remark}


% -------------------------------------------------------------------
% 5. KNOWLEDGE DIFFUSION
% -------------------------------------------------------------------
\section{Knowledge Diffusion}\label{me:sec:diffusion}

\subsection{Laplacian Dynamics}\label{me:sec:laplacian}

Let $\mathbf{u}(t) \in \mathbb{R}^N$ represent the knowledge state of each node (e.g., model weights, fine-tuning updates, or capability parameters).  Knowledge diffusion on the network follows:
\begin{equation}\label{me:eq:diffusion}
\frac{\partial \mathbf{u}}{\partial t} = -L \cdot \mathbf{u}
\end{equation}
where $L = D_{\text{deg}} - A$ is the graph Laplacian, with $D_{\text{deg}}$ the diagonal degree matrix and $A$ the adjacency matrix.  The convergence rate to the consensus state is governed by $\lambda_2(L)$, the Fiedler eigenvalue (algebraic connectivity).

\begin{lemma}[Convergence Rate]\label{me:lem:fiedler}
The knowledge state $\mathbf{u}(t)$ converges to the average $\bar{u} = N^{-1}\sum_i u_i(0)$ at rate $\lambda_2(L)$:
\begin{equation}
\|\mathbf{u}(t) - \bar{u}\mathbf{1}\|_2 \leq \|\mathbf{u}(0) - \bar{u}\mathbf{1}\|_2 \cdot e^{-\lambda_2(L) t}
\end{equation}
For connected graphs, $\lambda_2(L) > 0$.
\end{lemma}


\subsection{Bandwidth Scaling}\label{me:sec:bandwidth}

The total bandwidth available for knowledge diffusion in the mesh scales as $B_{\text{mesh}} = O(N \cdot \langle k \rangle)$, where $\langle k \rangle$ is the mean degree.  The centralized hub has fixed bandwidth $B_{\text{hub}}$ determined by datacenter interconnect capacity.  Once:
\begin{equation}\label{me:eq:bandwidth}
N \cdot \langle k \rangle > B_{\text{hub}}
\end{equation}
the mesh serves more total queries per unit time than the centralized provider.


\subsection{Vanishing Epidemic Threshold on Scale-Free Networks}\label{me:sec:epidemic}

Pastor-Satorras and Vespignani~\cite{me:pastor2001} established that the SIS epidemic threshold on networks with degree distribution $P(k) \sim k^{-\gamma}$ is:
\begin{equation}\label{me:eq:epidemic}
\lambda_c = \frac{\langle k \rangle}{\langle k^2 \rangle}
\end{equation}

For scale-free networks with $\gamma \leq 3$: $\langle k^2 \rangle$ diverges in the thermodynamic limit, so $\lambda_c \to 0$.

\begin{proposition}[Self-Sustaining Knowledge Propagation]\label{me:prop:epidemic}
If the mesh has a scale-free degree distribution with $\gamma \leq 3$---which MoE routing produces endogenously through preferential specialization---then any nonzero rate of knowledge sharing sustains itself indefinitely.  The topology ensures propagation without requiring a minimum transmission rate.
\end{proposition}

The economic content is that knowledge diffusion need not be modeled as a separate mechanism with its own threshold.  Once the mesh achieves a fat-tailed degree distribution (which the specialization dynamics of Section~\ref{me:sec:FRT} produce through preferential attachment to high-quality specialists), capability propagation is guaranteed.  Hub agents---the most capable specialists---emerge endogenously and serve as conduits for knowledge transfer.


\subsection{Combined Dynamics}\label{me:sec:combined}

The three layers interact as follows.  Percolation ensures the mesh is connected ($\Sinf > 0$).  CES aggregation ensures agents specialize ($\Ceff$ grows with diversity).  Knowledge diffusion ensures information propagates (the Fiedler eigenvalue is positive, and on scale-free topologies, propagation is self-sustaining).  The combined effect is a mesh whose aggregate capability $\Cmesh(N)$ is superlinear in $N$ over the relevant range, because additional diverse specialists both increase the CES aggregate and increase the rate at which existing knowledge diffuses to new entrants.


% -------------------------------------------------------------------
% 6. THE CENTRAL THEOREM: MESH EQUILIBRIUM
% -------------------------------------------------------------------
\section{The Central Theorem: Mesh Equilibrium}\label{me:sec:meshthm}

\begin{theorem}[Mesh Equilibrium Existence, Uniqueness, and Dominance]\label{me:thm:main}
For $\Rz > 1$ and $\rho < 1$, there exists a finite $N^*$ such that for all $N > N^*$:
\begin{enumerate}[label=(\alph*)]
\item The mesh equilibrium exists: a positive fraction $\Sinf^* > 0$ of agents form a connected component with specialized roles covering all $J$ task types.
\item The mesh equilibrium is unique among equilibria with $\Sinf > 0$.
\item The mesh equilibrium is locally asymptotically stable.
\item $\Cmesh(N) > \Ccent$: the mesh's aggregate capability exceeds centralized provision.
\item $N^*$ is decreasing in the diversity of the agent population, measured by the entropy of the fitness distribution $H(\rho) = -\int \rho(\eta) \ln \rho(\eta)\, d\eta$.
\end{enumerate}
\end{theorem}

\begin{proof}
The proof proceeds in five steps.

\emph{Step 1: Existence via percolation (Proposition~\ref{me:prop:gc}).}  For $\Rz > 1$, equation~\eqref{me:eq:Sinf} has a unique positive solution $\Sinf^*$.  The giant component contains $\Sinf^* \cdot N$ agents.  For $N$ sufficiently large, this exceeds the minimum number of agents required to cover all $J$ task types (at least one specialist per type), establishing existence.

\emph{Step 2: Uniqueness via supermodularity.}  The mesh participation game---in which each agent decides whether to join the mesh and which task type to specialize in---is a supermodular game.  Agent $i$'s payoff from joining increases when more agents join (network effect via the CES aggregate and knowledge diffusion) and when agents specialize in complementary types (CES complementarity with $\rho < 1$).  By Tarski's~\cite{me:tarski1955} fixed point theorem, the game has a greatest and least equilibrium.  By the strict concavity of the CES function in each $C_j$, the greatest equilibrium is unique among equilibria with $\Sinf > 0$: any equilibrium with different specialization allocations is payoff-dominated by the efficient allocation.

\emph{Step 3: Stability via Lyapunov analysis.}  Consider the Lyapunov function $V = -\Ceff(N) + \sum_j \phi_j(C_j)$ where $\phi_j$ is the potential function of the specialization dynamics.  At the equilibrium, $\dot{V} \leq 0$ with equality only at the fixed point, by the dissipative property of the threshold dynamics~\eqref{me:eq:threshold_dynamics} and the concavity of $\Ceff$.  The equilibrium is locally asymptotically stable by LaSalle's invariance principle.

\emph{Step 4: Capability dominance via CES growth.}  The mesh capability is:
\begin{equation}
\Cmesh(N) = \left(\sum_{j=1}^{J} \left(\sum_{i \in \text{mesh}} c_{ij}\right)^{\rho}\right)^{1/\rho}
\end{equation}
Under specialization, each agent $i$ concentrates capability in task $j^*(i)$, so $C_j \approx |\{i : j^*(i) = j\}| \cdot \bar{c}$ where $\bar{c}$ is the mean specialist capability.  If agents distribute approximately uniformly across $J$ types, then $C_j \approx \Sinf^* N \bar{c} / J$ and:
\begin{equation}
\Cmesh(N) \approx J^{(1-\rho)/\rho} \cdot \Sinf^* N \bar{c} / J = J^{1/\rho - 2} \cdot \Sinf^* N \bar{c}
\end{equation}
This exceeds $\Ccent = J^{(1-\rho)/\rho} \cdot M\bar{c}_{\text{cent}}$ when $N > N^* \equiv M\bar{c}_{\text{cent}} / (\Sinf^* \bar{c})$.  Since $M$ and $\bar{c}_{\text{cent}}$ are fixed, $N^*$ is finite.

\emph{Step 5: $N^*$ decreasing in diversity.}  Higher entropy $H(\rho)$ of the fitness distribution implies broader coverage of the capability space for any given $N$.  The CES diversity premium (Lemma~\ref{me:lem:diversity}) increases with the number of effectively distinct specialization types.  Higher diversity means fewer agents are needed to achieve full task coverage, reducing $N^*$.
\end{proof}

\begin{corollary}[Centralized Market Share Decline]\label{me:cor:decline}
For $N > N^*$, the centralized provider's market share is strictly decreasing in $N$.  In the Bianconi-Barab\'{a}si framework, the centralized ``condensate'' fraction declines continuously as the fitness distribution broadens, transitioning from the BEC phase (macroscopic condensate) to the FGR phase (distributed traffic).
\end{corollary}

\begin{remark}[Self-Consistency Across Fields]\label{me:rem:unification}
The critical condition $\Rz = 1$ in Theorem~\ref{me:thm:main} is a universal transcritical bifurcation.  Table~\ref{me:tab:unification} shows the correspondence across fields.  All produce the same self-consistency equation $m = f(m; \lambda)$ with the identical phase structure.
\end{remark}

\begin{table}[htbp]
\centering
\caption{Universal self-consistency across fields.}
\label{me:tab:unification}
\small
\begin{tabular}{@{}lllll@{}}
\toprule
Field & Order parameter $m$ & Control $\lambda$ & Critical condition & Source \\
\midrule
Percolation & Giant component $\Sinf$ & Mean degree $\langle k \rangle$ & $\langle k \rangle = 1$ & Erd\H{o}s-R\'{e}nyi~\cite{me:erdos1960} \\
Epidemiology & Infected fraction & $R_0$ & $R_0 = 1$ & Kermack-McKendrick~\cite{me:kermack1927} \\
Ising/Potts & Magnetization & $\beta_T J_c q$ & $\beta_T J_c q = 1$ & Ising~\cite{me:ising1925}; Potts~\cite{me:potts1952} \\
Ecology & Productivity & Species richness $S$ & Min $S$ for resilience & Loreau-Hector~\cite{me:loreau2001} \\
Network econ. & Market share & Transaction benefit & Critical liquidity & Katz-Shapiro~\cite{me:katz1985} \\
\textbf{This chapter} & \textbf{Mesh fraction $\Sinf$} & $\boldsymbol{\Rz}$ & $\boldsymbol{\Rz = 1}$ & --- \\
\bottomrule
\end{tabular}
\end{table}


% -------------------------------------------------------------------
% 7. POST-CROSSING DYNAMICS
% -------------------------------------------------------------------
\section{Post-Crossing Dynamics}\label{me:sec:postcrossing}

The path from $x(t) = 0$ to mesh dominance is not monotonic.  Three phases, distinguished by the value of $\Rz$ and the state of the specialization structure, characterize the transition.

\subsection{\texorpdfstring{Phase 1: Nucleation ($\Rz \approx 1$)}{Phase 1: Nucleation (R0mesh approx 1)}}\label{me:sec:nucleation}

Immediately after crossing, $\Rz$ is only marginally above unity.  The giant component is small ($\Sinf^* \approx 0$ for $\Rz$ near 1, since $\Sinf^* \sim 2(\Rz - 1)/{\Rz}^2$ to leading order).  Growth is slow and stochastic.  Small specialist clusters form around high-fitness agents---the enthusiast-tier hardware users running quantized models---but the clusters are fragile.  Exogenous shocks (model-release events, API pricing changes, hardware supply disruptions) can temporarily push $\Rz$ below unity, collapsing nascent clusters.

The mesh first achieves capability dominance on the \emph{long tail} of niche queries that centralized systems underserve.  Centralized providers optimize for the highest-volume query types (general chat, code generation, summarization).  Specialized queries---domain-specific technical reasoning, low-resource language translation, real-time edge processing for robotics---are underserved because the revenue per query does not justify dedicated model fine-tuning.  The mesh's heterogeneous agents, each fine-tuned for a niche, collectively cover the long tail.

This is the Christensen~\cite{me:christensen1997} pattern: disruption begins in markets the incumbent rationally ignores.


\subsection{\texorpdfstring{Phase 2: Rapid Growth ($\Rz \gg 1$)}{Phase 2: Rapid Growth (R0mesh >> 1)}}\label{me:sec:rapid}

As additional device types become inference-capable (driven by the continuing packaging learning curve) and coordination infrastructure matures ($\kappa$ declines), $\Rz$ accelerates well above unity.  Network effects dominate.  Each new specialist joining the mesh increases $\Ceff$ superlinearly (by the CES diversity premium) and increases the Fiedler eigenvalue $\lambda_2(L)$ (by adding connectivity), which accelerates knowledge diffusion to subsequent entrants.

In this phase, the Potts crystallization occurs: the division of labor among mesh agents transitions from fragmented proto-specialization to a structured, self-reinforcing configuration.  By Proposition~\ref{me:prop:firstorder}, this transition is first-order for $q > 2$ specialization types.  The crystallization is observable as a sudden increase in the concentration of agent capabilities around distinct specialization types, accompanied by the emergence of routing hub agents that handle disproportionate query traffic.

Centralized providers lose market share in progressively more mainstream query types, beginning with the long-tail niches of Phase~1 and expanding to higher-volume categories as mesh coverage broadens.


\subsection{Phase 3: Maturity}\label{me:sec:maturity}

Growth saturates as the mesh's $J$ task types are fully covered.  The CES aggregate $\Ceff$ approaches its maximum for the given device population.  Competition shifts from mesh growth to mesh composition: which specialists are included, the quality of their fine-tuning, and the efficiency of the routing layer.

Centralized providers retain structural advantage in two domains that the mesh cannot replicate:
\begin{enumerate}[label=(\roman*)]
\item \emph{Frontier model training:} As established in Chapter~4, training requires tightly synchronized GPU clusters at scales incompatible with distributed architecture.  The mesh depends on centralized training for the base models it fine-tunes.
\item \emph{Capabilities requiring single-device scale beyond any edge device:} Tasks requiring the full activation of 1T+ parameter dense models in a single forward pass remain centralized.  This is the inference analog of the training constraint, but applies to a shrinking fraction of queries as MoE architectures reduce the active parameter requirement.
\end{enumerate}

The mature equilibrium is coexistence: centralized providers dominate training and frontier-capability inference; the mesh dominates the long tail, latency-sensitive applications, and the broad middle of the query distribution where specialized, fine-tuned models outperform general-purpose frontier models.

At maturity, Part~I's analysis reaches a terminal state---the mesh exists, is stable, and dominates, but its capability is bounded by the fixed-capability assumption.  Part~II removes that assumption.


% ===================================================================
%
%  PART II: CAPABILITY GROWTH
%
% ===================================================================

\bigskip
\begin{center}
\rule{0.6\textwidth}{0.5pt}\\[6pt]
{\Large\bfseries Part II: Capability Growth}\\[3pt]
{\itshape Endogenous Improvement and the Baumol Bottleneck}\\[6pt]
\rule{0.6\textwidth}{0.5pt}
\end{center}
\bigskip


% -------------------------------------------------------------------
% 8. THE FIXED-CAPABILITY ASSUMPTION RELAXED
% -------------------------------------------------------------------
\section{The Fixed-Capability Assumption Relaxed}\label{me:sec:fixedrelaxed}

Part~I assumes fixed capabilities: each agent's capability vector $\mathbf{c}_i$ redistributes through specialization but does not grow in total magnitude.  The CES aggregate $\Ceff(N)$ increases only by adding diverse agents.  Knowledge diffusion ($\partial \mathbf{u}/\partial t = -L \mathbf{u}$) equalizes existing knowledge across nodes---it does not create new capability.

This section removes that assumption.  Three mechanisms make capability endogenous:
\begin{enumerate}[label=(\roman*)]
\item \emph{Autocatalytic capability growth:} Training agents within the mesh improve other agents, and the improved agents can in turn improve others.
\item \emph{Self-referential learning:} The mesh's operation generates training data---queries, responses, user feedback, inter-agent evaluations---that can be used to improve mesh agents.
\item \emph{Endogenous variety expansion:} The mesh modifies its own composition by spawning new specialization types in response to unmet demand signals.
\end{enumerate}

When $\mathbf{c}_i$ becomes a dynamical variable coupled to the mesh's operation, the central question changes from \emph{whether} the mesh dominates (answered in Part~I) to \emph{how fast} capability grows.  By the timescale separation of the thesis framework (Chapter~3, Theorem~3.1(iv)), Part~I's equilibrium---the network structure, specialization allocation, and degree distribution---provides the slow backdrop against which capability dynamics unfold.  The network is approximately static on the months timescale of capability growth.


% -------------------------------------------------------------------
% 9. THE AUTOCATALYTIC EXISTENCE THRESHOLD
% -------------------------------------------------------------------
\section{The Autocatalytic Existence Threshold}\label{me:sec:autocatalytic}

Does the mesh contain a self-sustaining improvement core---a subset of agents whose training interactions can maintain and improve capabilities given only exogenous base models as external input?

\subsection{Training Operations as a Reaction System}\label{me:sec:training_ops}

\begin{definition}[Training Operation]\label{me:def:training}
A training operation is a tuple $r = (I_r, K_r, O_r)$ where:
\begin{itemize}
\item $I_r \subseteq \{1, \ldots, J\}$ is the set of input capability types consumed or modified;
\item $K_r \subseteq \{1, \ldots, J\}$ is the set of catalyst capability types required to execute the operation (not consumed);
\item $O_r \subseteq \{1, \ldots, J\}$ is the set of output capability types produced or enhanced.
\end{itemize}
\end{definition}

The catalyst distinction is critical.  A training agent that orchestrates fine-tuning of a medical reasoning specialist requires its own orchestration capability ($K_r$) but is not consumed by the process.  It can catalyze multiple training operations.  This is the biochemical analogy that motivates the autocatalytic framework: enzymes catalyze reactions without being consumed.


\subsection{The Food Set}\label{me:sec:food}

\begin{definition}[Food Set]\label{me:def:food}
The food set $F \subset \{1, \ldots, J\}$ is the set of capability types available exogenously from centralized training.  For each $j \in F$, base model capability of type $j$ is available without requiring any mesh training operation.  The food set is determined by frontier model releases from centralized providers and is exogenous to the mesh.
\end{definition}

The food set corresponds to the training persistence assumption from Part~I: frontier model training remains centralized.  Base models (GPT-class, Claude-class, Gemini-class) are the ``raw materials'' that the mesh fine-tunes, adapts, and combines.  The food set grows exogenously at a rate determined by centralized infrastructure investment---the rate that will emerge as the Baumol bottleneck in Section~\ref{me:sec:baumol}.


\subsection{RAF Sets in the Mesh}\label{me:sec:RAF}

\begin{definition}[Reflexively Autocatalytic and Food-generated (RAF) Set]\label{me:def:RAF}
Following Hordijk and Steel~\cite{me:hordijk2004}, a set $\mathcal{R}$ of training operations is a RAF set if:
\begin{enumerate}[label=(\alph*)]
\item \emph{Reflexively Autocatalytic (RA):} Every training operation $r \in \mathcal{R}$ is catalyzed by at least one capability type that is either in the food set $F$ or is produced by some other operation in $\mathcal{R}$.
\item \emph{Food-generated (F):} Every input capability type of every operation $r \in \mathcal{R}$ can be constructed from the food set $F$ by successive application of operations in $\mathcal{R}$.
\end{enumerate}
\end{definition}

A RAF set is self-sustaining: given only exogenous base models (the food set), the mesh can maintain and improve all capabilities involved in the set through its own internal training operations.


\subsection{Existence Threshold}\label{me:sec:existence}

\begin{proposition}[Autocatalytic Existence Threshold]\label{me:prop:RAF}
Let $J(N)$ denote the number of distinct capability types present in a mesh of $N$ agents, and let $p(N) = 1 - (1 - \beta_t)^{N}$ be the probability that a given capability type is available as a catalyst in the mesh, where $\beta_t$ is the per-agent probability of possessing a given catalyst capability.  There exists a critical mesh size $\Nauto$ such that for $N > \Nauto$, the mesh contains a RAF set with probability approaching unity.  Moreover:
\begin{equation}\label{me:eq:Nauto}
\Nauto = O\!\left(\frac{\ln |\mathcal{R}|}{\beta_t}\right)
\end{equation}
where $|\mathcal{R}|$ is the total number of potential training operations.  The threshold $\Nauto$ scales logarithmically with system complexity.
\end{proposition}

\begin{proof}
The result follows from the probabilistic analysis of RAF sets by Hordijk and Steel~\cite{me:hordijk2004} (Theorem~2).  In their framework, a random catalytic reaction system with $n$ molecule types, $r$ reactions, and catalysis probability $p$ per type-reaction pair contains a RAF set with probability approaching 1 when $p$ exceeds $1/n$.  In our setting, $n = J$ capability types and the effective catalysis probability per type is $p(N) = 1 - (1-\beta_t)^N$.

The condition $p(N) > 1/J$ is satisfied when:
\begin{equation}
1 - (1-\beta_t)^N > \frac{1}{J}
\end{equation}
which gives $N > \ln(1 - 1/J) / \ln(1 - \beta_t) \approx (1/J) / \beta_t = 1/(J \beta_t)$ for small $\beta_t$.  Since $J$ grows at most polynomially in the number of potential operations $|\mathcal{R}|$, the threshold scales as $\Nauto = O(\ln |\mathcal{R}| / \beta_t)$.

The logarithmic scaling arises because adding agents to the mesh increases the probability of \emph{every} catalyst assignment simultaneously.  Each new agent with a novel catalyst capability unlocks multiple potential training operations.
\end{proof}

\begin{remark}[Relationship to $N^*$]\label{me:rem:NautoNstar}
The autocatalytic threshold $\Nauto$ and the critical mass $N^*$ from Theorem~\ref{me:thm:main} are distinct.  $N^*$ is the mesh size at which collective capability exceeds centralized provision (a \emph{static} comparison).  $\Nauto$ is the mesh size at which self-sustaining capability improvement becomes possible (a \emph{dynamic} property).  Generically, $\Nauto > N^*$: the mesh can be collectively capable before it is self-improving.  The gap $\Nauto - N^*$ represents the period during which the mesh exceeds centralized inference but depends entirely on exogenous base model releases for capability growth.
\end{remark}


\subsection{Autocatalytic Core Dynamics: The Jain-Krishna Process}\label{me:sec:JK}

Once a RAF set exists, the mesh's autocatalytic core evolves through the adaptive network dynamics of Jain and Krishna~\cite{me:jain1998,me:jain2001}.  Define the catalytic matrix $\mathbf{M}$ where $M_{ij} = 1$ if capability type $j$ catalyzes the improvement of capability type $i$.  The growth rate of capability type $i$ in the mesh is governed by:
\begin{equation}\label{me:eq:JK}
\dot{c}_i = c_i \left(\sum_j M_{ij} c_j - \phi_0\right)
\end{equation}
where $\phi_0 = N^{-1} \sum_i c_i \sum_j M_{ij} c_j$ is a dilution term ensuring bounded growth.

\begin{proposition}[Perron-Frobenius Selection]\label{me:prop:PF}
The long-run composition of the autocatalytic core is determined by the leading eigenvector of the catalytic matrix $\mathbf{M}$.  Capability types with large components in the Perron-Frobenius eigenvector $\mathbf{v}_1$ of $\mathbf{M}$ dominate; types with small components go extinct.  The leading eigenvalue $\lambda_1(\mathbf{M})$ determines whether the autocatalytic core expands or contracts relative to the rest of the mesh.
\end{proposition}

\begin{proof}
Equation~\eqref{me:eq:JK} is a replicator equation on the simplex of capability-type shares.  The fixed point analysis follows from the standard replicator dynamics result \cite{me:hofbauer1998}: the dynamics converge to a state where surviving types have equal fitness, and the surviving set corresponds to the support of the Perron-Frobenius eigenvector of $\mathbf{M}$.  Types $i$ with $v_{1,i} > 0$ persist; types with $v_{1,i} = 0$ go extinct.

The eigenvalue $\lambda_1(\mathbf{M})$ determines the growth rate of the autocatalytic core because the aggregate fitness of the surviving set equals $\lambda_1(\mathbf{M})$.  When $\lambda_1(\mathbf{M}) > \phi_0$, the core expands as a fraction of total mesh activity.
\end{proof}

The Jain-Krishna dynamics predict a specific temporal pattern: the autocatalytic core does not grow smoothly.  Instead, it undergoes a series of reorganization cascades---periods of stasis punctuated by rapid restructuring events in which poorly connected capability types are replaced by types with stronger catalytic linkages.  Each cascade increases the leading eigenvalue $\lambda_1(\mathbf{M})$, producing a staircase pattern of increasing autocatalytic efficiency.


% -------------------------------------------------------------------
% 10. GROWTH DYNAMICS
% -------------------------------------------------------------------
\section{Growth Dynamics: The Central Model}\label{me:sec:growth}

Having established that the mesh contains a self-sustaining improvement core (Section~\ref{me:sec:autocatalytic}), we now characterize the rate at which capability grows.

\subsection{The Improvement Function}\label{me:sec:improvement}

Let $f \in (0,1)$ denote the fraction of mesh capability devoted to self-improvement (training operations) rather than serving external queries (inference).  The mesh's aggregate capability evolves according to:
\begin{equation}\label{me:eq:growth}
\ddt \Ceff = g\!\left(f \cdot \Ceff,\; J(t),\; \alpha(t)\right) - \delta \cdot \Ceff
\end{equation}
where $g$ is the improvement function, $J(t)$ is the number of effective specialization types, $\alpha(t)$ is the fraction of training signal from external sources, and $\delta > 0$ is the depreciation rate capturing model obsolescence.

The improvement function $g$ has three arguments because the three mechanisms of endogenous capability contribute separately:
\begin{enumerate}[label=(\roman*)]
\item $f \cdot \Ceff$: \emph{Autocatalytic training.}  The total capability devoted to self-improvement.  More capable training agents produce better improvements.
\item $J(t)$: \emph{Variety expansion.}  New specialization types expand the capability space.  By the diversity premium (Lemma~\ref{me:lem:diversity}), adding a new task type increases $\Ceff$ superlinearly.
\item $\alpha(t)$: \emph{Data quality.}  The fraction $\alpha$ determines whether self-referential learning improves or degrades capability.  Below the critical threshold $\alphacrit$, training on internally generated data causes model collapse~\cite{me:shumailov2024}.
\end{enumerate}


\subsection{The Semi-Endogenous Growth Formulation}\label{me:sec:semiendogenous}

Following Jones~\cite{me:jones1995,me:jones2005}, specify the improvement function as:
\begin{equation}\label{me:eq:jones}
g(f \cdot \Ceff, J, \alpha) = \delta_g \cdot (f \cdot \Ceff)^{\lambda} \cdot \Ceff^{\varphi - 1} \cdot J^{\gamma_J} \cdot \mathbf{1}[\alpha > \alphacrit]
\end{equation}
where $\delta_g > 0$ is a productivity parameter, $\lambda \in (0,1]$ is the duplication parameter, $\varphi$ is the training productivity elasticity, $\gamma_J > 0$ governs the contribution of variety expansion, and $\mathbf{1}[\alpha > \alphacrit]$ is the model collapse indicator.

Letting $C \equiv \Ceff$ for notational compactness:
\begin{equation}\label{me:eq:growth_expanded}
\dot{C} = \delta_g \cdot f^{\lambda} \cdot C^{\lambda + \varphi - 1} \cdot J(t)^{\gamma_J} \cdot \mathbf{1}[\alpha > \alphacrit] - \delta \cdot C
\end{equation}

The growth rate of capability is:
\begin{equation}\label{me:eq:gC}
g_C \equiv \frac{\dot{C}}{C} = \delta_g \cdot f^{\lambda} \cdot C^{\lambda + \varphi - 2} \cdot J(t)^{\gamma_J} \cdot \mathbf{1}[\alpha > \alphacrit] - \delta
\end{equation}


\subsection{Deriving the Mesh's Effective $\varphi$}\label{me:sec:phieff}

The parameter $\varphi$ measures the elasticity of new capability production with respect to the existing stock.  Empirical evidence strongly suggests $\varphi < 1$ for individual training interactions: ideas are getting harder to find \cite{me:bloom2020}.  The mesh, however, has internal structure that amplifies the effective $\varphi$ through autocatalytic coupling.

\begin{proposition}[Effective Training Productivity]\label{me:prop:phieff}
Let $\varphi_0 < 1$ be the raw training productivity elasticity for a single training interaction.  Let $\beta_{\emph{auto}} \in [0,1)$ be the fraction of the training improvement process that can be automated by the mesh's own training agents.  Then the mesh's effective training productivity elasticity is:
\begin{equation}\label{me:eq:phieff}
\phieff = \frac{\varphi_0}{1 - \beta_{\emph{auto}} \cdot \varphi_0}
\end{equation}
This exceeds $\varphi_0$ for all $\beta_{\emph{auto}} > 0$, and $\phieff \geq 1$ when $\beta_{\emph{auto}} \geq (1 - \varphi_0)/\varphi_0$.
\end{proposition}

\begin{proof}
Following the Aghion-Jones-Jones~\cite{me:aghion2018} framework, decompose the training improvement process into a continuum of subtasks indexed on $[0,1]$.  A fraction $\beta_{\text{auto}}$ of subtasks are automated by mesh agents (whose productivity scales with $C^{\varphi_0}$), and the remaining fraction $1 - \beta_{\text{auto}}$ requires exogenous input.  The effective production function for capability improvement is:
\begin{equation}
\dot{C} \propto C^{\varphi_0/(1 - \beta_{\text{auto}} \cdot \varphi_0)} \cdot Z^{(1-\beta_{\text{auto}})/(1 - \beta_{\text{auto}} \cdot \varphi_0)}
\end{equation}
where $Z$ is the exogenous input (frontier model releases).  The exponent on $C$ is $\varphi_0/(1 - \beta_{\text{auto}} \varphi_0) = \phieff$.

The threshold condition $\phieff = 1$ requires $\beta_{\text{auto}} = (1 - \varphi_0)/\varphi_0$.  For example, with $\varphi_0 = 0.5$, the knife-edge requires $\beta_{\text{auto}} = 1.0$---full automation, which contradicts training persistence.  With $\varphi_0 = 0.8$, the threshold is $\beta_{\text{auto}} = 0.25$.
\end{proof}

\begin{remark}[The Automation Ladder]\label{me:rem:ladder}
The quantity $\beta_{\text{auto}}$ is not fixed; it evolves endogenously as the mesh's autocatalytic core matures.  Initially $\beta_{\text{auto}} \approx 0$: the mesh cannot automate any part of its own training improvement.  As training agents emerge and improve (the Jain-Krishna process of Section~\ref{me:sec:JK}), $\beta_{\text{auto}}$ rises.  The growth dynamics are therefore non-stationary: $\phieff(t) = \varphi_0 / (1 - \beta_{\text{auto}}(t) \cdot \varphi_0)$ increases over time, potentially transitioning the system from convergence to exponential growth.  The transition is not guaranteed; it depends on whether $\beta_{\text{auto}}$ can reach the threshold before the Baumol bottleneck binds (Section~\ref{me:sec:baumol}).
\end{remark}


\subsection{Training Saturation}\label{me:sec:saturation}

Individual training interactions exhibit diminishing returns.  Following the Lotka-Volterra mutualistic framework \cite{me:bastolla2005}, the saturating interaction modifies the growth equation:
\begin{equation}\label{me:eq:saturation}
\dot{C}_j = \frac{\sum_k a_{jk} \cdot C_k}{1 + h \sum_k a_{jk} \cdot C_k} - \delta C_j
\end{equation}
where $a_{jk}$ is the training benefit from type-$k$ agents to type-$j$ agents, and $h > 0$ is the saturation parameter.

\begin{lemma}[Saturation Ceiling]\label{me:lem:saturation}
For fixed $J$ and $h > 0$, the system~\eqref{me:eq:saturation} has a unique globally stable equilibrium with $C_j^* < 1/(h \cdot \delta)$ for all $j$.  The aggregate ceiling is:
\begin{equation}
\Cmax = \left(\sum_{j=1}^J \left(C_j^*\right)^\rho\right)^{1/\rho} \leq J^{1/\rho} \cdot \frac{1}{h \delta}
\end{equation}
\end{lemma}

\begin{proof}
At steady state, $\dot{C}_j = 0$ implies $\sum_k a_{jk} C_k / (1 + h \sum_k a_{jk} C_k) = \delta C_j$.  The left side is bounded above by $1/h$ for any $C_k \geq 0$, so $C_j^* \leq 1/(h\delta)$.  The CES bound follows from substitution and the power mean inequality.  For global stability, the system is cooperative (all off-diagonal Jacobian elements are non-negative) and bounded, so by the Hirsch~\cite{me:hirsch1985} monotone dynamical systems result, the system has a unique globally attracting equilibrium.
\end{proof}


\subsection{Variety Expansion as Saturation Escape}\label{me:sec:variety}

Romer's~\cite{me:romer1990} key insight is that new product varieties can sustain growth even when returns to individual products diminish.  The mesh analog: when fine-tuning existing specialists hits saturation ($h > 0$), the mesh can create \emph{new} specialization types.

Let $J(t)$ evolve according to:
\begin{equation}\label{me:eq:variety}
\dot{J} = \eta_J \cdot f_J \cdot \Ceff^{\varphi_J} \cdot (J_{\max} - J) \cdot \mathbf{1}[\alpha > \alphacrit]
\end{equation}
where $\eta_J > 0$ is the innovation rate, $f_J$ is the fraction of training capability devoted to creating new specialization types, $\varphi_J$ is the capability elasticity of variety creation, and $J_{\max}$ is the maximum number of viable specialization types.

\begin{proposition}[Saturation Escape via Variety]\label{me:prop:escape}
Even with training saturation $h > 0$, the growth rate of $\Ceff$ can remain positive if $J(t)$ is growing.  Specifically, for constant per-type capability $C_j^*$ at the saturation ceiling:
\begin{equation}
\frac{d \Ceff}{dt} = \frac{1-\rho}{\rho} \cdot \frac{\Ceff}{J} \cdot \dot{J} \cdot \left(\frac{(C_{J+1}^*)^\rho}{J^{-1}\sum_j (C_j^*)^\rho}\right)
\end{equation}
which is positive whenever a new type with $C_{J+1}^* > 0$ is created.  The growth rate from variety expansion does not depend on $h$.
\end{proposition}

\begin{proof}
Differentiating $\Ceff = (\sum_j C_j^\rho)^{1/\rho}$ with respect to $J$, treating $J$ as continuous:
\begin{equation}
\frac{\partial \Ceff}{\partial J} = \frac{1}{\rho} \left(\sum_j C_j^\rho\right)^{1/\rho - 1} \cdot (C_{J+1})^\rho
\end{equation}
Since the saturation ceiling applies per type but not to the aggregate, variety expansion circumvents saturation.  The aggregate grows as $J^{(1-\rho)/\rho}$ even when each individual $C_j$ is bounded.
\end{proof}


% -------------------------------------------------------------------
% 11. THE THREE GROWTH REGIMES
% -------------------------------------------------------------------
\section{The Central Theorem: Growth Regimes}\label{me:sec:regimes}

We now unify the three layers into a complete characterization of the mesh's growth dynamics.

\begin{definition}[Growth Regimes]\label{me:def:regimes}
Define $\Phi \equiv \lambda + \varphi - 1$ as the composite capability elasticity.  The growth dynamics~\eqref{me:eq:growth_expanded} exhibit three regimes:
\begin{enumerate}[label=(\alph*)]
\item \textbf{Convergence} ($\Phi < 1$, i.e.\ $\lambda + \varphi < 2$): The growth rate $g_C$ is decreasing in $C$.  The system converges to a steady state $C^*$ where $g_C = 0$.  This includes the Jones~\cite{me:jones1995} semi-endogenous case where $\varphi < 1$.
\item \textbf{Exponential growth} ($\Phi = 1$, i.e.\ $\lambda + \varphi = 2$): The growth rate $g_C$ is independent of $C$.  This is the Romer~\cite{me:romer1990} knife-edge.
\item \textbf{Superexponential growth} ($\Phi > 1$, i.e.\ $\lambda + \varphi > 2$): The growth rate $g_C$ is increasing in $C$.  The system exhibits a finite-time singularity: $C(t) \to \infty$ as $t \to T_s < \infty$.
\end{enumerate}
\end{definition}

\begin{theorem}[Growth Regime Classification]\label{me:thm:regimes}
Consider the mesh growth system defined by equations~\eqref{me:eq:growth_expanded}, \eqref{me:eq:phieff}, \eqref{me:eq:saturation}, \eqref{me:eq:variety}, and~\eqref{me:eq:alphaeff}, with the autocatalytic core existing for $N > \Nauto$ (Proposition~\ref{me:prop:RAF}).  The long-run behavior of $\Ceff(t)$ falls into one of three regimes:

\medskip

\textbf{Regime (a): Convergence to a ceiling.}  If $\phieff < 1$ (equivalently, $\beta_{\emph{auto}} < (1 - \varphi_0)/\varphi_0$), training saturation $h > 0$, and variety is bounded ($J \leq J_{\max} < \infty$), then:
\begin{equation}
\Ceff(t) \to \Cmax \equiv J_{\max}^{(1-\rho)/\rho} \cdot \frac{1}{h\delta} \quad \text{as } t \to \infty
\end{equation}
The convergence rate is governed by $1 - \phieff$.  The ceiling $\Cmax$ is increasing in $J_{\max}$, decreasing in $h$, and decreasing in $\delta$.  In this regime, the mesh's long-run growth rate equals the growth rate of the exogenous food set (frontier model releases).  \emph{This is the Baumol bottleneck.}

\medskip

\textbf{Regime (b): Balanced exponential growth.}  If $\phieff = 1$ and $J(t)$ grows endogenously at rate $g_J > 0$, then:
\begin{equation}
\Ceff(t) \sim \Ceff(0) \cdot \exp\!\left[\left(\delta_g f^{\lambda} J_0^{\gamma_J} e^{\gamma_J g_J t} - \delta\right) t\right]
\end{equation}
Even in the exponential regime, the long-run growth rate is bounded by $g_Z / (1 - \beta_{\emph{auto}})$ where $g_Z$ is the growth rate of frontier model capability.

\medskip

\textbf{Regime (c): Finite-time singularity.}  If $\phieff > 1$, $h = 0$ (no training saturation), and $\alphaeff > \alphacrit$ (model collapse avoided), then:
\begin{equation}
\Ceff(t) = \left(C_0^{1-\Phi} - (1-\Phi) \cdot \delta_g f^{\lambda} J^{\gamma_J} \cdot t\right)^{1/(1-\Phi)}
\end{equation}
where $\Phi = \lambda + \phieff - 1 > 1$.  This diverges at finite time $T_s = C_0^{1-\Phi} / [(\Phi - 1) \cdot \delta_g f^{\lambda} J^{\gamma_J}]$.  The singularity requires three conditions simultaneously---each individually demanding and collectively unlikely.
\end{theorem}

\begin{proof}
\emph{Regime (a):}  With $\Phi < 1$, the growth rate $g_C = \delta_g f^\lambda C^{\Phi - 1} J^{\gamma_J} - \delta$ is decreasing in $C$.  The unique steady state $C^*$ satisfies $\delta_g f^\lambda (C^*)^{\Phi-1} J^{\gamma_J} = \delta$.  With training saturation $h > 0$, each $C_j \leq 1/(h\delta)$ (Lemma~\ref{me:lem:saturation}), and with bounded $J \leq J_{\max}$, the ceiling $\Cmax$ is finite.  Global stability follows from the monotone dynamical systems theorem~\cite{me:hirsch1985}.

\emph{Regime (b):}  With $\Phi = 1$, the growth equation becomes $\dot{C} = \delta_g f^\lambda J(t)^{\gamma_J} - \delta C$, a linear ODE with time-varying coefficients.  The solution is exponential with growth rate modulated by $J(t)$.  The Baumol constraint (Section~\ref{me:sec:baumol}) eventually binds: the non-automated fraction of training requires exogenous input $Z$ growing at rate $g_Z$, bounding long-run growth.

\emph{Regime (c):}  With $\Phi > 1$ and $h = 0$, the ODE $\dot{C} = \delta_g f^\lambda C^{\Phi} J^{\gamma_J}$ is a Bernoulli equation.  The substitution $v = C^{1-\Phi}$ yields $\dot{v} = (1-\Phi)\delta_g f^\lambda J^{\gamma_J}$, integrating to $v(t) = v(0) + (1-\Phi)\delta_g f^\lambda J^{\gamma_J} t$.  Since $1 - \Phi < 0$, $v$ decreases linearly to zero at $T_s$.
\end{proof}

\begin{table}[htbp]
\centering
\caption{Growth regime classification.}
\label{me:tab:regimes}
\small
\begin{tabular}{@{}lcccl@{}}
\toprule
Regime & $\phieff$ & $h$ & $J$ & Long-run $\Ceff(t)$ \\
\midrule
(a) Convergence & $< 1$ & $> 0$ & bounded & $\to \Cmax$ (ceiling) \\
(b) Exponential & $= 1$ & $\geq 0$ & growing & $\sim e^{rt}$ \\
(c) Singularity & $> 1$ & $= 0$ & any & $\to \infty$ at $T_s < \infty$ \\
\midrule
\multicolumn{5}{@{}l@{}}{\emph{Additional condition for all regimes:} $\alphaeff > \alphacrit$ (collapse avoided)} \\
\bottomrule
\end{tabular}
\end{table}

\begin{remark}[Which Regime Is Likely?]\label{me:rem:likely}
The empirical evidence strongly favors regime~(a) in the near term.  Bloom et al.~\cite{me:bloom2020} estimate $\varphi \approx 0.5$--$0.7$ for research productivity, implying $\varphi_0 < 1$ by a substantial margin.  For $\phieff$ to reach unity with $\varphi_0 = 0.6$, the autocatalytic fraction must reach $\beta_{\text{auto}} = 0.67$---the mesh must automate two-thirds of its own training improvement.  Individual training interactions exhibit clear saturation ($h > 0$).  And while variety expansion can escape per-type saturation, the total task space $J_{\max}$ is finite.

The most probable trajectory is: regime~(a) with an increasing ceiling.  As the mesh matures, $\beta_{\text{auto}}$ rises (pushing $\phieff$ toward 1), $J$ expands (raising $\Cmax$), and the ceiling lifts---but never vanishes entirely, because the Baumol bottleneck anchors the long-run growth rate to exogenous frontier model improvement.
\end{remark}


% -------------------------------------------------------------------
% 12. DIVERSITY AS COLLAPSE PROTECTION
% -------------------------------------------------------------------
\section{Diversity as Collapse Protection}\label{me:sec:collapse}

The mesh's self-referential learning---agents training on data generated by other agents---risks model collapse.  This section proves that the mesh's CES heterogeneity maintains training signal quality, connecting the production-theoretic parameter $\rho$ to information-theoretic robustness.  The underlying mechanism is the correlation robustness established in Chapter~2 (Theorem~5.1): the CES nonlinearity extracts idiosyncratic variation that linear aggregates miss.


\subsection{The Model Collapse Framework}\label{me:sec:collapse_framework}

Following Shumailov et al.~\cite{me:shumailov2024}, consider a generative model $Q_t$ trained on a mixture of authentic data from the true distribution $P$ (fraction $\alpha$) and synthetic data from the model's own previous generation $Q_{t-1}$ (fraction $1 - \alpha$).  The KL divergence from the true distribution evolves as:
\begin{equation}\label{me:eq:collapse}
\text{KL}(Q_{t+1} \| P) \geq \text{KL}(Q_t \| P) \quad \text{when } \alpha < \alphacrit
\end{equation}
For a single model training on its own outputs ($\alpha = 0$), collapse is inevitable.


\subsection{Effective Data Diversity in the Mesh}\label{me:sec:data_diversity}

The mesh is not a single model.  It is a collection of heterogeneous specialists whose outputs are drawn from different distributions.  From the perspective of agent $k \neq i$, training data from agent $i$ is not ``self-generated''---it comes from a different distribution.

\begin{definition}[Effective External Data Fraction]\label{me:def:alphaeff}
For agent $i$ in a mesh with $J$ specialization types and CES parameter $\rho$, the effective external data fraction is:
\begin{equation}\label{me:eq:alphaeff}
\alphaeff(\rho, J) = \alpha_{\text{ext}} + (1 - \alpha_{\text{ext}}) \cdot D(\rho, J)
\end{equation}
where $\alpha_{\text{ext}}$ is the fraction of training data from sources outside the mesh and $D(\rho, J)$ is the diversity correction measuring the informational diversity of mesh-internal training data.
\end{definition}


\subsection{The Diversity Correction}\label{me:sec:diversity_correction}

\begin{lemma}[Specialization Implies Distributional Diversity]\label{me:lem:diversity_info}
Let agents be fully specialized: agent $i$ produces outputs entirely from its specialization type $j^*(i)$.  If the $J$ specialization types are distributed uniformly across the mesh, the expected diversity correction is:
\begin{equation}\label{me:eq:D}
D(\rho, J) = \frac{J - 1}{J} \cdot \left(1 - \rho^{1/(1-\rho)}\right)
\end{equation}
For $\rho < 1$: $D > 0$, and $D$ is increasing in $J$ and decreasing in $\rho$.
\end{lemma}

\begin{proof}
When agents are fully specialized, agent $i$'s output distribution $Q_i$ has support concentrated on task type $j^*(i)$.  For agent $k$ with $j^*(k) \neq j^*(i)$, the supports are disjoint, giving maximal Jensen-Shannon divergence.  The fraction of other agents with different specializations is $(J-1)/J$ under uniform distribution.  The term $(1 - \rho^{1/(1-\rho)})$ quantifies how the CES substitution parameter translates production complementarity into distributional diversity.
\end{proof}


\subsection{CES Heterogeneity as Collapse Protection}\label{me:sec:collapse_theorem}

\begin{theorem}[CES Heterogeneity as Collapse Protection]\label{me:thm:collapse}
Let $\alpha_{\emph{ext}} \geq 0$ be the exogenous external data fraction and let $\rho < 1$ be the CES substitution parameter.  The mesh avoids model collapse ($\alphaeff > \alphacrit$) whenever:
\begin{equation}\label{me:eq:collapse_condition}
\alpha_{\emph{ext}} + (1 - \alpha_{\emph{ext}}) \cdot \frac{J-1}{J} \cdot \left(1 - \rho^{1/(1-\rho)}\right) > \alphacrit
\end{equation}
This condition can be satisfied even when $\alpha_{\emph{ext}} < \alphacrit$---even when the mesh's external data supply is below the collapse threshold for any individual model---provided $J$ is sufficiently large and $\rho$ is sufficiently small.
\end{theorem}

\begin{proof}
Substituting equation~\eqref{me:eq:D} into equation~\eqref{me:eq:alphaeff} gives condition~\eqref{me:eq:collapse_condition} directly.  The condition $\alphaeff > \alphacrit$ holds when:
\begin{equation}
D(\rho, J) > \frac{\alphacrit - \alpha_{\text{ext}}}{1 - \alpha_{\text{ext}}}
\end{equation}
The right side is positive only when $\alpha_{\text{ext}} < \alphacrit$.  The left side $D(\rho, J)$ increases as $\rho$ decreases and $J$ increases.  The minimum $J$ required for collapse avoidance is:
\begin{equation}\label{me:eq:Jmin}
J_{\min}(\rho, \alpha_{\text{ext}}) = \left\lceil \frac{1}{1 - \frac{\alphacrit - \alpha_{\text{ext}}}{(1-\alpha_{\text{ext}})(1 - \rho^{1/(1-\rho)})}} \right\rceil
\end{equation}
This is finite for $\rho < 1$ and decreasing in $\rho$.
\end{proof}

\begin{remark}[The Triple Role in Action]\label{me:rem:triple_role}
The CES parameter $\rho$ now instantiates two of the three roles identified in Chapter~2 (Theorem~7.1).  In Part~I, $\rho < 1$ generates the \emph{superadditivity} (diversity premium): heterogeneous specialists collectively outperform centralized provision (Lemma~\ref{me:lem:diversity}).  In Part~II, $\rho < 1$ generates \emph{correlation robustness} (collapse protection): heterogeneous specialists generate informationally diverse training data that prevents model collapse even when external data is scarce (Theorem~\ref{me:thm:collapse}).  The same curvature parameter $K$ that quantifies the production benefit of diversity also quantifies the informational benefit.  This is not a coincidence; it is the CES triple role at work.
\end{remark}


% -------------------------------------------------------------------
% 13. THE BAUMOL BOTTLENECK
% -------------------------------------------------------------------
\section{The Baumol Bottleneck}\label{me:sec:baumol}

The training persistence assumption---frontier model training remains centralized---is not imposed exogenously in Part~II.  It emerges from the growth dynamics as a Baumol~\cite{me:baumol1967} cost disease.  This result instantiates the hierarchical ceiling from Chapter~3 (Proposition~8.1): Level~3 (Capability) is bounded by the rate of change at Level~1 (Hardware), mediated through Level~2 (Network).


\subsection{The Two-Sector Structure}\label{me:sec:twosector}

\textbf{Sector 1: Inference and fine-tuning} (progressively automated).  The mesh automates an increasing fraction $\beta(t)$ of inference and fine-tuning tasks.  The mesh's productivity in this sector grows at rate $g_C$.

\textbf{Sector 2: Frontier model training} (non-automatable).  Training frontier models requires tightly synchronized GPU clusters at scales of $10^{25}$+ FLOPs per run, with synchronization bandwidth as the binding constraint (Chapter~4).  Frontier training productivity grows at exogenous rate $g_Z$.


\subsection{Derivation}\label{me:sec:baumol_derivation}

Following Aghion, Jones and Jones~\cite{me:aghion2018}, model the aggregate AI capability as a Cobb-Douglas composite:
\begin{equation}\label{me:eq:baumol}
\Ceff = C_1^{\beta(t)} \cdot C_2^{1 - \beta(t)}
\end{equation}
where $C_1$ is the mesh's inference/fine-tuning capability (growing at $g_C$) and $C_2$ is frontier training capability (growing at exogenous rate $g_Z$).

\begin{proposition}[Endogenous Baumol Bottleneck]\label{me:prop:baumol}
As $\beta(t) \to 1$, the growth rate $g_{\Ceff} \to g_Z$ regardless of $g_C$, provided $g_C > g_Z$.  The non-automatable sector becomes the binding constraint even as its share of total activity shrinks.
\end{proposition}

\begin{proof}
The growth rate of the aggregate is:
\begin{equation}\label{me:eq:baumol_growth}
g_{\Ceff} = \beta(t) \cdot g_C + (1 - \beta(t)) \cdot g_Z + \dot{\beta}(t) \cdot \ln\!\left(\frac{C_1}{C_2}\right)
\end{equation}
When $g_C > g_Z$, the relative price of frontier training rises: its cost share increases even as its volume share $(1-\beta)$ falls---Baumol's cost disease.  In the limit $\beta \to 1$:
\begin{equation}
\lim_{\beta \to 1} g_{\Ceff} = g_Z + \lim_{\beta \to 1} \dot{\beta} \cdot \ln(C_1/C_2)
\end{equation}
If $\dot{\beta} \to 0$ as $\beta \to 1$ (the last tasks are hardest to automate), then $g_{\Ceff} \to g_Z$.
\end{proof}


\subsection{Closing the Circle}\label{me:sec:circle}

The Baumol bottleneck connects this chapter back to Chapter~4.  The chain of determination is:
\begin{enumerate}[label=(\roman*)]
\item \emph{Concentrated investment} (Chapter~4): Datacenter capital investment finances the GPU clusters that train frontier models.  The rate $g_Z$ is determined by the rate of investment.
\item \emph{Learning curves} (Chapter~4): The same investment finances the packaging learning curves ($\alpha = 0.23$) that enable distributed inference.
\item \emph{Mesh formation} (this chapter, Part~I): After crossing, the mesh self-organizes into a specialized network exceeding centralized provision at $N > N^*$.
\item \emph{Endogenous growth} (this chapter, Part~II): The mesh improves itself through autocatalytic training, self-referential learning, and variety expansion.
\item \emph{Baumol ceiling} (this chapter): The mesh's growth rate converges to $g_Z$---the rate determined by the concentrated investment of step~(i).
\end{enumerate}

The circle closes.  The concentrated capital that creates the crossing also determines the ceiling.  The mesh amplifies frontier model improvement but cannot exceed it indefinitely.  This is the Baumol bottleneck instantiated as the hierarchical ceiling (Chapter~3, Proposition~8.1): capability is bounded by the network, which is bounded by hardware.


% -------------------------------------------------------------------
% 14. TRANSITION TO SETTLEMENT
% -------------------------------------------------------------------
\section{Transition to Settlement}\label{me:sec:settlement}

The mesh's routing and compensation requirements endogenously generate the need for a programmable settlement layer.  Consider agent $A$ receiving a query for which agent $B$ is the best specialist.  Three requirements arise:
\begin{enumerate}[label=(\roman*)]
\item \emph{Discovery:} $A$ must identify $B$ as the appropriate specialist.
\item \emph{Incentive compatibility:} $B$ must be compensated for serving $A$'s query.
\item \emph{Settlement:} The compensation must be transferred peer-to-peer, programmably, and at low latency.
\end{enumerate}

\begin{proposition}[Settlement Layer Necessity]\label{me:prop:settlement}
Any mesh equilibrium with $N > N^*$ and $\Cmesh > \Ccent$ requires a settlement layer capable of processing $O(N \cdot \langle k \rangle)$ transactions per second at $O(1)$~ms latency between arbitrary node pairs.
\end{proposition}

With endogenous capability growth (Part~II), the settlement layer must additionally process the micro-transactions of the autocatalytic loop: training agent compensation, data marketplace transactions, and variety expansion incentives.  The transaction volume from autocatalytic operations scales as $O(|\mathcal{R}| \cdot f \cdot N)$, where $|\mathcal{R}|$ is the size of the RAF set.

The price system in the mesh plays exactly the role Hayek~\cite{me:hayek1945} described: it aggregates dispersed information into sufficient statistics for decentralized decision-making.  The bid-ask spread between agents encodes current demand for each query type, current supply of each specialization, and optimal routing.  No central coordinator computes these allocations.

The detailed analysis of how existing and emerging monetary infrastructure maps to these requirements---and the monetary policy consequences of mesh-scale settlement demand---is the subject of Chapter~6.  The mesh's capability growth rate may be constrained by settlement infrastructure before it is constrained by training productivity, saturation, or data quality.  In this case, the binding constraint on mesh improvement is \emph{monetary}, not technological.


% -------------------------------------------------------------------
% 15. FRAMEWORKS CONSIDERED AND REJECTED
% -------------------------------------------------------------------
\section{Frameworks Considered and Rejected}\label{me:sec:rejected}

Several candidate frameworks were evaluated for the formal model and rejected for specific technical reasons.

\textbf{Mean Field Games (Lasry-Lions~\cite{me:lasry2007}).}  MFG assumes a continuum of exchangeable (identical) agents whose individual optimization depends on the population distribution.  The mesh's agents are heterogeneous specialists---heterogeneity is the source of the CES diversity premium that drives Theorem~\ref{me:thm:main}.  Replacing heterogeneous agents with a continuum of identical agents eliminates the mechanism.  The supermodular game framework (Topkis~\cite{me:topkis1998}; Milgrom-Roberts~\cite{me:milgrom1990}) handles heterogeneity naturally.

\textbf{Spin Glasses (Edwards-Anderson~\cite{me:ea1975}; Sherrington-Kirkpatrick~\cite{me:SK1975}).}  Spin glass models require frustrated interactions---a mix of positive and negative couplings.  In the mesh, all interactions are positive: each agent benefits from others joining the network and from others specializing in complementary tasks.  There is no frustration.  The appropriate model is the random-field Ising/Potts model (positive couplings, heterogeneous external fields).

\textbf{Ecological Niche Models (Tilman~\cite{me:tilman1982}; Loreau-Hector~\cite{me:loreau2001}).}  The conceptual analogy is precise: diverse specialist communities outperform monocultures.  The formal ecological models, however, are calibrated to plant biomass dynamics with resource-competition mechanics that do not transfer.  The CES aggregation function captures the identical qualitative result while being native to the economics literature.

\textbf{Immune System / Clonal Selection (Burnet~\cite{me:burnet1959}).}  The adaptive immune system provides a vivid metaphor: a diverse repertoire of specialized agents that collectively cover a vast space through local adaptation.  However, the formal ODE models are calibrated to lymphocyte population dynamics with no meaningful economic analog.

\textbf{Eigen's Hypercycle (Eigen \& Schuster~\cite{me:eigen1977}).}  The hypercycle imposes a conservation law: $\sum_i x_i = \text{const}$ (total concentration is fixed).  This forces zero-sum dynamics.  The mesh is an open system where total capability can grow.  The RAF framework (Hordijk \& Steel~\cite{me:hordijk2004}) provides autocatalytic structure without the conservation constraint.

\textbf{Chemical Reaction Network Theory (Feinberg~\cite{me:feinberg2019}).}  CRNT assumes closed systems with stoichiometric conservation laws.  The mesh is open.  Moreover, CRNT characterizes equilibrium existence, not growth trajectories---the central question of Part~II.

\textbf{NK Fitness Landscapes (Kauffman~\cite{me:kauffman1993}).}  The NK model lacks analytical results on convergence or divergence.  The co-evolutionary extension (NKC model) is an open problem.  Useful as motivation for variety expansion, but not as formal machinery.


% -------------------------------------------------------------------
% 16. FALSIFIABLE PREDICTIONS
% -------------------------------------------------------------------
\section{Falsifiable Predictions}\label{me:sec:predictions}

The model generates twelve predictions with timing and failure conditions.  Predictions~1--6 concern network formation (Part~I); Predictions~7--12 concern capability growth (Part~II).

\medskip
\noindent\textbf{Part~I: Network Formation}

\medskip
\textbf{Prediction 1: First-Order Crystallization, Not Gradual Adoption.}  Mesh formation exhibits a discontinuous jump in adoption metrics rather than smooth logistic growth.  The transition from $<$5\% to $>$25\% distributed inference share occurs within 18 months.  \emph{Timing:} 2030--2033.  \emph{Evidence against:} distributed inference share growing smoothly at $<$5 percentage points per year through 2035.

\textbf{Prediction 2: Specialization Precedes Generalization.}  Early mesh agents are narrow specialists (fine-tuned for specific domains).  General-purpose mesh capability emerges only after the specialization structure crystallizes.  \emph{Evidence against:} early mesh participants predominantly running general-purpose base models without fine-tuning.

\textbf{Prediction 3: Long-Tail Niche Dominance First.}  The mesh achieves capability dominance first on long-tail queries before competing on mainstream tasks.  Distributed inference share should exceed 50\% for long-tail categories while remaining below 20\% for high-volume mainstream categories.  \emph{Timing:} within 2 years of $R_0 > 1$ crossing.  \emph{Evidence against:} mesh competing first on mainstream query types.

\textbf{Prediction 4: Endogenous Hub Emergence.}  The mesh's degree distribution becomes fat-tailed ($\gamma \leq 3$) within 3 years of crystallization, with $<$1\% of nodes handling $>$30\% of routing traffic.  \emph{Evidence against:} degree distribution remaining thin-tailed through 2036.

\textbf{Prediction 5: Nonlinear Knowledge Acceleration.}  The rate of capability improvement across the mesh accelerates nonlinearly once the degree distribution becomes fat-tailed, consistent with the vanishing epidemic threshold (Proposition~\ref{me:prop:epidemic}).  \emph{Evidence against:} capability improvement following a constant exponential rate through 2036.

\textbf{Prediction 6: Settlement Layer as Binding Constraint.}  The settlement layer becomes the binding constraint on mesh growth before device capability, network connectivity, or model quality bind.  \emph{Timing:} 2031--2034.  \emph{Evidence against:} mesh growth constrained by device capability or bandwidth through 2035.

\medskip
\noindent\textbf{Part~II: Capability Growth}

\medskip
\textbf{Prediction 7: Autocatalytic Threshold Timing.}  The mesh achieves self-sustaining capability improvement within 3 years of crystallization.  Observable as: mesh capability improving on standard benchmarks without new base model releases.  \emph{Timing:} 2033--2036.  \emph{Evidence against:} mesh capability plateauing during any 12-month period without new base model releases through 2038.

\textbf{Prediction 8: Training Agent Emergence.}  Specialized training agents---agents whose primary function is improving other agents---capture $>$10\% of internal mesh transactions within 5 years of crystallization.  \emph{Timing:} 2035--2038.  \emph{Evidence against:} mesh transactions remaining $>$95\% pure inference through 2040.

\textbf{Prediction 9: Diversity-Collapse Protection.}  Heterogeneous meshes ($J \geq 10$, $\rho \leq 0.5$) maintain capability when training on $>$50\% internally generated data, while homogeneous networks ($J \leq 3$) exhibit model collapse under the same conditions.  \emph{Evidence against:} homogeneous networks showing no degradation from synthetic data training.

\textbf{Prediction 10: Baumol Bottleneck Binding.}  The mesh's capability growth rate correlates with, and is bounded by, the rate of frontier model releases.  The mesh's annualized capability growth rate should track within $1.5\times$ of the frontier model improvement rate.  \emph{Timing:} 2034--2040.  \emph{Evidence against:} mesh capability growth exceeding $3\times$ the frontier release rate sustained over $>$2 years.

\textbf{Prediction 11: $\phieff$ Estimate.}  The mesh's effective training productivity elasticity is initially $0.6$--$0.8$ (regime~(a), converging), potentially rising toward $0.9$--$1.0$.  \emph{Evidence against:} $\phieff > 1.0$ sustained over $>$1 year.

\textbf{Prediction 12: Variety Expansion Rate.}  The number of effective specialization types $J$ grows at $15$--$30\%$ annually during the rapid growth phase, decelerating as $J$ approaches $J_{\max}$.  \emph{Evidence against:} $J$ remaining constant or declining after crystallization.


% -------------------------------------------------------------------
% 17. CONCLUSION
% -------------------------------------------------------------------
\section{Conclusion}\label{me:sec:conclusion}

This chapter has answered two questions about the distributed AI ecosystem after the crossing point identified in Chapter~4.

The first question---what organizational form emerges?---is answered in Part~I.  The answer is not isolated devices running local inference.  It is a self-organizing mesh of heterogeneous specialized agents whose collective capability exceeds centralized provision once the mesh reaches critical mass.  The mesh equilibrium emerges from three composable mechanisms: percolation-based connectivity ($\Rz > 1$), CES-aggregated heterogeneous specialization ($\rho < 1$, with the diversity premium quantified by the curvature parameter $K$ from Chapter~2), and Laplacian knowledge diffusion with a vanishing epidemic threshold on scale-free topologies.  The Fortuin-Kasteleyn unification reveals that connectivity and specialization are the same mathematical object at different Potts parameter values, predicting first-order crystallization for $q > 2$ types.  Inverse Bose-Einstein condensation connects the crossing to the dissolution of the centralized traffic condensate.

The second question---can the mesh improve itself?---is answered in Part~II.  Above the autocatalytic threshold $\Nauto$, the mesh contains a self-sustaining RAF set.  Three parameters govern the growth regime: training productivity elasticity $\varphi$, saturation $h$, and external data fraction $\alpha$.  The CES parameter $\rho$ does double duty: it generates both the diversity premium (Part~I, via Chapter~2's superadditivity theorem) and the collapse protection (Part~II, via Chapter~2's correlation robustness theorem).  The most likely near-term regime is convergence to a ceiling that rises as the autocatalytic core matures and variety expands---but the ceiling never vanishes, because the Baumol bottleneck anchors growth to the exogenous frontier training rate.

The Baumol bottleneck closes the circle.  The concentrated capital investment modeled in Chapter~4 determines both when the crossing occurs and the ceiling the mesh approaches.  The mesh is a multiplier, not a generator.  This is the hierarchical ceiling from Chapter~3 (Proposition~8.1) in concrete form: capability is bounded by the network, which is bounded by hardware.

The organizational form that emerges is not merely a static division of labor but a dynamical system capable of self-improvement.  The mesh does not just distribute inference---it creates a substrate for the endogenous growth of intelligence.  The rate of that growth is the empirical question this chapter has formalized, and the twelve predictions of Section~\ref{me:sec:predictions} make it testable.


% -------------------------------------------------------------------
% APPENDICES
% -------------------------------------------------------------------
\appendix

\section{Renormalization Group Universality}\label{me:app:RG}

\begin{remark}[Mean-Field Exactness Above the Upper Critical Dimension]\label{me:rem:RG}
For systems on networks with spectral dimension $d_s > 4$, mean-field theory is exact---not an approximation, but a rigorous result \cite{me:dorogovtsev2008}.  Real-world networks, including the internet, social networks, and infrastructure graphs, generically have $d_s > 4$ due to the small-world property.

This means that the mean-field percolation result (Proposition~\ref{me:prop:gc}), the mean-field Potts transition (Proposition~\ref{me:prop:firstorder}), and the Katz-Shapiro network goods model that underlies the supermodular game are not approximations for the mesh.  They are exact characterizations of the phase structure.  Corrections to mean-field scaling are suppressed by powers of $1/(d_s - 4)$.
\end{remark}


\section{RAF Theory Background}\label{me:app:RAF}

The Reflexively Autocatalytic and Food-generated (RAF) framework was introduced by Hordijk and Steel~\cite{me:hordijk2004} to formalize Kauffman's~\cite{me:kauffman1971,me:kauffman1993} theory of autocatalytic sets in the context of the origin of life.  The original question was: in a random soup of molecular species, how large must the system be for a self-sustaining network of catalyzed reactions to emerge?  The answer---that the threshold scales logarithmically with system complexity---is foundational.

The translation to the mesh is direct.  ``Molecule types'' are capability types ($J$).  ``Reactions'' are training operations.  ``Catalysts'' are training agents.  ``Food set'' is base model capabilities from centralized training.  The Hordijk-Steel result translates to Proposition~\ref{me:prop:RAF}.


\section{Proof Details for Effective Training Productivity}\label{me:app:phieff}

Consider the training improvement process as consisting of a unit continuum of subtasks $s \in [0,1]$.  For automated subtasks ($s \in [0, \beta_{\text{auto}}]$), the input is supplied by mesh agents: $x(s) = A_s \cdot C^{\varphi_0}$.  For non-automated subtasks ($s \in (\beta_{\text{auto}}, 1]$), the input is exogenous: $x(s) = Z$.

The aggregate improvement is Cobb-Douglas across subtasks:
\begin{equation}
\dot{C} \propto \exp\left(\int_0^1 \ln x(s)\, ds\right) = \left(\prod_{s \leq \beta_{\text{auto}}} A_s\right) \cdot C^{\beta_{\text{auto}} \varphi_0} \cdot Z^{1-\beta_{\text{auto}}}
\end{equation}

The growth rate of $C$ is:
\begin{equation}
g_C = \beta_{\text{auto}} \varphi_0 \cdot g_C + (1-\beta_{\text{auto}}) g_Z + \text{const}
\end{equation}

Solving: $g_C (1 - \beta_{\text{auto}} \varphi_0) = (1 - \beta_{\text{auto}}) g_Z + \text{const}$, confirming $\phieff = \varphi_0 / (1 - \beta_{\text{auto}} \varphi_0)$.


\section{Weitzman Recombinant Growth Connection}\label{me:app:weitzman}

Weitzman~\cite{me:weitzman1998} models the growth of ideas as a combinatorial process: new ideas are produced by recombining existing ideas.  The mesh's variety expansion mechanism (Section~\ref{me:sec:variety}) has a Weitzman interpretation.  New specialization types are produced by combining existing specializations: a medical-legal specialist combines medical reasoning and legal analysis.  The number of potential combinations grows as $\binom{J}{2} \sim J^2/2$, providing a microfoundation for why $\dot{J}$ can be superlinear in $J$ over portions of the trajectory.


\section{Nordhaus Singularity Analysis}\label{me:app:nordhaus}

Nordhaus~\cite{me:nordhaus2021} asks whether we are approaching an economic singularity.  This chapter's regime~(c) is precisely Nordhaus's singularity condition applied to the mesh.  The contribution is identifying the three specific parameters ($\phieff$, $h$, $\alpha$) whose conjunction determines the singularity.  The conclusion aligns with Nordhaus: the conditions are restrictive and unlikely to hold simultaneously.


% -------------------------------------------------------------------
% REFERENCES
% -------------------------------------------------------------------
\newpage
\begin{thebibliography}{99}

\bibitem{me:smirl2026a} Smirl, J. (2026a). The CES triple role: Superadditivity, correlation robustness, and strategic independence as three views of isoquant curvature. Thesis Chapter~2.

\bibitem{me:smirl2026b} Smirl, J. (2026b). Complementary heterogeneity: A port-Hamiltonian framework for the AI-crypto transition. Thesis Chapter~3.

\bibitem{me:aghion2018} Aghion, P., Jones, B.~F., \& Jones, C.~I. (2018). Artificial intelligence and economic growth. In A.~Agrawal, J.~Gans, \& A.~Goldfarb (Eds.), \emph{The Economics of Artificial Intelligence} (pp.~237--282). University of Chicago Press.

\bibitem{me:barabasi2001} Bianconi, G., \& Barab\'{a}si, A.-L. (2001). Bose-Einstein condensation in complex networks. \emph{Physical Review Letters}, 86(24), 5632--5635.

\bibitem{me:bastolla2005} Bastolla, U., L\"{a}ssig, M., Manrubia, S.~C., \& Valleriani, A. (2005). Biodiversity in model ecosystems, I: Coexistence conditions for competing species. \emph{Journal of Theoretical Biology}, 235(4), 521--530.

\bibitem{me:baumol1967} Baumol, W.~J. (1967). Macroeconomics of unbalanced growth: The anatomy of urban crisis. \emph{American Economic Review}, 57(3), 415--426.

\bibitem{me:baxter1982} Baxter, R.~J. (1982). \emph{Exactly Solved Models in Statistical Mechanics}. Academic Press.

\bibitem{me:becker1992} Becker, G.~S., \& Murphy, K.~M. (1992). The division of labor, coordination costs, and knowledge. \emph{Quarterly Journal of Economics}, 107(4), 1137--1160.

\bibitem{me:bloom2020} Bloom, N., Jones, C.~I., Van Reenen, J., \& Webb, M. (2020). Are ideas getting harder to find? \emph{American Economic Review}, 110(4), 1104--1144.

\bibitem{me:bonabeau1998} Bonabeau, E., Theraulaz, G., \& Deneubourg, J.-L. (1998). Fixed response thresholds and the regulation of division of labor in insect societies. \emph{Bulletin of Mathematical Biology}, 60(4), 753--807.

\bibitem{me:bresnahan1995} Bresnahan, T.~F., \& Trajtenberg, M. (1995). General purpose technologies: Engines of growth? \emph{Journal of Econometrics}, 65(1), 83--108.

\bibitem{me:burnet1959} Burnet, F.~M. (1959). \emph{The Clonal Selection Theory of Acquired Immunity}. Cambridge University Press.

\bibitem{me:christensen1997} Christensen, C.~M. (1997). \emph{The Innovator's Dilemma}. Harvard Business School Press.

\bibitem{me:dorogovtsev2008} Dorogovtsev, S.~N., Goltsev, A.~V., \& Mendes, J.~F.~F. (2008). Critical phenomena in complex networks. \emph{Reviews of Modern Physics}, 80(4), 1275--1335.

\bibitem{me:ea1975} Edwards, S.~F., \& Anderson, P.~W. (1975). Theory of spin glasses. \emph{Journal of Physics F: Metal Physics}, 5(5), 965--974.

\bibitem{me:eigen1977} Eigen, M., \& Schuster, P. (1977). The hypercycle: A principle of natural self-organization. Part A: Emergence of the hypercycle. \emph{Naturwissenschaften}, 64(11), 541--565.

\bibitem{me:erdos1960} Erd\H{o}s, P., \& R\'{e}nyi, A. (1960). On the evolution of random graphs. \emph{Publications of the Mathematical Institute of the Hungarian Academy of Sciences}, 5, 17--61.

\bibitem{me:feinberg2019} Feinberg, M. (2019). \emph{Foundations of Chemical Reaction Network Theory}. Springer.

\bibitem{me:FK1972} Fortuin, C.~M., \& Kasteleyn, P.~W. (1972). On the random-cluster model: I. Introduction and relation to other models. \emph{Physica}, 57(4), 536--564.

\bibitem{me:hayek1945} Hayek, F.~A. (1945). The use of knowledge in society. \emph{American Economic Review}, 35(4), 519--530.

\bibitem{me:hirsch1985} Hirsch, M.~W. (1985). Systems of differential equations that are competitive or cooperative. II: Convergence almost everywhere. \emph{SIAM Journal on Mathematical Analysis}, 16(3), 423--439.

\bibitem{me:hofbauer1998} Hofbauer, J., \& Sigmund, K. (1998). \emph{Evolutionary Games and Population Dynamics}. Cambridge University Press.

\bibitem{me:hordijk2004} Hordijk, W., \& Steel, M. (2004). Detecting autocatalytic, self-sustaining sets in chemical reaction systems. \emph{Journal of Theoretical Biology}, 227(4), 451--461.

\bibitem{me:jain1998} Jain, S., \& Krishna, S. (1998). Autocatalytic sets and the growth of complexity in an evolutionary model. \emph{Physical Review Letters}, 81(25), 5684--5687.

\bibitem{me:jain2001} Jain, S., \& Krishna, S. (2001). A model for the emergence of cooperation, interdependence, and structure in evolving networks. \emph{Proceedings of the National Academy of Sciences}, 98(2), 543--547.

\bibitem{me:jones1995} Jones, C.~I. (1995). R\&D-based models of economic growth. \emph{Journal of Political Economy}, 103(4), 759--784.

\bibitem{me:jones2005} Jones, C.~I. (2005). Growth and ideas. In P.~Aghion \& S.~N. Durlauf (Eds.), \emph{Handbook of Economic Growth} (Vol.~1B, pp.~1063--1111). Elsevier.

\bibitem{me:katz1985} Katz, M.~L., \& Shapiro, C. (1985). Network externalities, competition, and compatibility. \emph{American Economic Review}, 75(3), 424--440.

\bibitem{me:kauffman1971} Kauffman, S.~A. (1971). Cellular homeostasis, epigenesis and replication in randomly aggregated macromolecular systems. \emph{Journal of Cybernetics}, 1(1), 71--96.

\bibitem{me:kauffman1993} Kauffman, S.~A. (1993). \emph{The Origins of Order: Self-Organization and Selection in Evolution}. Oxford University Press.

\bibitem{me:lasry2007} Lasry, J.-M., \& Lions, P.-L. (2007). Mean field games. \emph{Japanese Journal of Mathematics}, 2(1), 229--260.

\bibitem{me:loreau2001} Loreau, M., \& Hector, A. (2001). Partitioning selection and complementarity in biodiversity experiments. \emph{Nature}, 412, 72--76.

\bibitem{me:milgrom1990} Milgrom, P., \& Roberts, J. (1990). Rationalizability, learning, and equilibrium in games with strategic complementarities. \emph{Econometrica}, 58(6), 1255--1277.

\bibitem{me:nordhaus2021} Nordhaus, W.~D. (2021). Are we approaching an economic singularity? Information technology and the future of economic growth. \emph{American Economic Journal: Macroeconomics}, 13(1), 299--332.

\bibitem{me:pastor2001} Pastor-Satorras, R., \& Vespignani, A. (2001). Epidemic spreading in scale-free networks. \emph{Physical Review Letters}, 86(14), 3200--3203.

\bibitem{me:potts1952} Potts, R.~B. (1952). Some generalized order-disorder transformations. \emph{Mathematical Proceedings of the Cambridge Philosophical Society}, 48(1), 106--109.

\bibitem{me:romer1990} Romer, P.~M. (1990). Endogenous technological change. \emph{Journal of Political Economy}, 98(5), S71--S102.

\bibitem{me:SK1975} Sherrington, D., \& Kirkpatrick, S. (1975). Solvable model of a spin-glass. \emph{Physical Review Letters}, 35(26), 1792--1796.

\bibitem{me:shumailov2024} Shumailov, I., Shumaylov, Z., Zhao, Y., Papernot, N., Anderson, R., \& Gal, Y. (2024). AI models collapse when trained on recursively generated data. \emph{Nature}, 631, 755--759.

\bibitem{me:tarski1955} Tarski, A. (1955). A lattice-theoretical fixpoint theorem and its applications. \emph{Pacific Journal of Mathematics}, 5(2), 285--309.

\bibitem{me:tilman1982} Tilman, D. (1982). \emph{Resource Competition and Community Structure}. Princeton University Press.

\bibitem{me:topkis1998} Topkis, D.~M. (1998). \emph{Supermodularity and Complementarity}. Princeton University Press.

\bibitem{me:weitzman1998} Weitzman, M.~L. (1998). Recombinant growth. \emph{Quarterly Journal of Economics}, 113(2), 331--360.

\bibitem{me:wu1982} Wu, F.~Y. (1982). The Potts model. \emph{Reviews of Modern Physics}, 54(1), 235--268.

\bibitem{me:kermack1927} Kermack, W.~O., \& McKendrick, A.~G. (1927). A contribution to the mathematical theory of epidemics. \emph{Proceedings of the Royal Society of London A}, 115(772), 700--721.

\bibitem{me:ising1925} Ising, E. (1925). Beitrag zur Theorie des Ferromagnetismus. \emph{Zeitschrift f\"{u}r Physik}, 31(1), 253--258.

\end{thebibliography}

\end{document}
