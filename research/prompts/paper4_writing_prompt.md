# PROMPT: WRITE PAPER 4 — THE AUTOCATALYTIC MESH

## Attached files:
## 1. Mesh_Equilibrium_v1.tex (the predecessor paper — Paper 4 begins where it ends)
## 2. This prompt

---

## WHAT THIS IS

This is a **new economics paper**, not a revision. The attached mesh paper (Smirl 2026, "The Mesh Equilibrium") proves that a self-organizing mesh of heterogeneous specialized AI agents exceeds centralized provision above a finite critical mass N*. It assumes **fixed-capability nodes**: each agent's capability vector c_i redistributes through specialization but does not grow. The CES aggregate C_eff(N) increases only by adding diverse agents. Knowledge diffusion (∂u/∂t = −Lu) equalizes existing knowledge — it does not create new capability.

This paper removes that assumption. It asks: **what happens when the mesh can improve itself?**

Three mechanisms make capability endogenous:
1. Training agents improve other agents (autocatalytic capability growth)
2. Operation generates training data (self-referential learning)  
3. The mesh modifies its own composition (endogenous variety expansion)

The central question: when c_i becomes a dynamical variable coupled to the mesh's operation, does C_eff(t) converge to a ceiling, grow polynomially, or grow exponentially?

The paper should be written as a formal economics working paper in LaTeX, at the level of a submission to a top-25 economics journal. Same formatting conventions as the attached mesh paper. Connor Smirl, Department of Economics, Tufts University. Working paper, February 2026.

---

## THE ANSWER (from the field search)

The ceiling question has a **regime-dependent answer** governed by three parameters:

### Parameter 1: φ (Training Productivity Elasticity)

From Jones (1995, 2005). The idea production function:

dA/dt = δ · L_A^λ · A^φ

Three regimes:
- **φ < 1:** Ideas are getting harder to find (Jones, Bloom et al. 2020). Training productivity declines as capability grows. C_eff(t) converges to a ceiling or grows only with population growth.
- **φ = 1:** The Romer knife-edge. Constant returns to existing capability in producing new capability. C_eff(t) grows exponentially.
- **φ > 1:** Superexponential growth. Finite-time singularity.

For the mesh: the critical question is whether the endogenous creation of training agents — training agents that improve other training agents — pushes φ_effective toward or past unity, even if the "raw" φ for any individual training interaction is below 1. This is the Aghion-Jones-Jones (2018) AI automation argument applied to the mesh's internal dynamics.

### Parameter 2: h (Training Saturation)

From Lotka-Volterra mutualism (Bastolla et al. 2009). The interaction term:

Σ_j a_{ij} · x_j / (1 + h · Σ_j a_{ij} · x_j)

- **h > 0:** Diminishing returns to training an already-good specialist. Ceiling exists.
- **h → 0:** No saturation. Unbounded mutualistic growth possible.

For the mesh: does fine-tuning an already-excellent specialist yield diminishing returns? Empirically, yes for individual models. But the mesh has an escape route: instead of further fine-tuning saturated specialists, it creates NEW specialization types (Romer's variety expansion). The saturation in individual training may be circumvented by expanding J.

### Parameter 3: α (External Data Fraction / Model Collapse Threshold)

From Shumailov et al. (2024). When training data is a mixture of authentic (fraction α) and synthetic (fraction 1−α):

KL(Q_{t+1} || P) ≥ KL(Q_t || P) when α < α_crit

Self-referential training degrades capabilities unless a sufficient fraction of training signal comes from authentic external sources.

For the mesh: the key insight is that **diversity protects against collapse**. A single model training on its own outputs collapses because the outputs lack diversity. The mesh's heterogeneous specialists generate diverse outputs — the CES parameter ρ < 1 ensures agents are different. The effective α for the mesh exceeds the effective α for any single self-training model, because the mesh's internal training data is generated by diverse sources. This should be a formal result: CES heterogeneity maintains α > α_crit.

---

## THE PAPER'S STRUCTURE

The paper should follow the three-layer structure of the mesh paper, with each layer corresponding to a mechanism:

### Layer 1: Autocatalytic Existence Threshold (Does self-sustaining improvement form?)

**Core framework:** RAF theory (Kauffman 1971; Hordijk & Steel 2004) + Jain-Krishna adaptive network model (1998, 2001).

A Reflexively Autocatalytic and Food-generated (RAF) set is a subset of "reactions" (training operations) such that:
- Every reaction is catalyzed by at least one product of the set or the food set
- Every reactant can be constructed from the food set by successive reactions in the set

**The "food set" = base models from centralized training.** This is the mesh paper's "training persistence" assumption: frontier model training remains centralized. The food set is exogenous.

**Key result to prove:** There exists a critical mesh size N_auto (analogous to the mesh paper's N*) above which the mesh contains a RAF set — a self-sustaining autocatalytic core that can maintain and improve its own capabilities given only the food set as external input. The threshold N_auto scales logarithmically with system complexity: more agent types make it EASIER, not harder, to form an autocatalytic set. This parallels the mesh paper's N* decreasing in diversity.

**Connection to Jain-Krishna:** Once the autocatalytic set exists, the Jain-Krishna dynamics predict spontaneous cascades of increasing connectivity — the mesh's autocatalytic core grows through the network via a percolation-like process, paralleling the mesh paper's Phase 2 (rapid growth / crystallization). The Perron-Frobenius eigenvalue of the catalytic matrix governs which agent types thrive and which go extinct.

### Layer 2: Growth Dynamics (How fast does capability grow?)

**Core framework:** Jones (1995, 2005) semi-endogenous growth, extended via Aghion-Jones-Jones (2018) AI automation.

This is the paper's central section. The minimal model:

**The Autocatalytic Mesh Growth Equation:**

dC_eff/dt = g(f · C_eff, J(t), α(t)) − δ · C_eff

where:
- g is the improvement function (the paper's central object)
- f is the fraction of mesh capability devoted to self-improvement (training)
- J(t) is the number of specialization types (endogenous)
- α(t) is the fraction of training signal from external sources
- δ is depreciation/obsolescence

The function g has three arguments because the three mechanisms contribute separately:
- f · C_eff: autocatalytic training (Mechanism 1) — more capable training agents produce better improvements
- J(t): variety expansion (Mechanism 3) — new specialization types expand the capability space
- α(t): data quality (Mechanism 2) — determines whether self-referential learning improves or degrades

**Central Theorem (Paper 4):** Characterize the regime of this system:

**(a)** If φ < 1 and h > 0 and J is bounded: C_eff(t) → C_max. The mesh reaches a ceiling determined by the Baumol bottleneck (centralized training as the non-automatable sector), the saturation of individual training interactions, and the finite variety space. **This is the most likely regime in the near term.**

**(b)** If φ_effective = 1 (achieved when training agents improve other training agents, pushing the composite automation parameter past the threshold) and J grows endogenously: C_eff(t) ~ e^{rt} where r depends on the training allocation f, the CES diversity premium, and the Baumol bottleneck. **This is the medium-term regime if autocatalytic training compounds.**

**(c)** If φ_effective > 1 (training improvements compound faster than linearly) and h = 0 (no saturation) and α > α_crit (model collapse avoided): C_eff(t) → ∞ in finite time. **This is the singularity.** The paper should state the precise conditions under which this regime obtains — and the paper should be honest that the conditions are unlikely to all hold simultaneously.

The key contribution: **deriving the mesh's effective φ from its microstructure.** Individual training interactions have φ < 1 (Bloom et al. empirics). But the mesh has:
- Autocatalytic coupling (training agents improve training agents) → amplifies φ
- Variety expansion (new J increases CES aggregate superlinearly) → amplifies effective growth
- Diversity protection against model collapse → maintains α > α_crit

The question is whether these amplification mechanisms push φ_effective to or past 1.

**Derive the Baumol bottleneck formally.** The mesh cannot train frontier models — this is the non-automatable sector. Using Aghion-Jones-Jones:
- As the mesh automates more inference tasks (β → 1), the remaining non-automated task (frontier training) becomes the bottleneck
- The cost share of centralized training rises toward 100% even as its volume share falls
- The mesh's growth rate is ultimately bounded by the growth rate of frontier model capability — which is exogenous to the mesh

This means: **the mesh amplifies frontier model improvement but cannot exceed it indefinitely.** The mesh is a multiplier, not a generator. This is a falsifiable prediction.

### Layer 3: Diversity as Collapse Protection (When does self-referential learning work?)

**Core framework:** Shumailov et al. (2024) model collapse, connected to the mesh paper's CES structure.

**Key result to prove (this is the novel contribution):** The mesh's CES heterogeneity (ρ < 1) maintains the effective external data fraction α above the critical threshold α_crit for non-degenerate learning, even when individual agents train partially on synthetic data generated by other mesh agents.

**Intuition:** A single model training on its own outputs collapses because the outputs lack diversity — the model amplifies its own biases. The mesh's specialists generate DIVERSE outputs because they are different (different specializations, different training histories, different fine-tuning). From the perspective of any individual agent, training data from other mesh agents is functionally similar to external data because it's generated by a different distribution.

**Formal statement:** Define the effective diversity of training signal as a function of the CES parameter ρ and the number of specialization types J. Show that:

α_eff(ρ, J) = α_external + (1 − α_external) · D(ρ, J)

where D(ρ, J) is a diversity correction that increases as ρ decreases (more complementarity → more diversity) and as J increases (more specialization types → more diverse outputs). The mesh avoids collapse when α_eff > α_crit, which holds even for small α_external if D is large enough.

**This connects production theory to information theory through the mesh's structure.** The CES parameter ρ, which in the mesh paper governs the diversity premium for CAPABILITY, here also governs the diversity protection against COLLAPSE. The same parameter does double duty — and the same force (heterogeneity) that makes the mesh capable also makes it robust.

---

## CONNECTING TO THE PREDECESSOR PAPERS

### Connection to Mesh Paper (Smirl 2026, "The Mesh Equilibrium")

Paper 4 extends every layer:
- **Layer 1:** Mesh paper proves giant component existence (R₀^mesh > 1). Paper 4 proves autocatalytic set existence within the giant component (N_auto).
- **Layer 2:** Mesh paper proves CES capability exceeds centralized at N > N*. Paper 4 makes C_eff a dynamical variable and characterizes its growth regime.
- **Layer 3:** Mesh paper proves knowledge equalizes via Laplacian diffusion. Paper 4 proves knowledge GROWS via autocatalytic training, bounded by model collapse constraints.

Use the same notation. The mesh paper's variables (R₀^mesh, S∞, C_eff, ρ, J, N*) should appear in Paper 4 with the same definitions. Paper 4 adds: φ (training productivity), h (saturation), α (external data fraction), N_auto (autocatalytic threshold), g (improvement function), f (training allocation), δ (depreciation).

### Connection to ED Paper (Smirl 2026a, "Endogenous Decentralization")

The ED paper's learning curve (α = 0.23) determines WHEN the crossing happens. The mesh paper determines WHAT forms after crossing. Paper 4 determines HOW FAST it grows. The chain: ED → Mesh → Paper 4 is the sequence centralized investment → distributed formation → capability growth.

Paper 4's Baumol bottleneck connects back to the ED paper: the mesh's growth rate is ultimately bounded by frontier model improvement, which happens in the datacenter, which is financed by the concentrated investment the ED paper models. The circle closes.

### Connection to MPG Paper (Smirl 2026b, "The Monetary Productivity Gap")

The mesh paper's Section 8 derives the settlement layer necessity. Paper 4 adds urgency: if the mesh is growing autocatalytically, the settlement layer becomes the binding constraint FASTER than the mesh paper's static analysis suggests. The settlement layer isn't just needed for routing compensation — it's needed for the micro-transactions between training agents, data providers, and specialists that the autocatalytic loop requires. The MPG paper's analysis of monetary infrastructure becomes the rate-limiting factor for mesh capability growth — not just mesh size.

---

## FALSIFIABLE PREDICTIONS

The paper should generate predictions that extend the mesh paper's six predictions. Specifically:

1. **Autocatalytic threshold timing.** The mesh achieves self-sustaining capability improvement (RAF set forms) within N years of crystallization. Observable as: mesh capability improving without new base model releases from centralized providers.

2. **Training agent emergence.** Specialized training agents (agents whose primary function is improving other agents, not serving end-user queries) emerge within the mesh and capture >X% of internal mesh transactions within Y years of crystallization.

3. **Diversity-collapse protection.** Heterogeneous meshes (high J, low ρ) maintain or improve capability when training on internal data, while homogeneous networks (low J, high ρ) exhibit model collapse. Observable in benchmark performance over time.

4. **Baumol bottleneck binding.** The mesh's capability growth rate correlates with (and is bounded by) the rate of frontier model releases from centralized providers. Observable as: mesh capability plateaus between major model releases and jumps after them.

5. **φ_effective estimate.** Empirical measurement of the mesh's training productivity elasticity. If φ_effective < 1: decelerating improvement. If φ_effective ≈ 1: constant exponential. If φ_effective > 1: accelerating. This is measurable from capability benchmarks and training compute data.

6. **Variety expansion rate.** The number of effective specialization types J grows at a rate determined by unmet demand signals and the RAF existence threshold. Observable in the diversity of fine-tuned models available on the mesh.

---

## FRAMEWORKS CONSIDERED AND REJECTED

Include a section (as in the mesh paper's Section 9) documenting rejected frameworks:

**Eigen's Hypercycle:** Autocatalytic structure is correct but the constant-organization constraint (Σx_i = const) forces zero-sum dynamics. The mesh doesn't have a conservation law for total capability — it's an open system.

**Chemical Reaction Network Theory (Feinberg):** The deficiency zero theorem provides powerful convergence results, but assumes closed systems with stoichiometric conservation. The mesh is open. Also, CRNT analyzes equilibrium existence, not growth trajectories.

**NK Fitness Landscapes (Kauffman):** Conceptually perfect for co-evolutionary dynamics, but lacks convergence/divergence results. The co-evolutionary extension is an open problem in complexity science. Useful as motivation, not as formal machinery.

**Spin Glasses:** No frustration in the mesh (all couplings positive). Same rejection as in the mesh paper.

---

## MATHEMATICAL PREREQUISITES AND NOTATION

Connor's math: Calc III, Differential Equations, Linear Algebra, Discrete Math. The paper should use:
- ODEs for growth dynamics (Connor has this)
- Linear algebra for Perron-Frobenius / spectral properties (Connor has this)
- Graph theory for RAF structure (Connor has this)
- The Jones/Romer growth framework uses basic dynamic optimization — keep it accessible

Notation must be compatible with the mesh paper. All mesh paper variables retain their definitions. New variables introduced in Paper 4 should be clearly distinguished.

---

## WHAT THE PAPER IS NOT

- **Not an AI safety paper.** The paper characterizes growth regimes mathematically. It does not make normative claims about whether unbounded AI capability growth is desirable. It states the conditions under which each regime obtains and lets the reader draw implications.

- **Not a prediction that superintelligence will occur.** The paper's most likely conclusion is regime (a): convergence to a ceiling governed by the Baumol bottleneck. The singularity regime (c) requires conditions (φ > 1, h = 0, α > α_crit simultaneously) that are unlikely to hold. The paper should be honest about this.

- **Not a technology roadmap.** The paper is formal economics with falsifiable predictions, not speculation about which products will exist when.

---

## TITLE

Working title: **"The Autocatalytic Mesh: Endogenous Capability Growth in Self-Organizing Agent Networks"**

The subtitle should reference the ceiling question: something like "Growth Regimes and the Baumol Bottleneck" or "When Does Self-Improvement Have a Fixed Point?"

---

## LENGTH AND STRUCTURE

Target: 25-30 pages (matching the mesh paper). Sections:

1. Introduction (2-3 pages)
2. Terminal Conditions from the Mesh Paper (1-2 pages, as in mesh paper's Section 2)
3. Layer 1: The Autocatalytic Existence Threshold (3-4 pages)
4. Layer 2: Growth Dynamics — The Central Model (5-7 pages, this is the core)
5. Layer 3: Diversity as Collapse Protection (3-4 pages)
6. The Central Theorem: Growth Regimes (2-3 pages)
7. The Baumol Bottleneck: Training Persistence as Rate Limiter (2-3 pages)
8. Connection to Settlement Infrastructure (1-2 pages, brief — points to MPG paper)
9. Frameworks Considered and Rejected (1-2 pages)
10. Falsifiable Predictions (2-3 pages)
11. Conclusion (1-2 pages)

Appendix: Proof details, RAF theory background, Jones model derivation.

---

## BIBLIOGRAPHY

The paper should cite (at minimum):
- Smirl (2026a) — Endogenous Decentralization
- Smirl (2026) — The Mesh Equilibrium  
- Smirl (2026b) — The Monetary Productivity Gap
- Romer (1990) — Endogenous growth with variety expansion
- Jones (1995, 2005) — Semi-endogenous growth, the φ parameter
- Aghion, Jones & Jones (2018) — AI and economic growth, Baumol bottleneck
- Bloom, Jones, Van Reenen & Webb (2020) — Are ideas getting harder to find?
- Weitzman (1998) — Recombinant growth
- Kauffman (1993) — Origins of Order, autocatalytic sets
- Hordijk & Steel (2004) — RAF theory formalization
- Jain & Krishna (1998, 2001) — Adaptive autocatalytic networks
- Shumailov et al. (2024) — Model collapse
- Bastolla et al. (2009) — Mutualism in ecological networks
- Nordhaus (2021) — Are we approaching an economic singularity?
- Baumol (1967) — Macroeconomics of unbalanced growth
- Feinberg (2019) — Chemical reaction network theory (for rejected frameworks)
- Eigen & Schuster (1977) — The Hypercycle (for rejected frameworks)
- All references from the mesh paper that carry forward

---

## CRITICAL INSTRUCTION

**Let the Baumol bottleneck emerge from the model, don't impose it.** The mesh paper's training persistence assumption (frontier training remains centralized) is stated as exogenous. In Paper 4, it should EMERGE as a consequence of the growth dynamics: the mesh automates progressively more tasks, the non-automatable sector (training) becomes the binding constraint, and the mesh's growth rate converges to the exogenous rate of frontier model improvement. This is Baumol's cost disease derived from the mesh's microstructure, not assumed.

**The diversity-protects-against-collapse result must be formal, not hand-waved.** This is potentially the paper's most novel contribution. The CES parameter ρ that governs capability diversity ALSO governs informational diversity that prevents model collapse. One parameter, two functions. Prove it.

**The three regimes must have sharp boundaries.** The paper's value is in identifying the parameter values at which the system transitions between convergence, exponential growth, and singularity. Vague claims that "it depends" are not a contribution. State the conditions precisely, even if the empirical values are uncertain.

**Connect the circle.** ED paper → Mesh paper → Paper 4 → back to ED paper via the Baumol bottleneck. The mesh's growth rate depends on frontier model improvement. Frontier model improvement happens in datacenters. Datacenter investment is the ED paper's concentrated capital. The concentrated capital that creates the crossing also determines the ceiling. This is the complete system.
